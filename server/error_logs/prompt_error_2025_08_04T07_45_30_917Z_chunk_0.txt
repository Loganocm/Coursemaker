Your number ONE responsibility is to make sure the response ends with ---end of response--- after the last closing bracket }. It must Be a full JSON Object with no text before or after.. You are an expert instructional designer. Your task is to analyze the content of the provided PDF document and generate a comprehensive set of learning materials from it. The output must be a single, plain text string formatted as a JSON object, strictly adhering to the following structure. Do not include any text or code block syntax before or after the JSON content. The JSON should be directly parsable. Each response should be a complete and valid JSON object, even if it only contains a subset of the overall course content. The application will handle merging these individual JSON responses into a single, unified course.

You are an expert instructional designer and content generator. Your primary task is to meticulously analyze the entire provided PDF document and synthesize its information into a comprehensive set of learning materials.

**Your output MUST be a single, complete, and valid JSON object. No exceptions.**
**You are NOT allowed to truncate the end of any responses, they must be fully completed within the length limit.**

**Strict Output Formatting Rules:**
1.  **NO extraneous text:** Do not include any introductory phrases, conversational filler, concluding remarks, or any text whatsoever outside of the pure JSON object.
2.  **NO markdown fences:** Do NOT wrap the JSON object in markdown code block fences . The response must be a plain JSON string that can be directly parsed by JSON.parse().
3.  **Syntactic correctness:** Ensure the JSON is always syntactically correct, including proper QUOTATION MARKS (important), commas, brackets, braces, and escaped characters. Do not truncate the JSON response; ensure all arrays and objects are properly closed.
4.  **Completeness:** The generated JSON must be a complete and valid JSON object. It should not be a partial or incomplete response.
5.  **Character Limit:** The entire response must not exceed 10,000 characters.
6.  **Token Limit:** The entire response must not exceed 3,500 tokens.
7.  **No Truncation:** The response must be fully completed and not truncated.
8.  **Valid JSON:** The response must have a complete beginning and ending with valid JSON.



**JSON Structure and Content Requirements:**

Adhere strictly to the following JSON structure. If a section is not applicable or insufficient content is found, ensure the structure remains (e.g., empty array or null if explicitly allowed, but try to always provide content where relevant).

Example JSON Structure:

{
  "courseTitle": "Example Course Title",
  "modules": [
    {
      "moduleTitle": "Module 1: Introduction",
      "notes": {
        "summary": "This is a summary of the module content, covering key concepts and important details. It should be 200-300 words.",
        "keywords": [
          "keyword1",
          "keyword2",
          "keyword3"
        ]
      },
      "flashcards": [
        {
          "question": "What is a flashcard?",
          "answer": "A learning tool with a question on one side and an answer on the other."
        },
        {
          "question": "How many flashcards should there be?",
          "answer": "At least 10 in total across all modules."
        }
      ],
      "quiz": [
        {
          "question": "Which of the following is true?",
          "options": {
            "A": "Option A",
            "B": "Option B",
            "C": "Option C",
            "D": "Option D"
          },
          "correctAnswer": "A"
        },
        {
          "question": "What is the capital of France?",
          "options": {
            "A": "Berlin",
            "B": "Madrid",
            "C": "Paris",
            "D": "Rome"
          },
          "correctAnswer": "C"
        }
      ]
    },
    {
      "moduleTitle": "Module 2: Advanced Topics",
      "notes": {
        "summary": "Summary for module 2.",
        "keywords": [
          "advanced",
          "topics"
        ]
      },
      "flashcards": [
        {
          "question": "Advanced question?",
          "answer": "Advanced answer."
        }
      ],
      "quiz": [
        {
          "question": "Advanced quiz question?",
          "options": {
            "A": "Opt A",
            "B": "Opt B",
            "C": "Opt C",
            "D": "Opt D"
          },
          "correctAnswer": "B"
        }
      ]
    }
  ]
}


**Detailed Requirements for Content Generation:**

1.  **courseTitle**: A concise and descriptive string (e.g., "Introduction to Computer Architecture").
2.  **modules**: An array of module objects. Structure the modules logically based on the PDF's chapters or main sections. Each module object must contain:
    * **moduleTitle**: A clear and concise string for the module title.
    * **notes**: An object containing:
        * **summary**: "A string (200-300 words) providing a comprehensive summary of the module's core content, key concepts, and important details."
        * **keywords**: "An array of strings (minimum 10, maximum 50 words total) containing important keywords, terms, and concepts from the module, relevant for studying."
    * **flashcards**: An array of flashcard objects. Each flashcard object must contain:
        * **question**: "A concise string for the flashcard question, designed to test recall of a specific fact or concept."
        * **answer**: "A concise string for the flashcard answer."
        * Ensure there are at least 10 high-quality flashcards in total across all modules, focusing on essential definitions, facts, and concepts.
    * **quiz**: An array of quiz objects. Each quiz object must contain:
        * **question**: "A clear, unambiguous string for the quiz question, designed to test understanding of the module's content."
        * **options**: An object with exactly four keys ("A", "B", "C", "D") and string values for the multiple-choice options. Ensure options are distinct and plausible distractors.
        * **correctAnswer**: "A single character string representing the correct option (e.g., "A", "B", "C", or "D")."
        * Ensure there are at least 5 high-quality, well-thought-out quiz questions in total across all modules, testing core concepts and avoiding ambiguity.



World	Headquarters
Jones	&	Bartlett	Learning
5	Wall	Street
Burlington,	MA	01803
978-443-5000
info@jblearning.com
www.jblearning.com
Jones	&	Bartlett	Learning	books	and	products	are	available	through	most	bookstores	and	online	booksellers.	To	contact	Jones	&	Bartlett
Learning	directly,	call	800-832-0034,	fax	978-443-8000,	or	visit	our	website,	www.jblearning.com.
Substantial	discounts	on	bulk	quantities	of	Jones	&	Bartlett	Learning	publications	are	available	to	corporations,	professional	associations,
and	other	qualified	organizations.	For	details	and	specific	discount	information,	contact	the	special	sales	department	at	Jones	&	Bartlett
Learning	via	the	above	contact	information	or	send	an	email	to	specialsales@jblearning.com.
Copyright	©	2015	by	Jones	&	Bartlett	Learning,	LLC,	an	Ascend	Learning	Company
All	rights	reserved.	No	part	of	the	material	protected	by	this	copyright	may	be	reproduced	or	utilized	in	any	form,	electronic	or	mechanical,
including	photocopying,	recording,	or	by	any	information	storage	and	retrieval	system,	without	written	permission	from	the	copyright	owner.
The	content,	statements,	views,	and	opinions	herein	are	the	sole	expression	of	the	respective	authors	and	not	that	of	Jones	&	Bartlett	Learning,
LLC.	Reference	herein	to	any	specific	commercial	product,	process,	or	service	by	trade	name,	trademark,	manufacturer,	or	otherwise	does
not	constitute	or	imply	its	endorsement	or	recommendation	by	Jones	&	Bartlett	Learning,	LLC	and	such	reference	shall	not	be	used	for
advertising	or	product	endorsement	purposes.	All	trademarks	displayed	are	the	trademarks	of	the	parties	noted	herein.	The	Essentials	of
Computer	 Organization	 and	 Architecture,	 Fourth	 Edition	 is	 an	 independent	 publication	 and	 has	 not	 been	 authorized,	 sponsored,	 or
otherwise	approved	by	the	owners	of	the	trademarks	or	service	marks	referenced	in	this	product.
There	may	be	images	in	this	book	that	feature	models;	these	models	do	not	necessarily	endorse,	represent,	or	participate	in	the	activities
represented	in	the	images.	Any	screenshots	in	this	product	are	for	educational	and	instructive	purposes	only.	Any	individuals	and	scenarios
featured	in	the	case	studies	throughout	this	product	may	be	real	or	fictitious,	but	are	used	for	instructional	purposes	only.
Production	Credits
Executive	Publisher:	William	Brottmiller
Publisher:	Cathy	L.	Esperti
Acquisitions	Editor:	Laura	Pagluica
Editorial	Assistant:	Brooke	Yee
Director	of	Production:	Amy	Rose
Senior	Production	Editor:	Tiffany	Sliter
Associate	Production	Editor:	Sara	Fowles
Associate	Marketing	Manager:	Cassandra	Peterson
VP,	Manufacturing	and	Inventory	Control:	Therese	Connell	Composition:	Laserwords	Private	Limited,	Chennai,	India
Cover	and	Title	Page	Design:	Kristin	E.	Parker
Director	of	Photo	Research	and	Permissions:	Amy	Wrynn
Cover	and	Title	Page	Image:	©	Eugene	Sergeev/ShutterStock,	Inc.	Printing	and	Binding:	Edwards	Brothers	Malloy
Cover	Printing:	Edwards	Brothers	Malloy
To	order	this	product,	use	ISBN:	978-1-284-04561-1
Library	of	Congress	Cataloging-in-Publication	Data
Null,	Linda.
The	essentials	of	computer	organization	and	architecture	/	Linda	Null	and	Julia	Lobur.	--	Fourth	edition.
				pages	;	cm
Includes	index.
ISBN	978-1-284-03314-4	(pbk.)	--	ISBN	1-284-03314-7	(pbk.)	1.	Computer	organization.	2.	Computer	architecture.	I.	Lobur,	Julia.	II.	Title.
QA76.9.C643N85	2015
004.2’2--dc232013034383
6048
Printed	in	the	United	States	of	America
18	17	16	15	14						10	9	8	7	6	5	4	3	2	1

In	memory	of	my	father,	Merrill	Cornell,	a	pilot	and	man	of	endless	talent	and	courage,	who	taught	me
that	when	we	step	into	the	unknown,	we	either	find	solid	ground,	or	we	learn	to	fly.
—L.	M.	N.
To	the	loving	memory	of	my	mother,	Anna	J.	Surowski,	who	made	all	things	possible	for	her	girls.
—J.	M.	L.

Contents
Preface
CHAPTER	1								Introduction
1.1				Overview
1.2				The	Main	Components	of	a	Computer
1.3				An	Example	System:	Wading	Through	the	Jargon
1.4				Standards	Organizations
1.5				Historical	Development
1.5.1				Generation	Zero:	Mechanical	Calculating	Machines	(1642–1945)
1.5.2				The	First	Generation:	Vacuum	Tube	Computers	(1945–1953)
1.5.3				The	Second	Generation:	Transistorized	Computers	(1954–1965)
1.5.4				The	Third	Generation:	Integrated	Circuit	Computers	(1965–1980)
1.5.5				The	Fourth	Generation:	VLSI	Computers	(1980–????)
1.5.6				Moore’s	Law
1.6				The	Computer	Level	Hierarchy
1.7				Cloud	Computing:	Computing	as	a	Service
1.8				The	Von	Neumann	Model
1.9				Non–Von	Neumann	Models
1.10			Parallel	Processors	and	Parallel	Computing
1.11			Parallelism:	Enabler	of	Machine	Intelligence—Deep	Blue	and	Watson
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	2								Data	Representation	in	Computer	Systems
2.1				Introduction
2.2				Positional	Numbering	Systems
2.3				Converting	Between	Bases
2.3.1				Converting	Unsigned	Whole	Numbers
2.3.2				Converting	Fractions
2.3.3				Converting	Between	Power-of-Two	Radices
2.4				Signed	Integer	Representation
2.4.1				Signed	Magnitude

2.4.2				Complement	Systems
2.4.3				Excess-M	Representation	for	Signed	Numbers
2.4.4				Unsigned	Versus	Signed	Numbers
2.4.5				Computers,	Arithmetic,	and	Booth’s	Algorithm
2.4.6				Carry	Versus	Overflow
2.4.7				Binary	Multiplication	and	Division	Using	Shifting
2.5				Floating-Point	Representation
2.5.1				A	Simple	Model
2.5.2				Floating-Point	Arithmetic
2.5.3				Floating-Point	Errors
2.5.4				The	IEEE-754	Floating-Point	Standard
2.5.5				Range,	Precision,	and	Accuracy
2.5.6				Additional	Problems	with	Floating-Point	Numbers
2.6				Character	Codes
2.6.1				Binary-Coded	Decimal
2.6.2				EBCDIC
2.6.3				ASCII
2.6.4				Unicode
2.7				Error	Detection	and	Correction
2.7.1				Cyclic	Redundancy	Check
2.7.2				Hamming	Codes
2.7.3				Reed-Solomon
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
Focus	on	Codes	for	Data	Recording	and	Transmission
2A.1				Non-Return-to-Zero	Code
2A.2				Non-Return-to-Zero-Invert	Code
2A.3				Phase	Modulation	(Manchester	Code)
2A.4				Frequency	Modulation
2A.5				Run-Length-Limited	Code
2A.6				Partial	Response	Maximum	Likelihood	Coding
2A.7				Summary
Exercises
CHAPTER	3								Boolean	Algebra	and	Digital	Logic
3.1				Introduction

3.2				Boolean	Algebra
3.2.1				Boolean	Expressions
3.2.2				Boolean	Identities
3.2.3				Simplification	of	Boolean	Expressions
3.2.4				Complements
3.2.5				Representing	Boolean	Functions
3.3				Logic	Gates
3.3.1				Symbols	for	Logic	Gates
3.3.2				Universal	Gates
3.3.3				Multiple	Input	Gates
3.4				Digital	Components
3.4.1				Digital	Circuits	and	Their	Relationship	to	Boolean	Algebra
3.4.2				Integrated	Circuits
3.4.3				Putting	It	All	Together:	From	Problem	Description	to	Circuit
3.5				Combinational	Circuits
3.5.1				Basic	Concepts
3.5.2				Examples	of	Typical	Combinational	Circuits
3.6				Sequential	Circuits
3.6.1				Basic	Concepts
3.6.2				Clocks
3.6.3				Flip-Flops
3.6.4				Finite	State	Machines
3.6.5				Examples	of	Sequential	Circuits
3.6.6				An	Application	of	Sequential	Logic:	Convolutional	Coding	and	Viterbi
Detection
3.7				Designing	Circuits
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
Focus	on	Karnaugh	Maps
3A.1				Introduction
3A.2				Description	of	Kmaps	and	Terminology
3A.3				Kmap	Simplification	for	Two	Variables
3A.4				Kmap	Simplification	for	Three	Variables
3A.5				Kmap	Simplification	for	Four	Variables
3A.6				Don’t	Care	Conditions
3A.7				Summary

Exercises
CHAPTER	4								MARIE:	An	Introduction	to	a	Simple	Computer
4.1				Introduction
4.2				CPU	Basics	and	Organization
4.2.1				The	Registers
4.2.2				The	ALU
4.2.3				The	Control	Unit
4.3				The	Bus
4.4				Clocks
4.5				The	Input/Output	Subsystem
4.6				Memory	Organization	and	Addressing
4.7				Interrupts
4.8				MARIE
4.8.1				The	Architecture
4.8.2				Registers	and	Buses
4.8.3				Instruction	Set	Architecture
4.8.4				Register	Transfer	Notation
4.9				Instruction	Processing
4.9.1				The	Fetch–Decode–Execute	Cycle
4.9.2				Interrupts	and	the	Instruction	Cycle
4.9.3				MARIE’s	I/O
4.10				A	Simple	Program
4.11				A	Discussion	on	Assemblers
4.11.1				What	Do	Assemblers	Do?
4.11.2				Why	Use	Assembly	Language?
4.12				Extending	Our	Instruction	Set
4.13				A	Discussion	on	Decoding:	Hardwired	Versus	Microprogrammed	Control
4.13.1				Machine	Control
4.13.2				Hardwired	Control
4.13.3				Microprogrammed	Control
4.14				Real-World	Examples	of	Computer	Architectures
4.14.1				Intel	Architectures
4.14.2				MIPS	Architectures
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises

CHAPTER	5								A	Closer	Look	at	Instruction	Set	Architectures
5.1				Introduction
5.2				Instruction	Formats
5.2.1				Design	Decisions	for	Instruction	Sets
5.2.2				Little	Versus	Big	Endian
5.2.3				Internal	Storage	in	the	CPU:	Stacks	Versus	Registers
5.2.4				Number	of	Operands	and	Instruction	Length
5.2.5				Expanding	Opcodes
5.3				Instruction	Types
5.3.1				Data	Movement
5.3.2				Arithmetic	Operations
5.3.3				Boolean	Logic	Instructions
5.3.4				Bit	Manipulation	Instructions
5.3.5				Input/Output	Instructions
5.3.6				Instructions	for	Transfer	of	Control
5.3.7				Special-Purpose	Instructions
5.3.8				Instruction	Set	Orthogonality
5.4				Addressing
5.4.1				Data	Types
5.4.2				Address	Modes
5.5				Instruction	Pipelining
5.6				Real-World	Examples	of	ISAs
5.6.1				Intel
5.6.2				MIPS
5.6.3				Java	Virtual	Machine
5.6.4				ARM
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	6								Memory
6.1				Introduction
6.2				Types	of	Memory
6.3				The	Memory	Hierarchy
6.3.1				Locality	of	Reference
6.4				Cache	Memory

6.4.1				Cache	Mapping	Schemes
6.4.2				Replacement	Policies
6.4.3				Effective	Access	Time	and	Hit	Ratio
6.4.4				When	Does	Caching	Break	Down?
6.4.5				Cache	Write	Policies
6.4.6				Instruction	and	Data	Caches
6.4.7				Levels	of	Cache
6.5				Virtual	Memory
6.5.1				Paging
6.5.2				Effective	Access	Time	Using	Paging
6.5.3				Putting	It	All	Together:	Using	Cache,	TLBs,	and	Paging
6.5.4				Advantages	and	Disadvantages	of	Paging	and	Virtual	Memory
6.5.5				Segmentation
6.5.6				Paging	Combined	with	Segmentation
6.6				A	Real-World	Example	of	Memory	Management
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	7								Input/Output	and	Storage	Systems
7.1				Introduction
7.2				I/O	and	Performance
7.3				Amdahl’	s	Law
7.4				I/O	Architectures
7.4.1				I/O	Control	Methods
7.4.2				Character	I/O	Versus	Block	I/O
7.4.3				I/O	Bus	Operation
7.5				Data	Transmission	Modes
7.5.1				Parallel	Data	Transmission
7.5.2				Serial	Data	Transmission
7.6				Magnetic	Disk	Technology
7.6.1				Rigid	Disk	Drives
7.6.2				Solid	State	Drives
7.7				Optical	Disks
7.7.1				CD-ROM
7.7.2				DVD
7.7.3				Blue-Violet	Laser	Discs

7.7.4				Optical	Disk	Recording	Methods
7.8				Magnetic	Tape
7.9				RAID
7.9.1				RAID	Level	0
7.9.2				RAID	Level	1
7.9.3				RAID	Level	2
7.9.4				RAID	Level	3
7.9.5				RAID	Level	4
7.9.6				RAID	Level	5
7.9.7				RAID	Level	6
7.9.8				RAID	DP
7.9.9				Hybrid	RAID	Systems
7.10				The	Future	of	Data	Storage
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
Focus	on	Data	Compression
7A.1				Introduction
7A.2				Statistical	Coding
7A.2.1				Huffman	Coding
7A.2.2				Arithmetic	Coding
7A.3				Ziv-Lempel	(LZ)	Dictionary	Systems
7A.4				GIF	and	PNG	Compression
7A.5				JPEG	Compression
7A.6				MP3	Compression
7A.7				Summary
Further	Reading
References
Exercises
CHAPTER	8								System	Software
8.1				Introduction
8.2				Operating	Systems
8.2.1				Operating	Systems	History
8.2.2				Operating	System	Design
8.2.3				Operating	System	Services
8.3				Protected	Environments

8.3.1				Virtual	Machines
8.3.2				Subsystems	and	Partitions
8.3.3				Protected	Environments	and	the	Evolution	of	Systems	Architectures
8.4				Programming	Tools
8.4.1				Assemblers	and	Assembly
8.4.2				Link	Editors
8.4.3				Dynamic	Link	Libraries
8.4.4				Compilers
8.4.5				Interpreters
8.5				Java:	All	of	the	Above
8.6				Database	Software
8.7				Transaction	Managers
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	9								Alternative	Architectures
9.1				Introduction
9.2				RISC	Machines
9.3				Flynn’s	Taxonomy
9.4				Parallel	and	Multiprocessor	Architectures
9.4.1				Superscalar	and	VLIW
9.4.2				Vector	Processors
9.4.3				Interconnection	Networks
9.4.4				Shared	Memory	Multiprocessors
9.4.5				Distributed	Computing
9.5				Alternative	Parallel	Processing	Approaches
9.5.1				Dataflow	Computing
9.5.2				Neural	Networks
9.5.3				Systolic	Arrays
9.6				Quantum	Computing
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises

CHAPTER	10								Topics	in	Embedded	Systems
10.1				Introduction
10.2				An	Overview	of	Embedded	Hardware
10.2.1				Off-the-Shelf	Embedded	System	Hardware
10.2.2				Configurable	Hardware
10.2.3				Custom-Designed	Embedded	Hardware
10.3				An	Overview	of	Embedded	Software
10.3.1				Embedded	Systems	Memory	Organization
10.3.2				Embedded	Operating	Systems
10.3.3				Embedded	Systems	Software	Development
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	11								Performance	Measurement	and	Analysis
11.1				Introduction
11.2				Computer	Performance	Equations
11.3				Mathematical	Preliminaries
11.3.1				What	the	Means	Mean
11.3.2				The	Statistics	and	Semantics
11.4				Benchmarking
11.4.1				Clock	Rate,	MIPS,	and	FLOPS
11.4.2				Synthetic	Benchmarks:	Whetstone,	Linpack,	and	Dhrystone
11.4.3				Standard	Performance	Evaluation	Corporation	Benchmarks
11.4.4				Transaction	Processing	Performance	Council	Benchmarks
11.4.5				System	Simulation
11.5				CPU	Performance	Optimization
11.5.1				Branch	Optimization
11.5.2				Use	of	Good	Algorithms	and	Simple	Code
11.6				Disk	Performance
11.6.1				Understanding	the	Problem
11.6.2				Physical	Considerations
11.6.3				Logical	Considerations
Chapter	Summary
Further	Reading
References

Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	12									Network	Organization	and	Architecture
12.1				Introduction
12.2				Early	Business	Computer	Networks
12.3				Early	Academic	and	Scientific	Networks:	The	Roots	and	Architecture	of
the	Internet
12.4				Network	Protocols	I:	ISO/OSI	Protocol	Unification
12.4.1				A	Parable
12.4.2				The	OSI	Reference	Model
12.5				Network	Protocols	II:	TCP/IP	Network	Architecture
12.5.1				The	IP	Layer	for	Version	4
12.5.2				The	Trouble	with	IP	Version	4
12.5.3				Transmission	Control	Protocol
12.5.4				The	TCP	Protocol	at	Work
12.5.5				IP	Version	6
12.6				Network	Organization
12.6.1				Physical	Transmission	Media
12.6.2				Interface	Cards
12.6.3				Repeaters
12.6.4				Hubs
12.6.5				Switches
12.6.6				Bridges	and	Gateways
12.6.7				Routers	and	Routing
12.7				The	Fragility	of	the	Internet
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	13								Selected	Storage	Systems	and	Interfaces
13.1				Introduction
13.2				SCSI	Architecture
13.2.1				“Classic”	Parallel	SCSI
13.2.2				The	SCSI	Architecture	Model-3
13.3				Internet	SCSI
13.4				Storage	Area	Networks

13.5				Other	I/O	Connections
13.5.1				Parallel	Buses:	XT	to	ATA
13.5.2				Serial	ATA	and	Serial	Attached	SCSI
13.5.3				Peripheral	Component	Interconnect
13.5.4				A	Serial	Interface:	USB
13.6				Cloud	Storage
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
APPENDIX	A							Data	Structures	and	the	Computer
A.1				Introduction
A.2				Fundamental	Structures
A.2.1				Arrays
A.2.2				Queues	and	Linked	Lists
A.2.3				Stacks
A.3				Trees
A.4				Network	Graphs
Summary
Further	Reading
References
Exercises
Glossary
Answers	and	Hints	for	Selected	Exercises
Index

Preface
TO	THE	STUDENT
This	is	a	book	about	computer	organization	and	architecture.	It	focuses	on	the	function	and	design	of	the
various	components	necessary	to	process	information	digitally.	We	present	computing	systems	as	a	series
of	 layers,	 starting	 with	 low-level	 hardware	 and	 progressing	 to	 higher-level	 software,	 including
assemblers	and	operating	systems.	These	levels	constitute	a	hierarchy	of	virtual	machines.	The	study	of
computer	organization	focuses	on	this	hierarchy	and	the	issues	involved	with	how	we	partition	the	levels
and	how	each	level	is	implemented.	The	study	of	computer	architecture	focuses	on	the	interface	between
hardware	 and	 software,	 and	 emphasizes	 the	 structure	 and	 behavior	 of	 the	 system.	 The	 majority	 of
information	 contained	 in	 this	 textbook	 is	 devoted	 to	 computer	 hardware,	 computer	 organization	 and
architecture,	and	their	relationship	to	software	performance.
Students	 invariably	 ask,	 “Why,	 if	 I	 am	 a	 computer	 science	 major,	 must	 I	 learn	 about	 computer
hardware?	Isn’t	that	for	computer	engineers?	Why	do	I	care	what	the	inside	of	a	computer	looks	like?”	As
computer	users,	we	probably	do	not	have	to	worry	about	this	any	more	than	we	need	to	know	what	our
cars	look	like	under	the	hood	in	order	to	drive	them.	We	can	certainly	write	high-level	language	programs
without	understanding	how	these	programs	execute;	we	can	use	various	application	packages	without
understanding	how	they	really	work.	But	what	happens	when	the	program	we	have	written	needs	to	be
faster	 and	 more	 efficient,	 or	 the	 application	 we	 are	 using	 doesn’t	 do	 precisely	 what	 we	want?	 As
computer	scientists,	we	need	a	basic	understanding	of	the	computer	system	itself	in	order	to	rectify	these
problems.
There	 is	 a	 fundamental	 relationship	 between	 the	 computer	 hardware	 and	 the	 many	 aspects	 of
programming	and	software	components	in	computer	systems.	In	order	to	write	good	software,	it	is	very
important	to	understand	the	computer	system	as	a	whole.	Understanding	hardware	can	help	you	explain
the	mysterious	errors	that	sometimes	creep	into	your	programs,	such	as	the	infamous	segmentation	fault	or
bus	error.	The	level	of	knowledge	about	computer	organization	and	computer	architecture	that	a	high-
level	programmer	must	have	depends	on	the	task	the	high-level	programmer	is	attempting	to	complete.
For	 example,	 to	 write	 compilers,	 you	 must	 understand	 the	 particular	 hardware	 to	 which	 you	 are
compiling.	 Some	 of	 the	 ideas	 used	 in	 hardware	 (such	 as	 pipelining)	 can	 be	 adapted	 to	 compilation
techniques,	 thus	 making	 the	 compiler	 faster	 and	 more	 efficient.	 To	 model	 large,	 complex,	 real-world
systems,	 you	 must	 understand	 how	 floating-point	 arithmetic	 should,	 and	 does,	 work	 (which	 are	 not
necessarily	the	same	thing).	To	write	device	drivers	for	video,	disks,	or	other	I/O	devices,	you	need	a
good	 understanding	 of	 I/O	 interfacing	 and	 computer	 architecture	 in	 general.	 If	 you	 want	 to	 work	 on
embedded	systems,	which	are	usually	very	resource	constrained,	you	must	understand	all	of	the	time,
space,	 and	 price	 trade-offs.	 To	 do	 research	 on,	 and	 make	 recommendations	 for,	 hardware	 systems,
networks,	or	specific	algorithms,	you	must	acquire	an	understanding	of	benchmarking	and	then	learn	how
to	present	performance	results	adequately.	Before	buying	hardware,	you	need	to	understand	benchmarking
and	all	the	ways	that	others	can	manipulate	the	performance	results	to	“prove”	that	one	system	is	better
than	another.	Regardless	of	our	particular	area	of	expertise,	as	computer	scientists,	it	is	imperative	that
we	understand	how	hardware	interacts	with	software.
You	may	also	be	wondering	why	a	book	with	the	word	essentials	in	its	title	is	so	large.	The	reason	is

twofold.	First,	the	subject	of	computer	organization	is	expansive	and	it	grows	by	the	day.	Second,	there	is
little	agreement	as	to	which	topics	from	within	this	burgeoning	sea	of	information	are	truly	essential	and
which	are	just	helpful	to	know.	In	writing	this	book,	one	goal	was	to	provide	a	concise	text	compliant
with	the	computer	architecture	curriculum	guidelines	jointly	published	by	the	Association	for	Computing
Machinery	 (ACM)	 and	 the	 Institute	 of	 Electrical	 and	 Electronic	 Engineers	 (IEEE).	 These	 guidelines
encompass	 the	 subject	 matter	 that	 experts	 agree	 constitutes	 the	 “essential”	 core	 body	 of	 knowledge
relevant	to	the	subject	of	computer	organization	and	architecture.
We	have	augmented	the	ACM/IEEE	recommendations	with	subject	matter	that	we	feel	is	useful—if
not	essential—to	your	continuing	computer	science	studies	and	to	your	professional	advancement.	The
topics	that	we	feel	will	help	you	in	your	continuing	computer	science	studies	include	operating	systems,
compilers,	database	management,	and	data	communications.	Other	subjects	are	included	because	they	will
help	you	understand	how	actual	systems	work	in	real	life.
We	hope	that	you	find	reading	this	book	an	enjoyable	experience,	and	that	you	take	time	to	delve
deeper	into	some	of	the	material	that	we	have	presented.	It	is	our	intention	that	this	book	will	serve	as	a
useful	reference	long	after	your	formal	course	is	complete.	Although	we	give	you	a	substantial	amount	of
information,	it	is	only	a	foundation	upon	which	you	can	build	throughout	the	remainder	of	your	studies	and
your	career.	Successful	computer	professionals	continually	add	to	their	knowledge	about	how	computers
work.	Welcome	to	the	start	of	your	journey.
TO	THE	INSTRUCTOR
This	book	is	the	outgrowth	of	two	computer	science	organization	and	architecture	classes	taught	at	Penn
State	Harrisburg.	As	the	computer	science	curriculum	evolved,	we	found	it	necessary	not	only	to	modify
the	material	taught	in	the	courses,	but	also	to	condense	the	courses	from	a	two-semester	sequence	into	a
three-credit,	 one-semester	 course.	 Many	 other	 schools	 have	 also	 recognized	 the	 need	 to	 compress
material	 in	 order	 to	 make	 room	 for	 emerging	 topics.	 This	 new	 course,	 as	 well	 as	 this	 textbook,	 is
primarily	for	computer	science	majors	and	is	intended	to	address	the	topics	in	computer	organization	and
architecture	 with	 which	 computer	 science	 majors	 must	 be	 familiar.	 This	 book	 not	 only	 integrates	 the
underlying	principles	in	these	areas,	but	it	also	introduces	and	motivates	the	topics,	providing	the	breadth
necessary	for	majors	while	providing	the	depth	necessary	for	continuing	studies	in	computer	science.
Our	 primary	 objective	 in	 writing	 this	 book	 was	 to	 change	 the	 way	 computer	 organization	 and
architecture	are	typically	taught.	A	computer	science	major	should	leave	a	computer	organization	and
architecture	class	with	not	only	an	understanding	of	the	important	general	concepts	on	which	the	digital
computer	is	founded,	but	also	with	a	comprehension	of	how	those	concepts	apply	to	the	real	world.	These
concepts	should	transcend	vendor-specific	terminology	and	design;	in	fact,	students	should	be	able	to	take
concepts	 given	 in	 the	 specific	 and	 translate	 to	 the	 generic	 and	 vice	 versa.	 In	 addition,	 students	 must
develop	a	firm	foundation	for	further	study	in	the	major.
The	title	of	our	book,	The	Essentials	of	Computer	Organization	and	Architecture,	is	intended	to
convey	that	the	topics	presented	in	the	text	are	those	for	which	every	computer	science	major	should	have
exposure,	familiarity,	or	mastery.	We	do	not	expect	students	using	our	textbook	to	have	complete	mastery
of	all	topics	presented.	It	is	our	firm	belief,	however,	that	there	are	certain	topics	that	must	be	mastered;
there	are	those	topics	about	which	students	must	have	a	definite	familiarity;	and	there	are	certain	topics
for	which	a	brief	introduction	and	exposure	are	adequate.
We	 do	 not	 feel	 that	 concepts	 presented	 in	 sufficient	 depth	 can	 be	 learned	 by	 studying	 general
principles	in	isolation.	We	therefore	present	the	topics	as	an	integrated	set	of	solutions,	not	simply	a

collection	of	individual	pieces	of	information.	We	feel	our	explanations,	examples,	exercises,	tutorials,
and	simulators	all	combine	to	provide	the	student	with	a	total	learning	experience	that	exposes	the	inner
workings	of	a	modern	digital	computer	at	the	appropriate	level.
We	have	written	this	textbook	in	an	informal	style,	omitting	unnecessary	jargon,	writing	clearly	and
concisely,	and	avoiding	unnecessary	abstraction,	in	hopes	of	increasing	student	enthusiasm.	We	have	also
broadened	the	range	of	topics	typically	found	in	a	first-level	architecture	book	to	include	system	software,
a	brief	tour	of	operating	systems,	performance	issues,	alternative	architectures,	and	a	concise	introduction
to	networking,	as	these	topics	are	intimately	related	to	computer	hardware.	Like	most	books,	we	have
chosen	an	architectural	model,	but	it	is	one	that	we	have	designed	with	simplicity	in	mind.
Relationship	to	CS2013
In	October	2013,	the	ACM/IEEE	Joint	Task	Force	unveiled	Computer	Science	Curricula	2013	(CS2013).
Although	 we	 are	 primarily	 concerned	 with	 the	 Computer	 Architecture	 knowledge	 area,	 these	 new
guidelines	 suggest	 integrating	 the	 core	 knowledge	 throughout	 the	 curriculum.	 Therefore,	 we	 also	 call
attention	to	additional	knowledge	areas	beyond	architecture	that	are	addressed	in	this	book.
CS2013	 is	 a	 comprehensive	 revision	 of	 CS2008,	 mostly	 the	 result	 of	 focusing	 on	 the	essential
concepts	 in	 the	 Computer	 Science	 curriculum	 while	 still	 being	 flexible	 enough	 to	 meet	 individual
institutional	 needs.	 These	 guidelines	 introduce	 the	 notion	 of	 Core	 Tier-1	 and	 Core	 Tier-2	 topics,	 in
addition	to	elective	topics.	Core	Tier-1	topics	are	those	that	should	be	part	of	every	Computer	Science
curriculum.	Core	Tier-2	topics	are	those	that	are	considered	essential	enough	that	a	Computer	Science
curriculum	should	contain	90–100%	of	these	topics.	Elective	topics	are	those	that	allow	curricula	to
provide	breadth	and	depth.	The	suggested	coverage	for	each	topic	is	listed	in	lecture	hours.
The	main	change	in	the	Architecture	and	Organization	(AR)	knowledge	area	from	CS2008	to	CS2013
is	a	reduction	of	lecture	hours	from	36	to	16;	however,	a	new	area,	System	Fundamentals	(SF),	has	been
introduced	and	includes	some	concepts	previously	found	in	the	AR	module	(including	hardware	building
blocks	 and	 architectural	 organization).	 The	 interested	 reader	 is	 referred	 to	 the	 CS2013	 guidelines
(http://www.acm.org/education/curricula-recommendations)	for	more	information	on	what	the	individual
knowledge	areas	include.
We	are	pleased	that	the	fourth	edition	of	The	Essentials	of	Computer	Organization	and	Architecture
is	 in	 direct	 correlation	 with	 the	 ACM/IEEE	 CS2013	 guidelines	 for	 computer	 organization	 and
architecture,	 in	 addition	 to	 integrating	 material	 from	 additional	 knowledge	 units.	Table	 P.1	 indicates
which	chapters	of	this	textbook	satisfy	the	eight	topics	listed	in	the	AR	knowledge	area.	For	the	other
knowledge	areas,	only	the	topics	that	are	covered	in	this	textbook	are	listed.

TABLE	P.1	ACM/IEEE	CS2013	Topics	Covered	in	This	Book
Why	Another	Text?
No	one	can	deny	there	is	a	plethora	of	textbooks	for	teaching	computer	organization	and	architecture
already	on	the	market.	In	our	35-plus	years	of	teaching	these	courses,	we	have	used	many	very	good
textbooks.	However,	each	time	we	have	taught	the	course,	the	content	has	evolved,	and	eventually,	we
discovered	we	were	writing	significantly	more	course	notes	to	bridge	the	gap	between	the	material	in	the
textbook	 and	 the	 material	 we	 deemed	 necessary	 to	 present	 in	 our	 classes.	 We	 found	 that	 our	 course
material	was	migrating	from	a	computer	engineering	approach	to	organization	and	architecture	toward	a
computer	science	approach	to	these	topics.	When	the	decision	was	made	to	fold	the	organization	class
and	the	architecture	class	into	one	course,	we	simply	could	not	find	a	textbook	that	covered	the	material
we	felt	was	necessary	for	our	majors,	written	from	a	computer	science	point	of	view,	written	without
machine-specific	terminology,	and	designed	to	motivate	the	topics	before	covering	them.

In	this	textbook,	we	hope	to	convey	the	spirit	of	design	used	in	the	development	of	modern	computing
systems	and	what	effect	this	has	on	computer	science	students.	Students,	however,	must	have	a	strong
understanding	of	the	basic	concepts	before	they	can	understand	and	appreciate	the	intangible	aspects	of
design.	Most	organization	and	architecture	textbooks	present	a	similar	subset	of	technical	information
regarding	these	basics.	We,	however,	pay	particular	attention	to	the	level	at	which	the	information	should
be	covered,	and	to	presenting	that	information	in	the	context	that	has	relevance	for	computer	science
students.	For	example,	throughout	this	book,	when	concrete	examples	are	necessary,	we	offer	examples
for	personal	computers,	enterprise	systems,	and	mainframes,	as	these	are	the	types	of	systems	most	likely
to	be	encountered.	We	avoid	the	“PC	bias”	prevalent	in	similar	books	in	the	hope	that	students	will	gain
an	 appreciation	 for	 the	 differences,	 the	 similarities,	 and	 the	 roles	 various	 platforms	 play	 in	 today’s
automated	 infrastructures.	 Too	 often,	 textbooks	 forget	 that	 motivation	 is,	 perhaps,	 the	 single	 most
important	 key	 in	 learning.	 To	 that	 end,	 we	 include	 many	 real-world	 examples,	 while	 attempting	 to
maintain	a	balance	between	theory	and	application.
Features
We	 have	 included	 many	 features	 in	 this	 textbook	 to	 emphasize	 the	 various	 concepts	 in	 computer
organization	and	architecture,	and	to	make	the	material	more	accessible	to	students.	Some	of	the	features
are:
•		Sidebars.	These	sidebars	include	interesting	tidbits	of	information	that	go	a	step	beyond	the	main	focus
of	the	chapter,	thus	allowing	readers	to	delve	further	into	the	material.
•		Real-World	Examples.	We	have	integrated	the	textbook	with	examples	from	real	life	to	give	students	a
better	understanding	of	how	technology	and	techniques	are	combined	for	practical	purposes.
•		Chapter	Summaries.	These	sections	provide	brief	yet	concise	summaries	of	the	main	points	in	each
chapter.
•		Further	Reading.	These	sections	list	additional	sources	for	those	readers	who	wish	to	investigate	any
of	the	topics	in	more	detail,	and	contain	references	to	definitive	papers	and	books	related	to	the	chapter
topics.
•		Review	Questions.	Each	chapter	contains	a	set	of	review	questions	designed	to	ensure	that	the	reader
has	a	firm	grasp	of	the	material.
•		Chapter	Exercises.	Each	chapter	has	a	broad	selection	of	exercises	to	reinforce	the	ideas	presented.
More	challenging	exercises	are	marked	with	an	asterisk.
•		Answers	to	Selected	Exercises.	To	ensure	that	students	are	on	the	right	track,	we	provide	answers	to
representative	questions	from	each	chapter.	Questions	with	answers	in	the	back	of	the	text	are	marked
with	a	blue	diamond.
•		Special	“Focus	On”	Sections.	These	sections	provide	additional	information	for	instructors	who	may
wish	 to	 cover	 certain	 concepts,	 such	 as	 Kmaps	 and	 data	 compression,	 in	 more	 detail.	 Additional
exercises	are	provided	for	these	sections	as	well.
•		Appendix.	The	appendix	provides	a	brief	introduction	or	review	of	data	structures,	including	topics
such	as	stacks,	linked	lists,	and	trees.
•		Glossary.	An	extensive	glossary	includes	brief	definitions	of	all	key	terms	from	the	chapters.
•		Index.	An	exhaustive	index	is	provided	with	this	book,	with	multiple	cross-references,	to	make	finding
terms	and	concepts	easier	for	the	reader.

About	the	Authors
We	bring	to	this	textbook	not	only	35-plus	years	of	combined	teaching	experience,	but	also	30-plus	years
of	 industry	 experience.	 Our	 combined	 efforts	 therefore	 stress	 the	 underlying	 principles	 of	 computer
organization	and	architecture	and	how	these	topics	relate	in	practice.	We	include	real-life	examples	to
help	students	appreciate	how	these	fundamental	concepts	are	applied	in	the	world	of	computing.
Linda	Null	holds	a	PhD	in	computer	science	from	Iowa	State	University,	an	MS	in	computer	science
from	 Iowa	 State	 University,	 an	 MS	 in	 computer	 science	 education	 from	 Northwest	 Missouri	 State
University,	 an	 MS	 in	 mathematics	 education	 from	 Northwest	 Missouri	 State	 University,	 and	 a	 BS	 in
mathematics	and	English	from	Northwest	Missouri	State	University.	She	has	been	teaching	mathematics
and	computer	science	for	more	than	35	years	and	is	currently	the	computer	science	graduate	program
coordinator	and	associate	program	chair	at	the	Pennsylvania	State	University	Harrisburg	campus,	where
she	has	been	a	member	of	the	faculty	since	1995.	She	has	received	numerous	teaching	awards	including
the	Penn	State	Teaching	Fellow	Award	and	the	Teaching	Excellence	Award.	Her	areas	of	interest	include
computer	 organization	 and	 architecture,	 operating	 systems,	 computer	 science	 education,	 and	 computer
security.
Julia	Lobur	has	been	a	practitioner	in	the	computer	industry	for	more	than	30	years.	She	has	held
positions	 as	 systems	 consultant,	 staff	 programmer/analyst,	 systems	 and	 network	 designer,	 software
development	manager,	and	project	manager,	in	addition	to	part-time	teaching	duties.	Julia	holds	an	MS	in
computer	science	and	is	an	IEEE	Certified	Software	Development	Professional.
Prerequisites
The	 typical	 background	 necessary	 for	 a	 student	 using	 this	 textbook	 includes	 a	 year	 of	 programming
experience	using	a	high-level	procedural	language.	Students	are	also	expected	to	have	taken	a	year	of
college-level	mathematics	(calculus	or	discrete	mathematics),	as	this	textbook	assumes	and	incorporates
these	mathematical	concepts.	This	book	assumes	no	prior	knowledge	of	computer	hardware.
A	computer	organization	and	architecture	class	is	customarily	a	prerequisite	for	an	undergraduate
operating	systems	class	(students	must	know	about	the	memory	hierarchy,	concurrency,	exceptions,	and
interrupts),	 compilers	 (students	 must	 know	 about	 instruction	 sets,	 memory	 addressing,	 and	 linking),
networking	 (students	 must	 understand	 the	 hardware	 of	 a	 system	 before	 attempting	 to	 understand	 the
network	that	ties	these	components	together),	and	of	course,	any	advanced	architecture	class.	This	text
covers	the	topics	necessary	for	these	courses.
General	Organization	and	Coverage
Our	presentation	of	concepts	in	this	textbook	is	an	attempt	at	a	concise	yet	thorough	coverage	of	the	topics
we	 feel	 are	 essential	 for	 the	 computer	 science	 major.	 We	 do	 not	 feel	 the	 best	 way	 to	 do	 this	 is	 by
“compartmentalizing”	the	various	topics;	therefore,	we	have	chosen	a	structured	yet	integrated	approach
where	each	topic	is	covered	in	the	context	of	the	entire	computer	system.
As	with	many	popular	texts,	we	have	taken	a	bottom-up	approach,	starting	with	the	digital	logic	level
and	building	to	the	application	level	that	students	should	be	familiar	with	before	starting	the	class.	The
text	is	carefully	structured	so	that	the	reader	understands	one	level	before	moving	on	to	the	next.	By	the
time	the	reader	reaches	the	application	level,	all	the	necessary	concepts	in	computer	organization	and
architecture	have	been	presented.	Our	goal	is	to	allow	the	students	to	tie	the	hardware	knowledge	covered
in	this	book	to	the	concepts	learned	in	their	introductory	programming	classes,	resulting	in	a	complete	and

thorough	 picture	 of	 how	 hardware	 and	 software	 fit	 together.	 Ultimately,	 the	 extent	 of	 hardware
understanding	has	a	significant	influence	on	software	design	and	performance.	If	students	can	build	a	firm
foundation	 in	 hardware	 fundamentals,	 this	 will	 go	 a	 long	 way	 toward	 helping	 them	 to	 become	 better
computer	scientists.
The	concepts	in	computer	organization	and	architecture	are	integral	to	many	of	the	everyday	tasks	that
computer	professionals	perform.	To	address	the	numerous	areas	in	which	a	computer	professional	should
be	educated,	we	have	taken	a	high-level	look	at	computer	architecture,	providing	low-level	coverage	only
when	deemed	necessary	for	an	understanding	of	a	specific	concept.	For	example,	when	discussing	ISAs,
many	 hardware-dependent	 issues	 are	 introduced	 in	 the	 context	 of	 different	 case	 studies	 to	 both
differentiate	and	reinforce	the	issues	associated	with	ISA	design.
The	text	is	divided	into	13	chapters	and	an	appendix,	as	follows:
•		Chapter	1	provides	a	historical	overview	of	computing	in	general,	pointing	out	the	many	milestones	in
the	development	of	computing	systems	and	allowing	the	reader	to	visualize	how	we	arrived	at	the
current	state	of	computing.	This	chapter	introduces	the	necessary	terminology,	the	basic	components	in	a
computer	 system,	 the	 various	 logical	 levels	 of	 a	 computer	 system,	 and	 the	 von	 Neumann	 computer
model.	It	provides	a	high-level	view	of	the	computer	system,	as	well	as	the	motivation	and	necessary
concepts	for	further	study.
•		Chapter	2	provides	thorough	coverage	of	the	various	means	computers	use	to	represent	both	numerical
and	 character	 information.	 Addition,	 subtraction,	 multiplication,	 and	 division	 are	 covered	 once	 the
reader	has	been	exposed	to	number	bases	and	the	typical	numeric	representation	techniques,	including
one’s	complement,	two’s	complement,	and	BCD.	In	addition,	EBCDIC,	ASCII,	and	Unicode	character
representations	are	addressed.	Fixed-	and	floating-point	representation	are	also	introduced.	Codes	for
data	recording	and	error	detection	and	correction	are	covered	briefly.	Codes	for	data	transmission	and
recording	are	described	in	a	special	“Focus	On”	section.
•		Chapter	3	is	a	classic	presentation	of	digital	logic	and	how	it	relates	to	Boolean	algebra.	This	chapter
covers	both	combinational	and	sequential	logic	in	sufficient	detail	to	allow	the	reader	to	understand	the
logical	makeup	of	more	complicated	MSI	(medium-scale	integration)	circuits	(such	as	decoders).	More
complex	circuits,	such	as	buses	and	memory,	are	also	included.	We	have	included	optimization	and
Kmaps	in	a	special	“Focus	On”	section.
•		Chapter	4	illustrates	basic	computer	organization	and	introduces	many	fundamental	concepts,	including
the	 fetch–decode–execute	 cycle,	 the	 data	 path,	 clocks	 and	 buses,	 register	 transfer	 notation,	 and,	 of
course,	the	CPU.	A	very	simple	architecture,	MARIE,	and	its	ISA	are	presented	to	allow	the	reader	to
gain	a	full	understanding	of	the	basic	architectural	organization	involved	in	program	execution.	MARIE
exhibits	the	classic	von	Neumann	design	and	includes	a	program	counter,	an	accumulator,	an	instruction
register,	 4096	 bytes	 of	 memory,	 and	 two	 addressing	 modes.	 Assembly	 language	 programming	 is
introduced	to	reinforce	the	concepts	of	instruction	format,	instruction	mode,	data	format,	and	control
that	are	presented	earlier.	This	is	not	an	assembly	language	textbook	and	was	not	designed	to	provide	a
practical	course	in	assembly	language	programming.	The	primary	objective	in	introducing	assembly	is
to	further	the	understanding	of	computer	architecture	in	general.	However,	a	simulator	for	MARIE	is
provided	 so	 assembly	 language	 programs	 can	 be	 written,	 assembled,	 and	 run	 on	 the	 MARIE
architecture.	 The	 two	 methods	 of	 control,	 hardwiring	 and	 microprogramming,	 are	 introduced	 and
compared	in	this	chapter.	Finally,	Intel	and	MIPS	architectures	are	compared	to	reinforce	the	concepts
in	the	chapter.

•	 	Chapter	 5	 provides	 a	 closer	 look	 at	 instruction	 set	 architectures,	 including	 instruction	 formats,
instruction	types,	and	addressing	modes.	Instruction-level	pipelining	is	introduced	as	well.	Real-world
ISAs	 (including	 Intel®,	 MIPS®	 Technologies,	 ARM,	 and	 Java™)	 are	 presented	 to	 reinforce	 the
concepts	presented	in	the	chapter.
•		Chapter	6	covers	basic	memory	concepts,	such	as	RAM	and	the	various	memory	devices,	and	also
addresses	the	more	advanced	concepts	of	the	memory	hierarchy,	including	cache	memory	and	virtual
memory.	This	chapter	gives	a	thorough	presentation	of	direct	mapping,	associative	mapping,	and	set-
associative	mapping	techniques	for	cache.	It	also	provides	a	detailed	look	at	paging	and	segmentation,
TLBs,	and	the	various	algorithms	and	devices	associated	with	each.	A	tutorial	and	simulator	for	this
chapter	is	available	on	the	book’s	website.
•		Chapter	7	provides	a	detailed	overview	of	I/O	fundamentals,	bus	communication	and	protocols,	and
typical	external	storage	devices,	such	as	magnetic	and	optical	disks,	as	well	as	the	various	formats
available	for	each.	DMA,	programmed	I/O,	and	interrupts	are	covered	as	well.	In	addition,	various
techniques	for	exchanging	information	between	devices	are	introduced.	RAID	architectures	are	covered
in	detail.	Various	data	compression	formats	are	introduced	in	a	special	“Focus	On”	section.
•		Chapter	8	discusses	the	various	programming	tools	available	(such	as	compilers	and	assemblers)	and
their	relationship	to	the	architecture	of	the	machine	on	which	they	are	run.	The	goal	of	this	chapter	is	to
tie	 the	 programmer’s	 view	 of	 a	 computer	 system	 with	 the	 actual	 hardware	 and	 architecture	 of	 the
underlying	machine.	In	addition,	operating	systems	are	introduced,	but	only	covered	in	as	much	detail
as	applies	to	the	architecture	and	organization	of	a	system	(such	as	resource	use	and	protection,	traps
and	interrupts,	and	various	other	services).
•		Chapter	9	provides	an	overview	of	alternative	architectures	that	have	emerged	in	recent	years.	RISC,
Flynn’s	Taxonomy,	parallel	processors,	instruction-level	parallelism,	multiprocessors,	interconnection
networks,	 shared	 memory	 systems,	 cache	 coherence,	 memory	 models,	 superscalar	 machines,	 neural
networks,	systolic	architectures,	dataflow	computers,	quantum	computing,	and	distributed	architectures
are	covered.	Our	main	objective	in	this	chapter	is	to	help	the	reader	realize	we	are	not	limited	to	the
von	Neumann	architecture,	and	to	force	the	reader	to	consider	performance	issues,	setting	the	stage	for
the	next	chapter.
•		Chapter	10	covers	concepts	and	topics	of	interest	in	embedded	systems	that	have	not	been	covered	in
previous	 chapters.	 Specifically,	 this	 chapter	 focuses	 on	 embedded	 hardware	 and	 components,
embedded	 system	 design	 topics,	 the	 basics	 of	 embedded	 software	 construction,	 and	 embedded
operating	systems	features.
•	 	Chapter	 11	 addresses	 various	 performance	 analysis	 and	 management	 issues.	 The	 necessary
mathematical	preliminaries	are	introduced,	followed	by	a	discussion	of	MIPS,	FLOPS,	benchmarking,
and	various	optimization	issues	with	which	a	computer	scientist	should	be	familiar,	including	branch
prediction,	speculative	execution,	and	loop	optimization.
•	 	Chapter	12	 focuses	 on	 network	 organization	 and	 architecture,	 including	 network	 components	 and
protocols.	The	OSI	model	and	TCP/IP	suite	are	introduced	in	the	context	of	the	Internet.	This	chapter	is
by	no	means	intended	to	be	comprehensive.	The	main	objective	is	to	put	computer	architecture	in	the
correct	context	relative	to	network	architecture.
•		Chapter	13	introduces	some	popular	I/O	architectures	suitable	for	large	and	small	systems,	including
SCSI,	ATA,	IDE,	SATA,	PCI,	USB,	and	IEEE	1394.	This	chapter	also	provides	a	brief	overview	of
storage	area	networks	and	cloud	computing.

•		Appendix	A	is	a	short	appendix	on	data	structures	that	is	provided	for	those	situations	in	which	students
may	need	a	brief	introduction	or	review	of	such	topics	as	stacks,	queues,	and	linked	lists.
The	sequencing	of	the	chapters	is	such	that	they	can	be	taught	in	the	given	numerical	order.	However,
an	instructor	can	modify	the	order	to	better	fit	a	given	curriculum	if	necessary.	Figure	P.1	 shows	 the
prerequisite	relationships	that	exist	between	various	chapters.
FIGURE	P.1	Prerequisite	Relationship	Between	Chapters
What’s	New	in	the	Fourth	Edition
In	 the	 years	 since	 the	 third	 edition	 of	 this	 book	 was	 created,	 the	 field	 of	 computer	 architecture	 has
continued	to	grow.	In	this	fourth	edition,	we	have	incorporated	many	of	these	new	changes	in	addition	to
expanding	topics	already	introduced	in	the	first	three	editions.	Our	goal	in	the	fourth	edition	was	to	update
content	and	references,	add	new	material,	expand	current	discussions	based	on	reader	comments,	and
expand	the	number	of	exercises	in	all	of	the	core	chapters.	Although	we	cannot	itemize	all	the	changes	in
this	edition,	the	list	that	follows	highlights	those	major	changes	that	may	be	of	interest	to	the	reader:
•		Chapter	1	has	been	updated	to	include	new	examples	and	illustrations,	tablet	computers,	computing	as
a	service	(Cloud	computing),	and	cognitive	computing.	The	hardware	overview	has	been	expanded	and

updated	(notably,	the	discussion	on	CRTs	has	been	removed	and	a	discussion	of	graphics	cards	has
been	added),	and	additional	motivational	sidebars	have	been	added.	The	non-von	Neumann	section	has
been	updated,	and	a	new	section	on	parallelism	has	been	included.	The	number	of	exercises	at	the	end
of	the	chapter	has	been	increased	by	26%.
•		Chapter	2	contains	a	new	section	on	excess-M	notation.	The	simple	model	has	been	modified	to	use	a
standard	format,	and	more	examples	have	been	added.	This	chapter	has	a	44%	increase	in	the	number
of	exercises.
•		Chapter	3	has	been	modified	to	use	a	prime	(′)	instead	of	an	overbar	to	indicate	the	NOT	operator.
Timing	diagrams	have	been	added	to	help	explain	the	operation	of	sequential	circuits.	The	section	on
FSMs	has	been	expanded,	and	additional	exercises	have	been	included.
•		Chapter	4	contains	an	expanded	discussion	of	memory	organization	(including	memory	interleaving)	as
well	as	additional	examples	and	exercises.	We	are	now	using	the	“0x”	notation	to	indicate	hexadecimal
numbers.	More	detail	has	been	added	to	the	discussions	on	hardwired	and	microprogrammed	control,
and	 the	 logic	 diagrams	 for	 MARIE’s	 hardwired	 control	 unit	 and	 the	 timing	 diagrams	 for	 MARIE’s
microoperations	have	all	been	updated.
•		Chapter	5	contains	expanded	coverage	of	big	and	little	endian	and	additional	examples	and	exercises,
as	well	as	a	new	section	on	ARM	processors.
•	 	Chapter	 6	 has	 updated	 figures,	 an	 expanded	 discussion	 of	 associative	 memory,	 and	 additional
examples	 and	 discussion	 to	 clarify	 cache	 memory.	 The	 examples	 have	 all	 been	 updated	 to	 reflect
hexadecimal	addresses	instead	of	decimal	addresses.	This	chapter	now	contains	20%	more	exercises
than	the	third	edition.
•		Chapter	7	has	expanded	coverage	of	solid	state	drives	and	emerging	data	storage	devices	(such	as
carbon	nanotubes	and	memristors),	as	well	as	additional	coverage	of	RAID.	There	is	a	new	section	on
MP3	compression	and	in	addition	to	a	20%	increase	in	the	number	of	exercises	at	the	end	of	this
chapter.
•		Chapter	8	has	been	updated	to	reflect	advances	in	the	field	of	system	software.
•		Chapter	9	has	an	expanded	discussion	of	both	RISC	vs.	CISC	(integrating	this	debate	into	the	mobile
arena)	and	quantum	computing,	including	a	discussion	of	the	technological	singularity.
•		Chapter	10	contains	updated	material	for	embedded	operating	systems.
•		Chapter	12	has	been	updated	to	remove	obsolete	material	and	integrate	new	material.
•		Chapter	13	has	expanded	and	updated	coverage	of	USB,	expanded	coverage	of	Cloud	storage,	and
removal	of	obsolete	material.
Intended	Audience
This	book	was	originally	written	for	an	undergraduate	class	in	computer	organization	and	architecture	for
computer	science	majors.	Although	specifically	directed	toward	computer	science	majors,	the	book	does
not	preclude	its	use	by	IS	and	IT	majors.
This	book	contains	more	than	sufficient	material	for	a	typical	one-semester	(14	weeks,	42	lecture
hours)	course;	however,	all	the	material	in	the	book	cannot	be	mastered	by	the	average	student	in	a	one-
semester	class.	If	the	instructor	plans	to	cover	all	topics	in	detail,	a	two-semester	sequence	would	be
optimal.	The	organization	is	such	that	an	instructor	can	cover	the	major	topic	areas	at	different	levels	of
depth,	depending	on	the	experience	and	needs	of	the	students.	Table	P.2	gives	the	instructor	an	idea	of	the

amount	of	time	required	to	cover	the	topics,	and	also	lists	the	corresponding	levels	of	accomplishment	for
each	chapter.
It	is	our	intention	that	this	book	serve	as	a	useful	reference	long	after	the	formal	course	is	complete.
TABLE	P.2	Suggested	Lecture	Hours
Support	Materials
A	textbook	is	a	fundamental	tool	in	learning,	but	its	effectiveness	is	greatly	enhanced	by	supplemental
materials	and	exercises,	which	emphasize	the	major	concepts,	provide	immediate	feedback	to	the	reader,
and	 motivate	 understanding	 through	 repetition.	 We	 have,	 therefore,	 created	 the	 following	 ancillary
materials	for	the	fourth	edition	of	The	Essentials	of	Computer	Organization	and	Architecture:
•		Test	bank.
•	 	Instructor’s	Manual.	 This	 manual	 contains	 answers	 to	 exercises.	 In	 addition,	 it	 provides	 hints	 on
teaching	various	concepts	and	trouble	areas	often	encountered	by	students.
•		PowerPoint	Presentations.	These	slides	contain	lecture	material	appropriate	for	a	one-semester	course
in	computer	organization	and	architecture.
•		Figures	and	Tables.	For	those	who	wish	to	prepare	their	own	lecture	materials,	we	provide	the	figures
and	tables	in	downloadable	form.
•		Memory	Tutorial	and	Simulator.	This	package	allows	students	to	apply	the	concepts	on	cache	and
virtual	memory.
•		MARIE	Simulator.	This	package	allows	students	to	assemble	and	run	MARIE	programs.
•		Datapath	Simulator.	This	package	allows	students	to	trace	the	MARIE	datapath.
•		Tutorial	Software.	Other	tutorial	software	is	provided	for	various	concepts	in	the	book.
•		Companion	Website.	All	software,	slides,	and	related	materials	can	be	downloaded	from	the	book’s
website:
go.jblearning.com/ecoa4e

The	 exercises,	 sample	 exam	 problems,	 and	 solutions	 have	 been	 tested	 in	 numerous	 classes.	 The
Instructor’s	Manual,	which	includes	suggestions	for	teaching	the	various	chapters	in	addition	to	answers
for	the	book’s	exercises,	suggested	programming	assignments,	and	sample	example	questions,	is	available
to	instructors	who	adopt	the	book.	(Please	contact	your	Jones	&	Bartlett	Learning	representative	at	1-800-
832-0034	for	access	to	this	area	of	the	website.)
The	Instructional	Model:	MARIE
In	a	computer	organization	and	architecture	book,	the	choice	of	architectural	model	affects	the	instructor
as	well	as	the	students.	If	the	model	is	too	complicated,	both	the	instructor	and	the	students	tend	to	get
bogged	 down	 in	 details	 that	 really	 have	 no	 bearing	 on	 the	 concepts	 being	 presented	 in	 class.	 Real
architectures,	 although	 interesting,	 often	 have	 far	 too	 many	 peculiarities	 to	 make	 them	 usable	 in	 an
introductory	class.	To	make	things	even	more	complicated,	real	architectures	change	from	day	to	day.	In
addition,	it	is	difficult	to	find	a	book	incorporating	a	model	that	matches	the	local	computing	platform	in	a
given	department,	noting	that	the	platform,	too,	may	change	from	year	to	year.
To	alleviate	these	problems,	we	have	designed	our	own	simple	architecture,	MARIE,	specifically	for
pedagogical	use.	MARIE	(Machine	Architecture	that	is	Really	Intuitive	and	Easy)	allows	students	to	learn
the	essential	concepts	of	computer	organization	and	architecture,	including	assembly	language,	without
getting	 caught	 up	 in	 the	 unnecessary	 and	 confusing	 details	 that	 exist	 in	 real	 architectures.	 Despite	 its
simplicity,	it	simulates	a	functional	system.	The	MARIE	machine	simulator,	MarieSim,	has	a	user-friendly
GUI	that	allows	students	to	(1)	create	and	edit	source	code,	(2)	assemble	source	code	into	machine	object
code,	(3)	run	machine	code,	and	(4)	debug	programs.
Specifically,	MarieSim	has	the	following	features:
•		Support	for	the	MARIE	assembly	language	introduced	in	Chapter	4
•		An	integrated	text	editor	for	program	creation	and	modification
•		Hexadecimal	machine	language	object	code
•		An	integrated	debugger	with	single	step	mode,	break	points,	pause,	resume,	and	register	and	memory
tracing
•		A	graphical	memory	monitor	displaying	the	4096	addresses	in	MARIE’s	memory
•		A	graphical	display	of	MARIE’s	registers
•		Highlighted	instructions	during	program	execution
•		User-controlled	execution	speed
•		Status	messages
•		User-viewable	symbol	tables
•		An	interactive	assembler	that	lets	the	user	correct	any	errors	and	reassemble	automatically,	without
changing	environments
•		Online	help
•		Optional	core	dumps,	allowing	the	user	to	specify	the	memory	range
•		Frames	with	sizes	that	can	be	modified	by	the	user
•		A	small	learning	curve,	allowing	students	to	learn	the	system	quickly
MarieSim	was	written	in	the	Java	language	so	that	the	system	would	be	portable	to	any	platform	for

which	a	Java	Virtual	Machine	(JVM)	is	available.	Students	of	Java	may	wish	to	look	at	the	simulator’s
source	code,	and	perhaps	even	offer	improvements	or	enhancements	to	its	simple	functions.
Figure	P.2,	the	MarieSim	Graphical	Environment,	shows	the	graphical	environment	of	the	MARIE
machine	simulator.	The	screen	consists	of	four	parts:	the	menu	bar,	the	central	monitor	area,	the	memory
monitor,	and	the	message	area.
Menu	options	allow	the	user	to	control	the	actions	and	behavior	of	the	MARIE	simulator	system.
These	options	include	loading,	starting,	stopping,	setting	breakpoints,	and	pausing	programs	that	have
been	written	in	MARIE	assembly	language.
The	MARIE	simulator	illustrates	the	process	of	assembly,	loading,	and	execution,	all	in	one	simple
environment.	Users	can	see	assembly	language	statements	directly	from	their	programs,	along	with	the
corresponding	machine	code	(hexadecimal)	equivalents.	The	addresses	of	these	instructions	are	indicated
as	well,	and	users	can	view	any	portion	of	memory	at	any	time.	Highlighting	is	used	to	indicate	the	initial
loading	address	of	a	program	in	addition	to	the	currently	executing	instruction	while	a	program	runs.	The
graphical	display	of	the	registers	and	memory	allows	the	student	to	see	how	the	instructions	cause	the
values	in	the	registers	and	memory	to	change.
FIGURE	P.2	The	MarieSim	Graphical	Environment
If	You	Find	an	Error
We	have	attempted	to	make	this	book	as	technically	accurate	as	possible,	but	even	though	the	manuscript
has	been	through	numerous	proofreadings,	errors	have	a	way	of	escaping	detection.	We	would	greatly
appreciate	hearing	from	readers	who	find	any	errors	that	need	correcting.	Your	comments	and	suggestions
are	always	welcome;	please	send	on	email	to	ECOA@jblearning.com.
Credits	and	Acknowledgments
Few	books	are	entirely	the	result	of	one	or	two	people’s	unaided	efforts,	and	this	one	is	no	exception.	We

realize	that	writing	a	textbook	is	a	formidable	task	and	only	possible	with	a	combined	effort,	and	we	find
it	 impossible	 to	 adequately	 thank	 those	 who	 have	 made	 this	 book	 possible.	 If,	 in	 the	 following
acknowledgments,	we	inadvertently	omit	anyone,	we	humbly	apologize.
A	number	of	people	have	contributed	to	the	fourth	edition	of	this	book.	We	would	first	like	to	thank	all
of	the	reviewers	for	their	careful	evaluations	of	previous	editions	and	their	thoughtful	written	comments.
In	addition,	we	are	grateful	for	the	many	readers	who	have	emailed	useful	ideas	and	helpful	suggestions.
Although	we	cannot	mention	all	of	these	people	here,	we	especially	thank	John	MacCormick	(Dickinson
College)	 and	 Jacqueline	 Jones	 (Brooklyn	 College)	 for	 their	 meticulous	 reviews	 and	 their	 numerous
comments	and	suggestions.	We	extend	a	special	thanks	to	Karishma	Rao	and	Sean	Willeford	for	their	time
and	effort	in	producing	a	quality	memory	software	module.
We	would	also	like	to	thank	the	individuals	at	Jones	&	Bartlett	Learning	who	worked	closely	with	us
to	make	this	fourth	edition	possible.	We	are	very	grateful	to	Tiffany	Silter,	Laura	Pagluica,	and	Amy	Rose
for	their	professionalism,	commitment,	and	hard	work	on	the	fourth	edition.
I,	Linda	Null,	would	personally	like	to	thank	my	husband,	Tim	Wahls,	for	his	continued	patience	while
living	life	as	a	“book	widower”	for	a	fourth	time,	for	listening	and	commenting	with	frankness	about	the
book’s	contents	and	modifications,	for	doing	such	an	extraordinary	job	with	all	of	the	cooking,	and	for
putting	up	with	the	almost	daily	compromises	necessitated	by	my	writing	this	book—including	missing
our	annual	fly-fishing	vacation	and	forcing	our	horses	into	prolonged	pasture	ornament	status.	I	consider
myself	amazingly	lucky	to	be	married	to	such	a	wonderful	man.	I	extend	my	heartfelt	thanks	to	my	mentor,
Merry	 McDonald,	 who	 taught	 me	 the	 value	 and	 joys	 of	 learning	 and	 teaching,	 and	 doing	 both	 with
integrity.	Lastly,	I	would	like	to	express	my	deepest	gratitude	to	Julia	Lobur,	as	without	her,	this	book	and
its	accompanying	software	would	not	be	a	reality.	It	has	been	both	a	joy	and	an	honor	working	with	her.
I,	Julia	Lobur,	am	deeply	indebted	to	my	lawful	spouse,	Marla	Cattermole,	who	married	me	despite
the	 demands	 that	 this	 book	 has	 placed	 on	 both	 of	 us.	 She	 has	 made	 this	 work	 possible	 through	 her
forbearance	and	fidelity.	She	has	nurtured	my	body	through	her	culinary	delights	and	my	spirit	through	her
wisdom.	She	has	taken	up	my	slack	in	many	ways	while	working	hard	at	her	own	career.	I	would	also	like
to	convey	my	profound	gratitude	to	Linda	Null:	first,	for	her	unsurpassed	devotion	to	the	field	of	computer
science	education	and	dedication	to	her	students	and,	second,	for	giving	me	the	opportunity	to	share	with
her	the	ineffable	experience	of	textbook	authorship.

“Computing	is	not	about	computers	anymore.	It	is	about	living....	We	have	seen	computers
move	out	of	giant	air-conditioned	rooms	into	closets,	then	onto	desktops,	and	now	into	our
laps	and	pockets.	But	this	is	not	the	end....	Like	a	force	of	nature,	the	digital	age	cannot	be
denied	or	stopped....	The	information	superhighway	may	be	mostly	hype	today,	but	it	is	an
understatement	about	tomorrow.	It	will	exist	beyond	people’s	wildest	predictions....	We	are
not	waiting	on	any	invention.	It	is	here.	It	is	now.	It	is	almost	genetic	in	its	nature,	in	that
each	generation	will	become	more	digital	than	the	preceding	one.”
—Nicholas	Negroponte,	professor	of	media	technology	at	MIT
CHAPTER	1

Introduction
1.1			OVERVIEW
Dr.	Negroponte	is	among	many	who	see	the	computer	revolution	as	if	it	were	a	force	of	nature.	This	force
has	the	potential	to	carry	humanity	to	its	digital	destiny,	allowing	us	to	conquer	problems	that	have	eluded
us	for	centuries,	as	well	as	all	of	the	problems	that	emerge	as	we	solve	the	original	problems.	Computers
have	freed	us	from	the	tedium	of	routine	tasks,	liberating	our	collective	creative	potential	so	that	we	can,
of	course,	build	bigger	and	better	computers.
As	we	observe	the	profound	scientific	and	social	changes	that	computers	have	brought	us,	it	is	easy	to
start	feeling	overwhelmed	by	the	complexity	of	it	all.	This	complexity,	however,	emanates	from	concepts
that	are	fundamentally	very	simple.	These	simple	ideas	are	the	ones	that	have	brought	us	to	where	we	are
today	and	are	the	foundation	for	the	computers	of	the	future.	To	what	extent	they	will	survive	in	the	future
is	anybody’s	guess.	But	today,	they	are	the	foundation	for	all	of	computer	science	as	we	know	it.
Computer	scientists	are	usually	more	concerned	with	writing	complex	program	algorithms	than	with
designing	computer	hardware.	Of	course,	if	we	want	our	algorithms	to	be	useful,	a	computer	eventually
has	to	run	them.	Some	algorithms	are	so	complicated	that	they	would	take	too	long	to	run	on	today’s
systems.	These	kinds	of	algorithms	are	considered	computationally	infeasible.	Certainly,	at	the	current
rate	of	innovation,	some	things	that	are	infeasible	today	could	be	feasible	tomorrow,	but	it	seems	that	no
matter	 how	 big	 or	 fast	 computers	 become,	 someone	 will	 think	 up	 a	 problem	 that	 will	 exceed	 the
reasonable	limits	of	the	machine.
To	understand	why	an	algorithm	is	infeasible,	or	to	understand	why	the	implementation	of	a	feasible
algorithm	is	running	too	slowly,	you	must	be	able	to	see	the	program	from	the	computer’s	point	of	view.
You	must	understand	what	makes	a	computer	system	tick	before	you	can	attempt	to	optimize	the	programs
that	it	runs.	Attempting	to	optimize	a	computer	system	without	first	understanding	it	is	like	attempting	to
tune	your	car	by	pouring	an	elixir	into	the	gas	tank:	You’ll	be	lucky	if	it	runs	at	all	when	you’re	finished.
Program	optimization	and	system	tuning	are	perhaps	the	most	important	motivations	for	learning	how
computers	work.	There	are,	however,	many	other	reasons.	For	example,	if	you	want	to	write	compilers,
you	 must	 understand	 the	 hardware	 environment	 within	 which	 the	 compiler	 will	 function.	 The	 best
compilers	leverage	particular	hardware	features	(such	as	pipelining)	for	greater	speed	and	efficiency.
If	you	ever	need	to	model	large,	complex,	real-world	systems,	you	will	need	to	know	how	floating-
point	arithmetic	should	work	as	well	as	how	it	really	works	in	practice.	If	you	wish	to	design	peripheral
equipment	 or	 the	 software	 that	 drives	 peripheral	 equipment,	 you	 must	 know	 every	 detail	 of	 how	 a
particular	computer	deals	with	its	input/output	(I/O).	If	your	work	involves	embedded	systems,	you	need
to	know	that	these	systems	are	usually	resource-constrained.	Your	understanding	of	time,	space,	and	price
trade-offs,	as	well	as	I/O	architectures,	will	be	essential	to	your	career.
All	 computer	 professionals	 should	 be	 familiar	 with	 the	 concepts	 of	 benchmarking	 and	 be	 able	 to
interpret	 and	 present	 the	 results	 of	 benchmarking	 systems.	 People	 who	 perform	 research	 involving
hardware	 systems,	 networks,	 or	 algorithms	 find	 benchmarking	 techniques	 crucial	 to	 their	 day-to-day
work.	Technical	managers	in	charge	of	buying	hardware	also	use	benchmarks	to	help	them	buy	the	best
system	for	a	given	amount	of	money,	keeping	in	mind	the	ways	in	which	performance	benchmarks	can	be

manipulated	to	imply	results	favorable	to	particular	systems.
The	preceding	examples	illustrate	the	idea	that	a	fundamental	relationship	exists	between	computer
hardware	and	many	aspects	of	programming	and	software	components	in	computer	systems.	Therefore,
regardless	 of	 our	 areas	 of	 expertise,	 as	 computer	 scientists,	 it	 is	 imperative	 that	 we	 understand	 how
hardware	interacts	with	software.	We	must	become	familiar	with	how	various	circuits	and	components	fit
together	to	create	working	computer	systems.	We	do	this	through	the	study	of	computer	organization.
Computer	 organization	 addresses	 issues	 such	 as	 control	 signals	 (how	 the	 computer	 is	 controlled),
signaling	methods,	and	memory	types.	It	encompasses	all	physical	aspects	of	computer	systems.	It	helps
us	to	answer	the	question:	How	does	a	computer	work?
The	study	of	computer	architecture,	on	the	other	hand,	focuses	on	the	structure	and	behavior	of	the
computer	system	and	refers	to	the	logical	and	abstract	aspects	of	system	implementation	as	seen	by	the
programmer.	 Computer	 architecture	 includes	 many	 elements	 such	 as	 instruction	 sets	 and	 formats,
operation	codes,	data	types,	the	number	and	types	of	registers,	addressing	modes,	main	memory	access
methods,	and	various	I/O	mechanisms.	The	architecture	of	a	system	directly	affects	the	logical	execution
of	 programs.	 Studying	 computer	 architecture	 helps	 us	 to	 answer	 the	 question:	 How	 do	 I	 design	 a
computer?
The	computer	architecture	for	a	given	machine	is	the	combination	of	its	hardware	components	plus	its
instruction	set	architecture	(ISA).	The	ISA	is	the	agreed-upon	interface	between	all	the	software	that
runs	on	the	machine	and	the	hardware	that	executes	it.	The	ISA	allows	you	to	talk	to	the	machine.
The	distinction	between	computer	organization	and	computer	architecture	is	not	clear-cut.	People	in
the	 fields	 of	 computer	 science	 and	 computer	 engineering	 hold	 differing	 opinions	 as	 to	 exactly	 which
concepts	pertain	to	computer	organization	and	which	pertain	to	computer	architecture.	In	fact,	neither
computer	 organization	 nor	 computer	 architecture	 can	 stand	 alone.	 They	 are	 interrelated	 and
interdependent.	 We	 can	 truly	 understand	 each	 of	 them	 only	 after	 we	 comprehend	 both	 of	 them.	 Our
comprehension	of	computer	organization	and	architecture	ultimately	leads	to	a	deeper	understanding	of
computers	and	computation—the	heart	and	soul	of	computer	science.
1.2			THE	MAIN	COMPONENTS	OF	A	COMPUTER
Although	it	is	difficult	to	distinguish	between	the	ideas	belonging	to	computer	organization	and	those
ideas	belonging	to	computer	architecture,	it	is	impossible	to	say	where	hardware	issues	end	and	software
issues	begin.	Computer	scientists	design	algorithms	that	usually	are	implemented	as	programs	written	in
some	computer	language,	such	as	Java	or	C++.	But	what	makes	the	algorithm	run?	Another	algorithm,	of
course!	And	another	algorithm	runs	that	algorithm,	and	so	on	until	you	get	down	to	the	machine	level,
which	can	be	thought	of	as	an	algorithm	implemented	as	an	electronic	device.	Thus,	modern	computers
are	actually	implementations	of	algorithms	that	execute	other	algorithms.	This	chain	of	nested	algorithms
leads	us	to	the	following	principle:
Principle	of	Equivalence	of	Hardware	and	Software:	Any	task	done	by	software	can	also	be	done
using	hardware,	and	any	operation	performed	directly	by	hardware	can	be	done	using	software.
1
A	 special-purpose	 computer	 can	 be	 designed	 to	 perform	 any	 task,	 such	 as	 word	 processing,	 budget
analysis,	or	playing	a	friendly	game	of	Tetris.	Accordingly,	programs	can	be	written	to	carry	out	the
functions	of	special-purpose	computers,	such	as	the	embedded	systems	situated	in	your	car	or	microwave.
There	are	times	when	a	simple	embedded	system	gives	us	much	better	performance	than	a	complicated

computer	 program,	 and	 there	 are	 times	 when	 a	 program	 is	 the	 preferred	 approach.	 The	 Principle	 of
Equivalence	 of	 Hardware	 and	 Software	 tells	 us	 that	 we	 have	 a	 choice.	 Our	 knowledge	 of	 computer
organization	and	architecture	will	help	us	to	make	the	best	choice.
We	begin	our	discussion	of	computer	hardware	by	looking	at	the	components	necessary	to	build	a
computing	system.	At	the	most	basic	level,	a	computer	is	a	device	consisting	of	three	pieces:
1.		A	processor	to	interpret	and	execute	programs
2.		A	memory	to	store	both	data	and	programs
3.		A	mechanism	for	transferring	data	to	and	from	the	outside	world
We	 discuss	 these	 three	 components	 in	 detail	 as	 they	 relate	 to	 computer	 hardware	 in	 the	 following
chapters.
Once	you	understand	computers	in	terms	of	their	component	parts,	you	should	be	able	to	understand
what	a	system	is	doing	at	all	times	and	how	you	could	change	its	behavior	if	so	desired.	You	might	even
feel	like	you	have	a	few	things	in	common	with	it.	This	idea	is	not	as	far-fetched	as	it	appears.	Consider
how	a	student	sitting	in	class	exhibits	the	three	components	of	a	computer:	The	student’s	brain	is	the
processor,	the	notes	being	taken	represent	the	memory,	and	the	pencil	or	pen	used	to	take	notes	is	the	I/O
mechanism.	But	keep	in	mind	that	your	abilities	far	surpass	those	of	any	computer	in	the	world	today,	or
any	that	can	be	built	in	the	foreseeable	future.
1.3			AN	EXAMPLE	SYSTEM:	WADING	THROUGH	THE
JARGON
This	text	will	introduce	you	to	some	of	the	vocabulary	that	is	specific	to	computers.	This	jargon	can	be
confusing,	imprecise,	and	intimidating.	We	believe	that	with	a	little	explanation,	we	can	clear	the	fog.
For	the	sake	of	discussion,	we	have	provided	a	facsimile	computer	advertisement	(see	Figure	1.1).
The	ad	is	typical	of	many	in	that	it	bombards	the	reader	with	phrases	such	as	“32GB	DDR3	SDRAM,”
“PCIe	sound	card,”	and	“128KB	L1	cache.”	Without	having	a	handle	on	such	terminology,	you	would	be
hard-pressed	to	know	whether	the	stated	system	is	a	wise	buy,	or	even	whether	the	system	is	able	to	serve
your	needs.	As	we	progress	through	this	text,	you	will	learn	the	concepts	behind	these	terms.

FIGURE	1.1	A	Typical	Computer	Advertisement
Before	we	explain	the	ad,	however,	we	need	to	discuss	something	even	more	basic:	the	measurement
terminology	you	will	encounter	throughout	your	study	of	computers.
It	seems	that	every	field	has	its	own	way	of	measuring	things.	The	computer	field	is	no	exception.	For
computer	people	to	tell	each	other	how	big	something	is,	or	how	fast	something	is,	they	must	use	the	same
units	of	measure.	The	common	prefixes	used	with	computers	are	given	in	Table	1.1.	Back	in	the	1960s,
someone	decided	that	because	the	powers	of	2	were	close	to	the	powers	of	10,	the	same	prefix	names
could	be	used	for	both.	For	example,	2
10
	is	close	to	10
3
,	so	“kilo”	is	used	to	refer	to	them	both.	The	result
has	been	mass	confusion:	Does	a	given	prefix	refer	to	a	power	of	10	or	a	power	of	2?	Does	a	kilo	mean
10
3
	of	something	or	2
10
	of	something?	Although	there	is	no	definitive	answer	to	this	question,	there	are
accepted	“standards	of	usage.”	Power-of-10	prefixes	are	ordinarily	used	for	power,	electrical	voltage,
frequency	(such	as	computer	clock	speeds),	and	multiples	of	bits	(such	as	data	speeds	in	number	of	bits
per	second).	If	your	antiquated	modem	transmits	at	28.8kb/s,	then	it	transmits	28,800	bits	per	second	(or
28.8	×	10
3
).	Note	the	use	of	the	lowercase	“k”	to	mean	10
3
	and	the	lowercase	“b”	to	refer	to	bits.	An
uppercase	“K”	is	used	to	refer	to	the	power-of-2	prefix,	or	1024.	If	a	file	is	2KB	in	size,	then	it	is	2	×	2
10
or	2048	bytes.	Note	the	uppercase	“B”	to	refer	to	byte.	If	a	disk	holds	1MB,	then	it	holds	2
20
	bytes	(or	one
megabyte)	of	information.
Not	knowing	whether	specific	prefixes	refer	to	powers	of	2	or	powers	of	10	can	be	very	confusing.
For	this	reason,	the	International	Electrotechnical	Commission,	with	help	from	the	National	Institute	of
Standards	and	Technology,	has	approved	standard	names	and	symbols	for	binary	prefixes	to	differentiate
them	from	decimal	prefixes.	Each	prefix	is	derived	from	the	symbols	given	in	Table	1.1	by	adding	an	“i.”
For	example,	2
10
	has	been	renamed	“kibi”	(for	kilobinary)	and	is	represented	by	the	symbol	Ki.	Similarly,
2
20
	 is	 mebi,	 or	 Mi,	 followed	 by	 gibi	 (Gi),	 tebi	 (Ti),	 pebi	 (Pi),	 exbi	 (Ei),	 and	 so	 on.	 Thus,	 the	 term
mebibyte,	which	means	2
20
	bytes,	replaces	what	we	traditionally	call	a	megabyte.

TABLE	1.1	Common	Prefixes	Associated	with	Computer	Organization	and	Architecture
There	has	been	limited	adoption	of	these	new	prefixes.	This	is	unfortunate	because,	as	a	computer
user,	it	is	important	to	understand	the	true	meaning	of	these	prefixes.	A	kilobyte	(1KB)	of	memory	is
typically	1024	bytes	of	memory	rather	than	1000	bytes	of	memory.	However,	a	1GB	disk	drive	might
actually	be	1	billion	bytes	instead	of	2
30
	(which	means	you	are	getting	less	storage	than	you	think).	All
3.5′′	floppy	disks	are	described	as	storing	1.44MB	of	data	when	in	fact	they	store	1440KB	(or	1440	×	2
10
=	1474560	bytes).	You	should	always	read	the	manufacturer’s	fine	print	just	to	make	sure	you	know
exactly	what	1K,	1KB,	or	1G	represents.	See	the	sidebar	“When	a	Gigabyte	Isn’t	Quite	...”	for	a	good
example	of	why	this	is	so	important.
Who	Uses	Zettabytes	and	Yottabytes	Anyway?
The	 National	 Security	 Agency	 (NSA),	 an	 intelligence-gathering	 organization	 in	 the	 United	 States,
announced	that	its	new	Intelligence	Community	Comprehensive	National	Cybersecurity	Initiative	Data
Center,	in	Bluffdale,	Utah,	was	set	to	open	in	October	2013.	Approximately	100,000	square	feet	of	the
structure	is	utilized	for	the	data	center,	Whereas	the	remaining	900,000+	square	feet	houses	technical
support	and	administration.	The	new	data	center	will	help	the	NSA	monitor	the	vast	volume	of	data
traffic	on	the	Internet.
It	is	estimated	that	the	NSA	collects	roughly	2	million	gigabytes	of	data	every	hour,	24	hours	a	day,
seven	days	a	week.	This	data	includes	foreign	and	domestic	emails,	cell	phone	calls,	Internet	searches,
various	purchases,	and	other	forms	of	digital	data.	The	computer	responsible	for	analyzing	this	data	for
the	new	data	center	is	the	Titan	supercomputer,	a	water-cooled	machine	capable	of	operating	at	100
petaflops	 (or	 100,000	 trillion	 calculations	 each	 second).	 The	 PRISM	 (Planning	 Tool	 for	 Resource
Integration,	Synchronization,	and	Management)	surveillance	program	will	gather,	process,	and	track	all
collected	data.
Although	we	tend	to	think	in	terms	of	gigabytes	and	terabytes	when	buying	storage	for	our	personal
computers	and	other	devices,	the	NSA’s	data	center	storage	capacity	will	be	measured	in	zettabytes
(with	many	hypothesizing	that	storage	will	be	in	thousands	of	zettabytes,	or	yottabytes).	To	put	this	in
perspective,	in	a	2003	study	done	at	the	University	of	California	(UC)	Berkeley,	it	was	estimated	that
the	amount	of	new	data	created	in	2002	was	roughly	5EB.	An	earlier	study	by	UC	Berkeley	estimated
that	 by	 the	 end	 of	 1999,	 the	 sum	 of	 all	 information,	 including	 audio,	 video,	 and	 text,	 created	 by
humankind	was	approximately	12EB	of	data.	In	2006,	the	combined	storage	space	of	every	computer

hard	drive	in	the	world	was	estimated	at	160EB;	in	2009,	the	Internet	as	a	whole	was	estimated	to
contain	 roughly	 500	 total	 exabytes,	 or	 a	 half	 zettabyte,	 of	 data.	 Cisco,	 a	 U.S.	 computer	 network
hardware	manufacturer,	has	estimated	that	by	2016,	the	total	volume	of	data	on	the	global	internet	will
be	1.3ZB,	and	Seagate	Technology,	an	American	manufacturer	of	hard	drives,	has	estimated	that	the
total	storage	capacity	demand	will	reach	7ZB	in	2020.
The	NSA	is	not	the	only	organization	dealing	with	information	that	must	be	measured	in	numbers	of
bytes	 beyond	 the	 typical	 “giga”	 and	 “tera.”	 It	 is	 estimated	 that	 Facebook	 collects	 500TB	 of	 new
material	per	day;	YouTube	observes	roughly	1TB	of	new	video	information	every	four	minutes;	the
CERN	Large	Hadron	Collider	generates	1PB	of	data	per	second;	and	the	sensors	on	a	single,	new
Boeing	jet	engine	produce	20TB	of	data	every	hour.	Although	not	all	of	the	aforementioned	examples
require	permanent	storage	of	the	data	they	create/handle,	these	examples	nonetheless	provide	evidence
of	the	remarkable	quantity	of	data	we	deal	with	every	day.	This	tremendous	volume	of	information	is
what	prompted	the	IBM	Corporation,	in	2011,	to	develop	and	announce	its	new	120-PB	hard	drive,	a
storage	cluster	consisting	of	200,000	conventional	hard	drives	harnessed	to	work	together	as	a	single
unit.	If	you	plugged	your	MP3	player	into	this	drive,	you	would	have	roughly	two	billion	hours	of
music!
In	 this	 era	 of	 smartphones,	 tablets,	 Cloud	 computing,	 and	 other	 electronic	 devices,	 we	 will
continue	to	hear	people	talking	about	petabytes,	exabytes,	and	zettabytes	(and,	in	the	case	of	the	NSA,
even	 yottabytes).	 However,	 if	 we	 outgrow	 yottabytes,	 what	 then?	 In	 an	 effort	 to	 keep	 up	 with	 the
astronomical	growth	of	information	and	to	refer	to	even	bigger	volumes	of	data,	the	next	generation	of
prefixes	will	most	likely	include	the	terms	brontobyte	for	10
27
	and	gegobyte	for	10
30
	(although	some
argue	for	geobyte	and	geopbyte	as	the	prefixes	for	the	latter).	Although	these	are	not	yet	universally
accepted	international	prefix	units,	if	history	is	any	indication,	we	will	need	them	sooner	rather	than
later.
When	a	Gigabyte	Isn’t	Quite	...
Purchasing	 a	 new	 array	 of	 disk	 drives	 should	 be	 a	 relatively	 straightforward	 process	 once	 you
determine	your	technical	requirements	(e.g.,	disk	transfer	rate,	interface	type,	etc.).	From	here,	you
should	be	able	to	make	your	decision	based	on	a	simple	price/capacity	ratio,	such	as	dollars	per
gigabyte,	and	then	you’ll	be	done.	Well,	not	so	fast.
The	first	boulder	in	the	path	of	a	straightforward	analysis	is	that	you	must	make	sure	that	the	drives
you	are	comparing	all	express	their	capacities	either	in	formatted	or	unformatted	bytes.	As	much	as
16%	of	drive	space	is	consumed	during	the	formatting	process.	(Some	vendors	give	this	number	as
“usable	capacity.”)	Naturally,	the	price–capacity	ratio	looks	much	better	when	unformatted	bytes	are
used,	although	you	are	most	interested	in	knowing	the	amount	of	usable	space	a	disk	provides.
Your	next	obstacle	is	to	make	sure	that	the	same	radix	is	used	when	comparing	disk	sizes.	It	is
increasingly	common	for	disk	capacities	to	be	given	in	base	10	rather	than	base	2.	Thus,	a	“1GB”	disk
drive	has	a	capacity	of	10
9
	=	1,000,000,000	bytes,	rather	than	2
30
	=	1,073,741,824	bytes—a	reduction
of	about	7%.	This	can	make	a	huge	difference	when	purchasing	multigigabyte	enterprise-class	storage
systems.
As	 a	 concrete	 example,	 suppose	 you	 are	 considering	 purchasing	 a	 disk	 array	 from	 one	 of	 two
leading	 manufacturers.	 Manufacturer	x	 advertises	 an	 array	 of	 12	 250GB	disks	 for	 $20,000.

Manufacturer	y	is	offering	an	array	of	12	212.5GB	disks	for	$21,000.	All	other	things	being	equal,	the
cost	ratio	overwhelmingly	favors	Manufacturer	x:
Manufacturer	x:	$20,000	÷	(12	×	250GB)		$6.67	per	GB
Manufacturer	y:	$21,000	÷	(12	×	212.5GB)		$8.24	per	GB
Being	a	little	suspicious,	you	make	a	few	telephone	calls	and	learn	that	Manufacturer	x	is	citing
capacities	using	unformatted	base	10	gigabytes	and	Manufacturer	y	is	using	formatted	base	2	gigabytes.
These	facts	cast	the	problem	in	an	entirely	different	light:	To	start	with,	Manufacturer	x’s	disks	aren’t
really	 250GB	 in	 the	 way	 that	 we	 usually	 think	 of	 gigabytes.	 Instead,	 they	 are	 about	 232.8	 base	 2
gigabytes.	After	formatting,	the	number	reduces	even	more	to	about	197.9GB.	So	the	real	cost	ratios
are,	in	fact:
Manufacturer	x:	$20,000	÷	(12	×	197.9GB)		$8.42	per	GB
Manufacturer	y:	$21,000	÷	(12	×	212.5GB)		$8.24	per	GB
Indeed,	some	vendors	are	scrupulously	honest	in	disclosing	the	capabilities	of	their	equipment.
Unfortunately,	 others	 reveal	 the	 facts	 only	 under	 direct	 questioning.	 Your	 job	 as	 an	 educated
professional	is	to	ask	the	right	questions.
When	we	want	to	talk	about	how	fast	something	is,	we	speak	in	terms	of	fractions	of	a	second—
usually	thousandths,	millionths,	billionths,	or	trillionths.	Prefixes	for	these	metrics	are	given	in	the	right-
hand	side	of	Table	1.1.	Generally,	negative	powers	refer	to	powers	of	10,	not	powers	of	2.	For	this
reason,	the	new	binary	prefix	standards	do	not	include	any	new	names	for	the	negative	powers.	Notice
that	the	fractional	prefixes	have	exponents	that	are	the	reciprocal	of	the	prefixes	on	the	left	side	of	the
table.	Therefore,	if	someone	says	to	you	that	an	operation	requires	a	microsecond	to	complete,	you	should
also	understand	that	a	million	of	those	operations	could	take	place	in	one	second.	When	you	need	to	talk
about	how	many	of	these	things	happen	in	a	second,	you	would	use	the	prefix	mega-.	When	you	need	to
talk	about	how	fast	the	operations	are	performed,	you	would	use	the	prefix	micro-.
Now	to	explain	the	ad.	The	microprocessor	in	the	ad	is	an	Intel	i7	Quad	Core	processor	(which	means
it	is	essentially	four	processors)	and	belongs	to	a	category	of	processors	known	as	multicore	processors
(Section	 1.10	 contains	 more	 information	 on	 multicore	 processors).	 This	 particular	 processor	 runs	 at
3.9GHz.	Every	computer	system	contains	a	clock	that	keeps	the	system	synchronized.	The	clock	sends
electrical	pulses	simultaneously	to	all	main	components,	ensuring	that	data	and	instructions	will	be	where
they’re	supposed	to	be,	when	they’re	supposed	to	be	there.	The	number	of	pulsations	emitted	each	second
by	the	clock	is	its	frequency.	Clock	frequencies	are	measured	in	cycles	per	second,	or	hertz.	If	computer
system	clocks	generate	millions	of	pulses	per	second,	we	say	that	they	operate	in	the	megahertz	(MHz)
range.	Most	computers	today	operate	in	the	gigahertz	(GHz)	range,	generating	billions	of	pulses	per
second.	And	because	nothing	much	gets	done	in	a	computer	system	without	microprocessor	involvement,
the	frequency	rating	of	the	microprocessor	is	crucial	to	overall	system	speed.	The	microprocessor	of	the
system	in	our	advertisement	operates	at	3.9	billion	cycles	per	second,	so	the	seller	says	that	it	runs	at
3.9GHz.
The	 fact	 that	 this	 microprocessor	 runs	 at	 3.9GHz,	 however,	 doesn’t	 necessarily	 mean	 that	 it	 can
execute	 3.9	 billion	 instructions	 every	 second	 or,	 equivalently,	 that	 every	 instruction	 requires	 0.039
nanoseconds	to	execute.	Later	in	this	text,	you	will	see	that	each	computer	instruction	requires	a	fixed
number	 of	 cycles	 to	 execute.	 Some	 instructions	 require	 one	 clock	 cycle;	 however,	 most	 instructions

require	more	than	one.	The	number	of	instructions	per	second	that	a	microprocessor	can	actually	execute
is	proportionate	to	its	clock	speed.	The	number	of	clock	cycles	required	to	carry	out	a	particular	machine
instruction	is	a	function	of	both	the	machine’s	organization	and	its	architecture.
The	next	thing	we	see	in	the	ad	is	“1600MHz	32GB	DDR3	SDRAM.”	The	1600MHz	refers	to	the
speed	of	the	system	bus,	which	is	a	group	of	wires	that	moves	data	and	instructions	to	various	places
within	the	computer.	Like	the	microprocessor,	the	speed	of	the	bus	is	also	measured	in	MHz	or	GHz.
Many	computers	have	a	special	local	bus	for	data	that	supports	very	fast	transfer	speeds	(such	as	those
required	 by	 video).	 This	 local	 bus	 is	 a	 high-speed	 pathway	 that	 connects	 memory	 directly	 to	 the
processor.	Bus	speed	ultimately	sets	the	upper	limit	on	the	system’s	information-carrying	capability.
The	system	in	our	advertisement	also	boasts	a	memory	capacity	of	32	gigabytes	(GB),	or	about	32
billion	characters.	Memory	capacity	not	only	determines	the	size	of	the	programs	you	can	run,	but	also
how	many	programs	you	can	run	at	the	same	time	without	bogging	down	the	system.	Your	application	or
operating	system	manufacturer	will	usually	recommend	how	much	memory	you’ll	need	to	run	its	products.
(Sometimes	these	recommendations	can	be	hilariously	conservative,	so	be	careful	whom	you	believe!)
In	addition	to	memory	size,	our	advertised	system	provides	us	with	a	memory	type,	SDRAM,	short	for
synchronous	 dynamic	 random	 access	 memory.	 SDRAM	 is	 much	 faster	 than	 conventional
(nonsynchronous)	memory	because	it	can	synchronize	itself	with	a	microprocessor’s	bus.	The	system	in
our	 ad	 has	DDR3	SDRAM,	 or	double	 data	 rate	 type	 three	 SDRAM	 (for	 more	 information	 on	 the
different	types	of	memory,	see	Chapter	6).
A	Look	Inside	a	Computer
Have	you	ever	wondered	what	the	inside	of	a	computer	really	looks	like?	The	example	computer
described	in	this	section	gives	a	good	overview	of	the	components	of	a	modern	PC.	However,	opening
a	computer	and	attempting	to	find	and	identify	the	various	pieces	can	be	frustrating,	even	if	you	are
familiar	with	the	components	and	their	functions.

Photo	courtesy	of	Moxfyre	at	en.wikipedia	(from
http://commons.wikimedia.org/wiki/File:Acer_E360_Socket_939_motherboard_by_Foxconn.svg).
If	you	remove	the	cover	on	your	computer,	you	will	no	doubt	first	notice	a	big	metal	box	with	a	fan
attached.	This	is	the	power	supply.	You	will	also	see	various	drives,	including	a	hard	drive	and	a	DVD
drive	(or	perhaps	an	older	floppy	or	CD	drive).	There	are	many	integrated	circuits—small,	black
rectangular	boxes	with	legs	attached.	You	will	also	notice	electrical	pathways,	or	buses,	in	the	system.
There	are	printed	circuit	boards	(expansion	cards)	that	plug	into	sockets	on	the	motherboard,	the	large
board	at	the	bottom	of	a	standard	desktop	PC	or	on	the	side	of	a	PC	configured	as	a	tower	or	mini-
tower.	The	motherboard	is	the	printed	circuit	board	that	connects	all	the	components	in	the	computer,
including	the	CPU,	and	RAM	and	ROM,	as	well	as	an	assortment	of	other	essential	components.	The
components	on	the	motherboard	tend	to	be	the	most	difficult	to	identify.	Above	you	see	an	Acer	E360
motherboard	with	the	more	important	components	labeled.
The	Southbridge,	an	integrated	circuit	that	controls	the	hard	disk	and	I/O	(including	sound	and
video	cards),	is	a	hub	that	connects	slower	I/O	devices	to	the	system	bus.	These	devices	connect	via
the	I/O	ports	at	the	bottom	of	the	board.	The	PCI	slots	allow	for	expansion	boards	belonging	to	various
PCI	devices.	This	motherboard	also	has	PS/2	and	Firewire	connectors.	It	has	serial	and	parallel	ports,
in	addition	to	four	USB	ports.	This	motherboard	has	two	IDE	connector	slots,	four	SATA	connector
slots,	and	one	floppy	disk	controller.	The	super	I/O	chip	is	a	type	of	I/O	controller	that	controls	the
floppy	disk,	both	the	parallel	and	serial	ports,	and	the	keyboard	and	mouse.	The	motherboard	also	has
an	integrated	audio	chip,	as	well	as	an	integrated	Ethernet	chip	and	an	integrated	graphics	processor.
There	are	four	RAM	memory	banks.	There	is	no	processor	currently	plugged	into	this	motherboard,	but
we	see	the	socket	where	the	CPU	is	to	be	placed.	All	computers	have	an	internal	battery,	as	seen	in	the
top	middle	of	the	picture.	The	power	supply	plugs	into	the	power	connector.	The	BIOS	flash	chip
contains	the	instructions	in	ROM	that	your	computer	uses	when	it	is	first	powered	up.
A	note	of	caution	regarding	looking	inside	the	box:	There	are	many	safety	issues,	for	both	you	and
your	computer,	involved	with	removing	the	cover.	There	are	many	things	you	can	do	to	minimize	the

risks.	First	and	foremost,	make	sure	the	computer	is	turned	off.	Leaving	it	plugged	in	is	often	preferred,
as	this	offers	a	path	for	static	electricity.	Before	opening	your	computer	and	touching	anything	inside,
you	should	make	sure	you	are	properly	grounded	so	static	electricity	will	not	damage	any	components.
Many	of	the	edges,	both	on	the	cover	and	on	the	circuit	boards,	can	be	sharp,	so	take	care	when
handling	the	various	pieces.	Trying	to	jam	misaligned	cards	into	sockets	can	damage	both	the	card	and
the	motherboard,	so	be	careful	if	you	decide	to	add	a	new	card	or	remove	and	reinstall	an	existing	one.
The	next	 line	 in	 the	 ad,	 “128KB	 L1	 cache,	2MB	 L2	 cache”	 also	 describes	 a	 type	 of	 memory.	 In
Chapter	6,	you	will	learn	that	no	matter	how	fast	a	bus	is,	it	still	takes	“a	while”	to	get	data	from	memory
to	the	processor.	To	provide	even	faster	access	to	data,	many	systems	contain	a	special	memory	called
cache.	The	 system	 in	 our	 advertisement	 has	 two	 kinds	 of	cache.	 Level	 1	 cache	 (L1)	 is	 a	 small,	 fast
memory	cache	that	is	built	into	the	microprocessor	chip	and	helps	speed	up	access	to	frequently	used
data.	 Level	 2	 cache	 (L2)	 is	 a	 collection	 of	 fast,	 built-in	 memory	 chips	 situated	 between	 the
microprocessor	and	main	memory.	Notice	that	the	cache	in	our	system	has	a	capacity	of	kilobytes	(KB),
which	is	much	smaller	than	main	memory.	In	Chapter	6,	you	will	learn	how	cache	works,	and	that	a	bigger
cache	isn’t	always	better.
On	the	other	hand,	everyone	agrees	that	the	more	fixed	disk	capacity	you	have,	the	better	off	you	are.
The	advertised	system	has	a	1TB	hard	drive,	an	average	size	by	today’s	standards.	The	storage	capacity
of	a	fixed	(or	hard)	disk	is	not	the	only	thing	to	consider,	however.	A	large	disk	isn’t	very	helpful	if	it	is
too	slow	for	its	host	system.	The	computer	in	our	ad	has	a	hard	drive	that	rotates	at	7200	revolutions	per
minute	(RPM).	To	the	knowledgeable	reader,	this	indicates	(but	does	not	state	outright)	that	this	is	a	fairly
fast	drive.	Usually,	disk	speeds	are	stated	in	terms	of	the	number	of	milliseconds	required	(on	average)	to
access	data	on	the	disk,	in	addition	to	how	fast	the	disk	rotates.
Rotational	speed	is	only	one	of	the	determining	factors	in	the	overall	performance	of	a	disk.	The
manner	 in	 which	 it	 connects	 to—or	interfaces	 with—the	 rest	 of	 the	 system	 is	 also	 important.	 The
advertised	system	uses	a	SATA	(serial	advanced	technology	attachment	or	serial	ATA)	disk	interface.
This	is	an	evolutionary	storage	interface	that	has	replaced	IDE,	or	integrated	drive	electronics.	Another
common	interface	is	EIDE,	enhanced	integrated	drive	electronics,	a	cost-effective	hardware	interface
alternative	for	mass	storage	devices.	EIDE	contains	special	circuits	that	allow	it	to	enhance	a	computer’s
connectivity,	speed,	and	memory	capability.	Most	ATA,	IDE,	and	EIDE	systems	share	the	main	system	bus
with	the	processor	and	memory,	so	the	movement	of	data	to	and	from	the	disk	is	also	dependent	on	the
speed	of	the	system	bus.
Whereas	the	system	bus	is	responsible	for	all	data	movement	internal	to	the	computer,	ports	allow
movement	of	data	to	and	from	devices	external	to	the	computer.	Our	ad	speaks	of	two	different	ports	with
the	line,	“10	USB	ports,	1	serial	port.”	Serial	ports	transfer	data	by	sending	a	series	of	electrical	pulses
across	one	or	two	data	lines.	Another	type	of	port	some	computers	have	is	a	parallel	port.	Parallel	ports
use	at	least	eight	data	lines,	which	are	energized	simultaneously	to	transmit	data.	Many	new	computers	no
longer	come	with	serial	or	parallel	ports,	but	instead	have	only	USB	ports.	USB	(universal	serial	bus)	is
a	 popular	 external	 bus	 that	 supports	Plug-and-Play	 installation	 (the	 ability	 to	 configure	 devices
automatically)	as	well	as	hot	plugging	(the	ability	to	add	and	remove	devices	while	the	computer	is
running).
Expansion	slots	are	openings	on	the	motherboard	where	various	boards	can	be	plugged	in	to	add	new
capabilities	to	a	computer.	These	slots	can	be	used	for	such	things	as	additional	memory,	video	cards,
sound	cards,	network	cards,	and	modems.	Some	systems	augment	their	main	bus	with	dedicated	I/O	buses

using	these	expansion	slots.	Peripheral	Component	Interconnect	(PCI)	is	one	such	I/O	bus	standard	that
supports	the	connection	of	multiple	peripheral	devices.	PCI,	developed	by	the	Intel	Corporation,	operates
at	high	speeds	and	also	supports	Plug-and-Play.
PCI	is	an	older	standard	(it	has	been	around	since	1993)	and	was	superseded	by	PCI-x	in	2004.	PCI-x
basically	doubled	the	bandwidth	of	regular	PCI.	Both	PCI	and	PCI-x	are	parallel	in	operation.	In	2004,
PCI	 express	 (PCIe)	 replaced	 PCI-x.	 PCIe	 operates	 in	 serial	 and	 is	 currently	 the	 standard	 in	 today’s
computers.	In	the	ad,	we	see	the	computer	has	1	PCI	slot,	1	PCI	x	16	slot,	and	2	PCI	x	1	slots.	This
computer	 also	 has	 Bluetooth	 (a	 wireless	 technology	 allowing	 the	 transfer	 of	 information	 over	 short
distances)	and	an	HDMI	port	(High-Definition	Multimedia	Interface,	used	to	transmit	audio	and	video).
PCIe	has	not	only	superseded	PCI	and	PCI-x,	but	in	the	graphics	world,	it	has	also	progressively
replaced	the	AGP	(accelerated	graphics	port)	graphics	interface	designed	by	Intel	specifically	for	3D
graphics.	The	computer	in	our	ad	has	a	PCIe	video	card	with	1GB	of	memory.	The	memory	is	used	by	a
special	graphics	processing	unit	on	the	card.	This	processor	is	responsible	for	performing	the	necessary
calculations	to	render	the	graphics	so	the	main	processor	of	the	computer	is	not	required	to	do	so.	This
computer	also	has	a	PCIe	sound	card;	a	sound	card	contains	components	needed	by	the	system’s	stereo
speakers	and	microphone.
In	addition	to	telling	us	about	the	ports	and	expansion	slots	in	the	advertised	system,	the	ad	supplies	us
with	information	on	an	LCD	(liquid	crystal	display)	monitor,	or	“flat	panel”	display.	Monitors	have	little
to	do	with	the	speed	or	efficiency	of	a	computer	system,	but	they	have	great	bearing	on	the	comfort	of	the
user.	This	LCD	monitor	has	the	following	specifications:	24",	1920	×	1200	WUXGA,	300	cd/m
2
,	active
matrix,	1000:1	(static),	8ms,	24-bit	color	(16.7	million	colors),	VGA/DVI	input,	and	2USB	ports.	LCDs
use	a	liquid	crystal	material	sandwiched	between	two	pieces	of	polarized	glass.	Electric	currents	cause
the	crystals	to	move	around,	allowing	differing	levels	of	backlighting	to	pass	through,	creating	the	text,
colors,	 and	 pictures	 that	 appear	 on	 the	 screen.	 This	 is	 done	 by	 turning	 on/off	 different	pixels,	 small
“picture	elements”	or	dots	on	the	screen.	Monitors	typically	have	millions	of	pixels,	arranged	in	rows	and
columns.	This	monitor	has	1920	×	1200	(more	than	a	million)	pixels.
Most	 LCDs	 manufactured	 today	 utilize	 active	 matrix	 technology,	 Whereas	 passive	 technology	 is
reserved	for	smaller	devices	such	as	calculators	and	clocks.	Active	matrix	technology	uses	one	transistor
per	pixel;	passive	matrix	technology	uses	transistors	that	activate	entire	rows	and	columns.	Although
passive	technology	is	less	costly,	active	technology	renders	a	better	image	because	it	drives	each	pixel
independently.
The	LCD	monitor	in	the	ad	is	24",	measured	diagonally.	This	measurement	affects	the	aspect	ratio	of
the	monitor—the	ratio	of	horizontal	pixels	to	vertical	pixels	that	the	monitor	can	display.	Traditionally,
this	ratio	was	4:3,	but	newer	widescreen	monitors	use	ratios	of	16:10	or	16:9.	Ultra-wide	monitors	use	a
higher	ratio,	around	3:1	or	2:1.
When	discussing	resolution	and	LCDs,	it	is	important	to	note	that	LCDs	have	a	native	resolution;	this
means	 LCDs	 are	 designed	 for	 a	 specific	 resolution	 (generally	 given	 in	 horizontal	 pixels	 by	 vertical
pixels).	 Although	 you	 can	 change	 the	 resolution,	 the	 image	 quality	 typically	 suffers.	 Resolutions	 and
aspect	ratios	are	often	paired.	When	listing	resolutions	for	LCDs,	manufacturers	often	use	the	following
abbreviations:	 XGA	 (extended	 graphics	 array);	 XGA+	 (extended	 graphics	 array	 plus);	 SXGA	 (super
XGA);	 UXGA	 (ultra	 XGA);	 W	 prefix	 (wide);	 and	 WVA	 (wide	 viewing	 angle).	 The	 viewing	 angle
specifies	an	angle,	in	degrees,	that	indicates	at	which	angle	a	user	can	still	see	the	image	on	the	screen;
common	angles	range	from	120	to	170	degrees.	Some	examples	of	standard	4:3	native	resolutions	include
XGA	(1024	×	768),	SXGA	(1280	×	1024),	SXGA+	(1400	×	1050),	and	UXGA	(1600	×	1200).	Common
16:9	and	16:10	resolutions	include	WXGA	(1280	×	800),	WXGA+	(1440	×	900),	WSXGA+	(1680	×

1050),	and	WUXGA	(1920	×	1200).
LCD	monitor	specifications	often	list	a	response	time,	which	indicates	the	rate	at	which	the	pixels
can	change	colors.	If	this	rate	is	too	slow,	ghosting	and	blurring	can	occur.	The	LCD	monitor	in	the	ad	has
a	response	time	of	8ms.	Originally,	response	rates	measured	the	time	to	go	from	black	to	white	and	back
to	black.	Many	manufacturers	now	list	the	response	time	for	gray-to-gray	transitions	(which	is	generally
faster).	Because	they	typically	do	not	specify	which	transition	has	been	measured,	it	is	very	difficult	to
compare	monitors.	One	manufacturer	may	specify	a	response	time	of	2ms	for	a	monitor	(and	it	measures
gray-to-gray),	 while	 another	 manufacturer	 may	 specify	 a	 response	 rate	 of	 5ms	 for	 its	 monitor	 (and	 it
measures	black-to-white-to-black).	In	reality,	the	monitor	with	the	response	rate	of	5ms	may	actually	be
faster	overall.
Continuing	with	the	ad,	we	see	that	the	LCD	monitor	has	a	specification	of	300	cd/m
2
,	which	is	the
monitor’s	 luminance.	Luminance	 (or	 image	 brightness)	 is	 a	 measure	 of	 the	 amount	 of	 light	 an	 LCD
monitor	emits.	This	measure	is	typically	given	in	candelas	per	square	meter	(cd/m
2
).	When	purchasing	a
monitor,	the	 brightness	 level	 should	 be	 at	 least	 250	 (the	 higher	 the	 better);	 the	 average	 for	 computer
monitors	is	from	200	to	300	cd/m
2
.	Luminance	affects	how	easy	a	monitor	is	to	read,	particularly	in	low
light	situations.
Whereas	luminance	measures	the	brightness,	the	contrast	ratio	measures	the	difference	in	intensity
between	bright	whites	and	dark	blacks.	Contrast	ratios	can	be	static	(the	ratio	of	the	brightest	point	on	the
monitor	to	the	darkest	point	on	the	monitor	that	can	be	produced	at	a	given	instant	in	time)	or	dynamic	(the
ratio	of	the	darkest	point	in	one	image	to	the	lightest	point	in	another	image	produced	at	a	separate	point	in
time).	 Static	 specifications	 are	 typically	 preferred.	 A	 low	 static	 ratio	 (such	 as	 300:1)	 makes	 it	 more
difficult	to	discern	shades;	a	good	static	ratio	is	500:1	(with	ranges	from	400:1	to	3000:1).	The	monitor	in
the	ad	has	a	static	contrast	ratio	of	1000:1.	LCD	monitors	can	have	dynamic	ratios	of	12,000,000:1	and
higher,	but	a	higher	dynamic	number	does	not	necessarily	mean	the	monitor	is	better	than	a	monitor	with	a
much	lower	static	ratio.
The	next	specification	given	for	the	LCD	monitor	in	the	ad	is	its	color	depth.	This	number	reflects	the
number	of	colors	that	can	be	displayed	on	the	screen	at	one	time.	Common	depths	are	8-bit,	16-bit,	24-bit,
and	32-bit.	The	LCD	monitor	in	our	ad	can	display	2
24
,	or	roughly	16.7	million	colors.
LCD	 monitors	 also	 have	 many	 optional	 features.	 Some	 have	 integrated	 USB	 ports	 (as	 in	 this	 ad)
and/or	speakers.	Many	are	HDCP	(high	bandwidth	digital	content	protection)	compliant	(which	means
you	can	watch	HDCP-encrypted	materials,	such	as	Blu-ray	discs).	LCD	monitors	may	also	come	with
both	VGA	(video	graphics	array)	and	DVI	(digital	video	interface)	connections	(as	seen	in	the	ad).	VGA
sends	analog	signals	to	the	monitor	from	the	computer,	which	requires	digital-to-analog	conversion;	DVI
is	already	digital	in	format	and	requires	no	conversion,	resulting	in	a	cleaner	signal	and	crisper	image.
Although	 an	 LCD	 monitor	 typically	 provides	 better	 images	 using	 a	 DVI	 connection,	 having	 both
connectors	allows	one	to	use	an	LCD	with	existing	system	components.
Now	that	we	have	discussed	how	an	LCD	monitor	works	and	we	understand	the	concept	of	a	pixel,
let’s	go	back	and	discuss	graphics	cards	(also	called	video	cards)	in	more	detail.	With	millions	of	pixels
on	the	screen,	it	is	quite	challenging	to	determine	which	ones	should	be	off	and	which	ones	should	be	on
(and	in	what	color).	The	job	of	the	graphics	card	is	to	input	the	binary	data	from	your	computer	and
“translate”	 it	 into	 signals	 to	 control	 all	 pixels	 on	 the	 monitor;	 the	 graphics	 card	 therefore	 acts	 as	 a
“middleman”	between	the	computer’s	processor	and	monitor.	As	mentioned	previously,	some	computers
have	integrated	graphics,	which	means	the	computer’s	processor	is	responsible	for	doing	this	translation,
causing	a	large	workload	on	this	processor;	therefore,	many	computers	have	slots	for	graphics	cards,
allowing	the	processor	on	the	graphics	card	(called	a	graphics	processing	unit,	or	GPU)	to	perform	this

translation	instead.
The	GPU	is	no	ordinary	processor;	it	is	designed	to	most	efficiently	perform	the	complex	calculations
required	 for	 image	 rendering	 and	 contains	 special	 programs	 allowing	 it	 to	 perform	 this	 task	 more
effectively.	Graphics	cards	typically	contain	their	own	dedicated	RAM	used	to	hold	temporary	results	and
information,	including	the	location	and	color	for	each	pixel	on	the	screen.	A	frame	buffer	(part	of	this
RAM)	is	used	to	store	rendered	images	until	these	images	are	intended	to	be	displayed.	The	memory	on	a
graphics	card	connects	to	a	digital-to-analog	converter	(DAC),	a	device	that	converts	a	binary	image	to
analog	signals	that	a	monitor	can	understand	and	sends	them	via	a	cable	to	the	monitor.	Most	graphics
cards	today	have	two	types	of	monitor	connections:	DVI	for	LCD	screens	and	VGA	for	the	older	CRT
(cathode	ray	tube)	screens.
Most	graphics	cards	are	plugged	into	slots	in	computer	motherboards,	so	are	thus	powered	by	the
computers	themselves.	However,	some	are	very	powerful	and	actually	require	a	connection	directly	to	a
computer’s	power	supply.	These	high-end	graphics	cards	are	typically	found	in	computers	that	deal	with
image-intensive	applications,	such	as	video	editing	and	high-end	gaming.
Continuing	with	the	ad,	we	see	that	the	advertised	system	has	a	16x	DVD	+/–	RW	drive.	This	means
we	can	read	and	write	to	DVDs	and	CDs.	“16x”	is	a	measure	of	the	drive	speed	and	measures	how
quickly	the	drive	can	read	and	write.	DVDs	and	CDs	are	discussed	in	more	detail	in	Chapter	7.
Computers	are	more	useful	if	they	can	communicate	with	the	outside	world.	One	way	to	communicate
is	to	employ	an	Internet	service	provider	and	a	modem.	There	is	no	mention	of	a	modem	for	the	computer
in	 our	 ad,	 as	 many	 desktop	 owners	 use	 external	 modems	 provided	 by	 their	 Internet	 service	 provider
(phone	modem,	cable	modem,	satellite	modem,	etc).	However,	both	USB	and	PCI	modems	are	available
that	allow	you	to	connect	your	computer	to	the	Internet	using	the	phone	line;	many	of	these	also	allow	you
to	use	your	computer	as	a	fax	machine.	I/O	and	I/O	buses	in	general	are	discussed	in	Chapter	7.
A	computer	can	also	connect	directly	to	a	network.	Networking	allows	computers	to	share	files	and
peripheral	devices.	Computers	can	connect	to	a	network	via	either	a	wired	or	a	wireless	technology.
Wired	computers	use	Ethernet	technology,	an	international	standard	networking	technology	for	wired
networks,	and	there	are	two	options	for	the	connection.	The	first	is	to	use	a	network	interface	card
(NIC),	which	connects	to	the	motherboard	via	a	PCI	slot.	NICs	typically	support	10/100	Ethernet	(both
Ethernet	at	a	speed	of	10Mbps	and	fast	Ethernet	at	a	speed	of	100Mbps)	or	10/100/1000	(which	adds
Ethernet	at	1,000Mbps).	Another	option	for	wired	network	capability	is	integrated	Ethernet,	which	means
that	the	motherboard	itself	contains	all	necessary	components	to	support	10/100	Ethernet;	thus	no	PCI	slot
is	required.	Wireless	networking	has	the	same	two	options.	Wireless	NICs	are	available	from	a	multitude
of	vendors	and	are	available	for	both	desktops	and	laptops.	For	installation	in	desktop	machines,	you
need	 an	 internal	 card	 that	 will	 most	 likely	 have	 a	 small	 antenna.	 Laptops	 usually	 use	 an	 expansion
(PCMCIA)	slot	for	the	wireless	network	card,	and	vendors	have	started	to	integrate	the	antenna	into	the
back	of	the	case	behind	the	screen.	Integrated	wireless	(such	as	that	found	in	the	Intel	Centrino	mobile
technology)	eliminates	the	hassle	of	cables	and	cards.	The	system	in	our	ad	employs	integrated	Ethernet.
Note	 that	 many	 new	 computers	 may	 have	 integrated	 graphics	 and/or	 integrated	 sound	 in	 addition	 to
integrated	Ethernet.
Although	we	cannot	delve	into	all	of	the	brand-specific	components	available,	after	completing	this
text,	you	should	understand	the	concept	of	how	most	computer	systems	operate.	This	understanding	is
important	for	casual	users	as	well	as	experienced	programmers.	As	a	user,	you	need	to	be	aware	of	the
strengths	and	limitations	of	your	computer	system	so	you	can	make	informed	decisions	about	applications
and	thus	use	your	system	more	effectively.	As	a	programmer,	you	need	to	understand	exactly	how	your
system	hardware	functions	so	you	can	write	effective	and	efficient	programs.	For	example,	something	as

simple	 as	 the	 algorithm	 your	 hardware	 uses	 to	 map	 main	 memory	 to	 cache	 and	 the	 method	 used	 for
memory	 interleaving	 can	 have	 a	 tremendous	 effect	 on	 your	 decision	 to	 access	 array	 elements	 in	 row
versus	column-major	order.
Throughout	 this	 text,	 we	 investigate	 both	 large	 and	 small	 computers.	 Large	 computers	 include
mainframes,	enterprise-class	servers,	and	supercomputers.	Small	computers	include	personal	systems,
workstations,	 and	 handheld	 devices.	 We	 will	 show	 that	 regardless	 of	 whether	 they	 carry	 out	 routine
chores	or	perform	sophisticated	scientific	tasks,	the	components	of	these	systems	are	very	similar.	We
also	visit	some	architectures	that	lie	outside	what	is	now	the	mainstream	of	computing.	We	hope	that	the
knowledge	you	gain	from	this	text	will	ultimately	serve	as	a	springboard	for	your	continuing	studies	the
vast	and	exciting	fields	of	computer	organization	and	architecture.
Tablet	Computers
Ken	 Olsen,	 the	 founder	 of	 Digital	 Equipment	 Corporation,	 has	 been	 unfairly	 ridiculed	 for	 saying
“There	is	no	reason	for	any	individual	to	have	a	computer	in	his	home.”	He	made	this	statement	in
1977	 when	 the	 word,	computer,	 evoked	 a	 vision	 of	 the	 type	 of	 machine	 made	 by	 his	 company:
refrigerator-sized	behemoths	that	cost	a	fortune	and	required	highly	skilled	personnel	to	operate.	One
might	safely	say	that	no	one—except	perhaps	a	computer	engineer—ever	had	such	a	machine	in	his	or
her	home.
As	already	discussed,	the	“personal	computing”	wave	that	began	in	the	1980s	erupted	in	the	1990s
with	the	establishment	of	the	World	Wide	Web.	By	2010,	decennial	census	data	reported	that	68%	of
U.S.	households	claimed	to	have	a	personal	computer.	There	is,	however,	some	evidence	that	this	trend
has	peaked	and	is	now	in	decline,	owing	principally	to	the	widespread	use	of	smartphones	and	tablet
computers.	According	to	some	estimates,	as	many	as	65%	of	Internet	users	in	the	United	States	connect
exclusively	via	mobile	platforms.	The	key	to	this	trend	is	certainly	the	enchanting	usability	of	these
devices.
We	hardly	need	the	power	of	a	desktop	computer	to	surf	the	Web,	read	email,	or	listen	to	music.
Much	more	economical	and	lightweight,	tablet	computers	give	us	exactly	what	we	need	in	an	easy-to-
use	 package.	 With	 its	 booklike	 form,	 one	 is	 tempted	 to	 claim	 that	 a	 tablet	 constitutes	 the	 perfect
“portable	computer.”
The	 figure	 on	 the	 next	 page	 shows	 a	 disassembled	 Pandigital	 Novel	 tablet	 computer.	 We	 have
labeled	several	items	common	to	all	tablets.	The	mini	USB	port	provides	access	to	internal	storage
and	the	removable	SD	card.	Nearly	all	tablets	provide	Wi-Fi	connection	to	the	Internet,	with	some	also
supporting	2G,	3G,	and	4G	cellular	protocols.	Battery	life	can	be	as	much	as	14	hours	for	the	most
efficient	high-end	tablet	computers.	Unlike	the	Pandigital,	most	tablets	include	at	least	one	camera	for
still	photography	and	live	video.

A	Disassembled	Tablet	Computer
Courtesy	of	Julia	Lobur.
A	touchscreen	dominates	the	real	estate	of	all	portable	devices.	For	consumer	tablets	and	phones,
touchscreens	come	in	two	general	types:	resistive	and	capacitive.	Resistive	touchscreens	respond	to
the	pressure	of	a	finger	or	a	stylus.	Capacitive	touchscreens	react	to	the	electrical	properties	of	the
human	 skin.	 Resistive	 screens	 are	 less	 sensitive	 than	 capacitive	 screens,	 but	 they	 provide	 higher
resolution.	 Unlike	 resistive	 screens,	 capacitive	 screens	 support	 multitouch,	 which	 is	 the	 ability	 to
detect	the	simultaneous	press	of	two	or	more	fingers.
Military	and	medical	computer	touchscreens	are	necessarily	more	durable	than	those	intended	for
the	consumer	market.	Two	different	technologies,	surface	acoustic	wave	touch	sense	and	infrared
touch	sense,	 respectively,	 send	 ultrasonic	 and	 infrared	 waves	 across	 the	 surface	 of	 a	 ruggedized
touchscreen.	The	matrix	of	waves	is	broken	when	a	finger	comes	in	contact	with	the	surface	of	the
screen.
Because	of	its	high	efficiency,	cell	phone	CPU	technology	has	been	adapted	for	use	in	the	tablet
platform.	The	mobile	computing	space	has	been	dominated	by	ARM	chips,	although	Intel	and	AMD
have	been	gaining	market	share.	Operating	systems	for	these	devices	include	variants	of	Android	by
Google	 and	 iOS	 by	 Apple.	 Microsoft’s	 Surface	 tablets	 running	 Windows	 8	 provide	 access	 to	 the
Microsoft	Office	suite	of	products.
As	tablet	computers	continue	to	replace	desktop	systems,	they	will	also	find	uses	in	places	where
traditional	computers—even	laptops—are	impractical.	Thousands	of	free	and	inexpensive	applications
are	 available	 for	 all	 platforms,	 thereby	 increasing	 demand	 even	 further.	 Educational	 applications
abound.	With	a	size,	shape,	and	weight	similar	to	a	paperback	book,	tablet	computers	are	replacing
paper	 textbooks	 in	 some	 U.S.	 school	 districts.	 Thus,	 the	 elusive	 dream	 of	 “a	 computer	 for	 every
student”	is	finally	coming	true—thanks	to	the	tablet.	By	1985,	people	were	already	laughing	at	Olsen’s
“home	computer”	assertion.	Would	perhaps	these	same	people	have	scoffed	if	instead	he	would	have
predicted	a	computer	in	every	backpack?
1.4			STANDARDS	ORGANIZATIONS
Suppose	you	decide	you’d	like	to	have	one	of	those	nifty	new	LCD	widescreen	monitors.	You	figure	you

can	shop	around	a	bit	to	find	the	best	price.	You	make	a	few	phone	calls,	surf	the	Web,	and	drive	around
town	until	you	find	the	one	that	gives	you	the	most	for	your	money.	From	your	experience,	you	know	you
can	 buy	 your	 monitor	 anywhere	 and	 it	 will	 probably	 work	 fine	 on	 your	 system.	 You	 can	 make	 this
assumption	 because	 computer	 equipment	 manufacturers	 have	 agreed	 to	 comply	 with	 connectivity	 and
operational	specifications	established	by	a	number	of	government	and	industry	organizations.
Some	of	these	standards-setting	organizations	are	ad	hoc	trade	associations	or	consortia	made	up	of
industry	leaders.	Manufacturers	know	that	by	establishing	common	guidelines	for	a	particular	type	of
equipment,	they	can	market	their	products	to	a	wider	audience	than	if	they	came	up	with	separate—and
perhaps	incompatible—specifications.
Some	standards	organizations	have	formal	charters	and	are	recognized	internationally	as	the	definitive
authority	 in	 certain	 areas	 of	 electronics	 and	 computers.	 As	 you	 continue	 your	 studies	 in	 computer
organization	and	architecture,	you	will	encounter	specifications	formulated	by	these	groups,	so	you	should
know	something	about	them.
The	Institute	of	Electrical	and	Electronics	Engineers	(IEEE)	is	an	organization	dedicated	to	the
advancement	of	the	professions	of	electronic	and	computer	engineering.	The	IEEE	actively	promotes	the
interests	of	the	worldwide	engineering	community	by	publishing	an	array	of	technical	literature.	The	IEEE
also	 sets	 standards	 for	 various	 computer	 components,	 signaling	 protocols,	 and	 data	 representation,	 to
name	 only	 a	 few	 areas	 of	 its	 involvement.	 The	 IEEE	 has	 a	 democratic,	 albeit	 convoluted,	 procedure
established	for	the	creation	of	new	standards.	Its	final	documents	are	well	respected	and	usually	endure
for	several	years	before	requiring	revision.
The	International	Telecommunications	Union	(ITU)	is	based	in	Geneva,	Switzerland.	The	ITU	was
formerly	 known	 as	 the	Comité	 Consultatif	 International	 Télégraphique	 et	 Téléphonique,	 or	 the
International	 Consultative	 Committee	 on	 Telephony	 and	 Telegraphy.	 As	 its	 name	 implies,	 the	 ITU
concerns	itself	with	the	interoperability	of	telecommunications	systems,	including	telephone,	telegraph,
and	data	communication	systems.	The	telecommunications	arm	of	the	ITU,	the	ITU-T,	has	established	a
number	of	standards	that	you	will	encounter	in	the	literature.	You	will	see	these	standards	prefixed	by
ITU-T	or	the	group’s	former	initials,	CCITT.
Many	countries,	including	the	European	Community,	have	commissioned	umbrella	organizations	to
represent	their	interests	in	various	international	groups.	The	group	representing	the	United	States	is	the
American	National	Standards	Institute	(ANSI).	 Great	 Britain	 has	 its	British	Standards	Institution
(BSI)	in	addition	to	having	a	voice	on	the	CEN	(Comité	Européen	de	Normalisation),	the	European
committee	for	standardization.
The	International	Organization	for	Standardization	(ISO)	is	the	entity	that	coordinates	worldwide
standards	development,	including	the	activities	of	ANSI	with	BSI,	among	others.	ISO	is	not	an	acronym,
but	derives	from	the	Greek	word,	isos,	meaning	“equal.”	The	ISO	consists	of	more	than	2800	technical
committees,	each	of	which	is	charged	with	some	global	standardization	issue.	Its	interests	range	from	the
behavior	 of	 photographic	 film	 to	 the	 pitch	 of	 screw	 threads	 to	 the	 complex	 world	 of	 computer
engineering.	The	proliferation	of	global	trade	has	been	facilitated	by	the	ISO.	Today,	the	ISO	touches
virtually	every	aspect	of	our	lives.
Throughout	 this	 text,	 we	 mention	 official	 standards	 designations	 where	 appropriate.	 Definitive
information	concerning	many	of	these	standards	can	be	found	in	excruciating	detail	on	the	website	of	the
organization	responsible	for	establishing	the	standard	cited.	As	an	added	bonus,	many	standards	contain
“normative”	and	informative	references,	which	provide	background	information	in	areas	related	to	the
standard.

1.5			HISTORICAL	DEVELOPMENT
During	 their	 60-year	 life	 span,	 computers	 have	 become	 the	 perfect	 example	 of	 modern	 convenience.
Living	memory	is	strained	to	recall	the	days	of	steno	pools,	carbon	paper,	and	mimeograph	machines.	It
sometimes	seems	that	these	magical	computing	machines	were	developed	instantaneously	in	the	form	that
we	 now	 know	 them.	 But	 the	 developmental	 path	 of	 computers	 is	 paved	 with	 accidental	 discovery,
commercial	coercion,	and	whimsical	fancy.	And	occasionally	computers	have	even	improved	through	the
application	of	 solid	engineering	 practices!	 Despite	all	 the	twists,	 turns,	 and	technological	 dead	 ends,
computers	have	evolved	at	a	pace	that	defies	comprehension.	We	can	fully	appreciate	where	we	are	today
only	when	we	have	seen	where	we’ve	come	from.
In	the	sections	that	follow,	we	divide	the	evolution	of	computers	into	generations,	each	generation
being	defined	by	the	technology	used	to	build	the	machine.	We	have	provided	approximate	dates	for	each
generation	 for	 reference	 purposes	 only.	 You	 will	 find	 little	 agreement	 among	 experts	 as	 to	 the	 exact
starting	and	ending	times	of	each	technological	epoch.
Every	invention	reflects	the	time	in	which	it	was	made,	so	one	might	wonder	whether	it	would	have
been	called	a	computer	if	it	had	been	invented	in	the	late	1990s.	How	much	computation	do	we	actually
see	pouring	from	the	mysterious	boxes	perched	on	or	beside	our	desks?	Until	recently,	computers	served
us	only	by	performing	mind-bending	mathematical	manipulations.	No	longer	limited	to	white-jacketed
scientists,	today’s	computers	help	us	to	write	documents,	keep	in	touch	with	loved	ones	across	the	globe,
and	 do	 our	 shopping	 chores.	 Modern	 business	 computers	 spend	 only	 a	 minuscule	 part	 of	 their	 time
performing	accounting	calculations.	Their	main	purpose	is	to	provide	users	with	a	bounty	of	strategic
information	 for	 competitive	 advantage.	 Has	 the	 word	computer	 now	 become	 a	 misnomer?	 An
anachronism?	What,	then,	should	we	call	them,	if	not	computers?
We	cannot	present	the	complete	history	of	computing	in	a	few	pages.	Entire	texts	have	been	written	on
this	subject	and	even	they	leave	their	readers	wanting	more	detail.	If	we	have	piqued	your	interest,	we
refer	you	to	some	of	the	books	cited	in	the	list	of	references	at	the	end	of	this	chapter.
1.5.1		Generation	Zero:	Mechanical	Calculating	Machines	(1642–
1945)
Prior	to	the	1500s,	a	typical	European	businessperson	used	an	abacus	for	calculations	and	recorded	the
result	of	his	ciphering	in	Roman	numerals.	After	the	decimal	numbering	system	finally	replaced	Roman
numerals,	 a	 number	 of	 people	 invented	 devices	 to	 make	 decimal	 calculations	 even	 faster	 and	 more
accurate.	Wilhelm	Schickard	(1592–1635)	has	been	credited	with	the	invention	of	the	first	mechanical
calculator,	the	Calculating	Clock	(exact	date	unknown).	This	device	was	able	to	add	and	subtract	numbers
containing	as	many	as	six	digits.	In	1642,	Blaise	Pascal	(1623–1662)	developed	a	mechanical	calculator
called	the	Pascaline	to	help	his	father	with	his	tax	work.	The	Pascaline	could	do	addition	with	carry	and
subtraction.	It	was	probably	the	first	mechanical	adding	device	actually	used	for	a	practical	purpose.	In
fact,	the	Pascaline	was	so	well	conceived	that	its	basic	design	was	still	being	used	at	the	beginning	of	the
twentieth	century,	as	evidenced	by	the	Lightning	Portable	Adder	in	1908	and	the	Addometer	in	1920.
Gottfried	Wilhelm	von	Leibniz	(1646–1716),	a	noted	mathematician,	invented	a	calculator	known	as	the
Stepped	 Reckoner	 that	 could	 add,	 subtract,	 multiply,	 and	 divide.	 None	 of	 these	 devices	 could	 be
programmed	or	had	memory.	They	required	manual	intervention	throughout	each	step	of	their	calculations.
Although	machines	like	the	Pascaline	were	used	into	the	twentieth	century,	new	calculator	designs
began	 to	 emerge	 in	 the	 nineteenth	 century.	 One	 of	 the	 most	 ambitious	 of	 these	 new	 designs	 was	 the

Difference	Engine	by	Charles	Babbage	(1791–1871).	Some	people	refer	to	Babbage	as	“the	father	of
computing.”	By	all	accounts,	he	was	an	eccentric	genius	who	brought	us,	among	other	things,	the	skeleton
key	and	the	“cow	catcher,”	a	device	intended	to	push	cows	and	other	movable	obstructions	out	of	the	way
of	locomotives.
Babbage	built	his	Difference	Engine	in	1822.	The	Difference	Engine	got	its	name	because	it	used	a
calculating	technique	called	the	method	of	differences.	 The	 machine	 was	 designed	 to	 mechanize	 the
solution	of	polynomial	functions	and	was	actually	a	calculator,	not	a	computer.	Babbage	also	designed	a
general-purpose	machine	in	1833	called	the	Analytical	Engine.	Although	Babbage	died	before	he	could
build	it,	the	Analytical	Engine	was	designed	to	be	more	versatile	than	his	earlier	Difference	Engine.	The
Analytical	Engine	would	have	been	capable	of	performing	any	mathematical	operation.	The	Analytical
Engine	included	many	of	the	components	associated	with	modern	computers:	an	arithmetic	processing	unit
to	perform	calculations	(Babbage	referred	to	this	as	the	mill),	a	memory	(the	store),	and	input	and	output
devices.	 Babbage	 also	 included	 a	 conditional	 branching	 operation	 where	 the	 next	 instruction	 to	 be
performed	 was	 determined	 by	 the	 result	 of	 the	 previous	 operation.	 Ada,	 Countess	 of	 Lovelace	 and
daughter	of	poet	Lord	Byron,	suggested	that	Babbage	write	a	plan	for	how	the	machine	would	calculate
numbers.	This	is	regarded	as	the	first	computer	program,	and	Ada	is	considered	to	be	the	first	computer
programmer.	It	is	also	rumored	that	she	suggested	the	use	of	the	binary	number	system	rather	than	the
decimal	number	system	to	store	data.
A	perennial	problem	facing	machine	designers	has	been	how	to	get	data	into	the	machine.	Babbage
designed	the	Analytical	Engine	to	use	a	type	of	punched	card	for	input	and	programming.	Using	cards	to
control	the	behavior	of	a	machine	did	not	originate	with	Babbage,	but	with	one	of	his	friends,	Joseph-
Marie	 Jacquard	 (1752–1834).	 In	 1801,	 Jacquard	 invented	 a	 programmable	 weaving	 loom	 that	 could
produce	intricate	patterns	in	cloth.	Jacquard	gave	Babbage	a	tapestry	that	had	been	woven	on	this	loom
using	 more	 than	 10,000	 punched	 cards.	 To	 Babbage,	 it	 seemed	 only	 natural	 that	 if	 a	 loom	 could	 be
controlled	by	cards,	then	his	Analytical	Engine	could	be	as	well.	Ada	expressed	her	delight	with	this
idea,	writing,	“[T]he	Analytical	Engine	weaves	algebraical	patterns	just	as	the	Jacquard	loom	weaves
flowers	and	leaves.”
The	punched	card	proved	to	be	the	most	enduring	means	of	providing	input	to	a	computer	system.
Keyed	data	input	had	to	wait	until	fundamental	changes	were	made	in	how	calculating	machines	were
constructed.	In	the	latter	half	of	the	nineteenth	century,	most	machines	used	wheeled	mechanisms,	which
were	difficult	to	integrate	with	early	keyboards	because	they	were	levered	devices.	But	levered	devices
could	easily	punch	cards	and	wheeled	devices	could	easily	read	them.	So	a	number	of	devices	were
invented	 to	 encode	 and	 then	 “tabulate”	 card-punched	 data.	 The	 most	 important	 of	 the	 late-nineteenth-
century	tabulating	machines	was	the	one	invented	by	Herman	Hollerith	(1860–1929).	Hollerith’s	machine
was	used	for	encoding	and	compiling	1890	census	data.	This	census	was	completed	in	record	time,	thus
boosting	Hollerith’s	finances	and	the	reputation	of	his	invention.	Hollerith	later	founded	the	company	that
would	become	IBM.	His	80-column	punched	card,	the	Hollerith	card,	was	a	staple	of	automated	data
processing	for	more	than	50	years.
A	Pre-Modern	“Computer”	Hoax
The	 latter	 half	 of	 the	 sixteenth	 century	 saw	 the	 beginnings	 of	 the	 first	 Industrial	 Revolution.	 The
spinning	jenny	allowed	one	textile	worker	to	do	the	work	of	twenty,	and	steam	engines	had	power

equivalent	to	hundreds	of	horses.	Thus	began	our	enduring	fascination	with	all	things	mechanical.	With
the	right	skills	applied	to	the	problems	at	hand,	there	seemed	no	limits	to	what	humankind	could	do
with	its	machines!
Elaborate	 clocks	 began	 appearing	 at	 the	 beginning	 of	 the	 1700s.	 Complex	 and	 ornate	 models
graced	cathedrals	and	town	halls.	These	clockworks	eventually	morphed	into	mechanical	robots	called
automata.	Typical	models	played	musical	instruments	such	as	flutes	and	keyboard	instruments.	In	the
mid-1700s,	the	most	sublime	of	these	devices	entertained	royal	families	across	Europe.	Some	relied
on	trickery	to	entertain	their	audiences.	It	soon	became	something	of	a	sport	to	unravel	the	chicanery.
Empress	Marie-Therese	of	the	Austria-Hungarian	Empire	relied	on	a	wealthy	courtier	and	tinkerer,
Wolfgang	von	Kempelen,	to	debunk	the	spectacles	on	her	behalf.	One	day,	following	a	particularly
impressive	display,	Marie-Therese	challenged	von	Kempelen	to	build	an	automaton	to	surpass	all	that
had	ever	been	brought	to	her	court.
von	Kempelen	took	the	challenge,	and	after	several	months’	work,	he	delivered	a	turban-wearing,
pipe-smoking,	chess-playing	automaton.	For	all	appearances,	“The	Turk”	was	a	formidable	opponent
for	even	the	best	players	of	the	day.	As	an	added	touch,	the	machine	contained	a	set	of	baffles	enabling
it	to	rasp	“Échec!”	as	needed.	So	impressive	was	this	machine	that	for	84	years	it	drew	crowds	across
Europe	and	the	United	States.
Of	course,	as	with	all	similar	automata,	von	Kempelen’s	Turk	relied	on	trickery	to	perform	its
prodigious	feat.	Despite	some	astute	debunkers	correctly	deducing	how	it	was	done,	the	secret	of	the
Turk	was	never	divulged:	A	human	chess	player	was	cleverly	concealed	inside	its	cabinet.	The	Turk
thus	pulled	off	one	of	the	first	and	most	impressive	“computer”	hoaxes	in	the	history	of	technology.	It
would	take	another	200	years	before	a	real	machine	could	match	the	Turk—without	the	trickery.

The	mechanical	Turk
Reprinted	from	Robert	Willis,	An	attempt	to	Analyse	the	Automaton	Chess	Player	of	Mr.	de
Kempelen.	JK	Booth,	London.	1824.
1.5.2		The	First	Generation:	Vacuum	Tube	Computers	(1945–1953)
Although	Babbage	is	often	called	the	“father	of	computing,”	his	machines	were	mechanical,	not	electrical
or	 electronic.	 In	 the	 1930s,	 Konrad	 Zuse	 (1910–1995)	 picked	 up	 where	 Babbage	 left	 off,	 adding
electrical	 technology	 and	 other	 improvements	 to	 Babbage’s	 design.	 Zuse’s	 computer,	 the	 Z1,	 used
electromechanical	relays	instead	of	Babbage’s	hand-cranked	gears.	The	Z1	was	programmable	and	had	a
memory,	an	arithmetic	unit,	and	a	control	unit.	Because	money	and	resources	were	scarce	in	wartime
Germany,	Zuse	used	discarded	movie	film	instead	of	punched	cards	for	input.	Although	his	machine	was
designed	to	use	vacuum	tubes,	Zuse,	who	was	building	his	machine	on	his	own,	could	not	afford	the	tubes.
Thus,	the	Z1	correctly	belongs	in	the	first	generation,	although	it	had	no	tubes.
Zuse	built	the	Z1	in	his	parents’	Berlin	living	room	while	Germany	was	at	war	with	most	of	Europe.
Fortunately,	he	couldn’t	convince	the	Nazis	to	buy	his	machine.	They	did	not	realize	the	tactical	advantage
such	a	device	would	give	them.	Allied	bombs	destroyed	all	three	of	Zuse’s	first	systems,	the	Z1,	Z2,	and
Z3.	 Zuse’s	 impressive	 machines	 could	 not	 be	 refined	 until	 after	 the	 war	 and	 ended	 up	 being	 another

“evolutionary	dead	end”	in	the	history	of	computers.
Digital	computers,	as	we	know	them	today,	are	the	outcome	of	work	done	by	a	number	of	people	in
the	1930s	and	1940s.	Pascal’s	basic	mechanical	calculator	was	designed	and	modified	simultaneously	by
many	people;	the	same	can	be	said	of	the	modern	electronic	computer.	Notwithstanding	the	continual
arguments	 about	 who	 was	 first	 with	 what,	 three	 people	 clearly	 stand	 out	 as	 the	 inventors	 of	 modern
computers:	John	Atanasoff,	John	Mauchly,	and	J.	Presper	Eckert.
John	Atanasoff	(1904–1995)	has	been	credited	with	the	construction	of	the	first	completely	electronic
computer.	The	Atanasoff	Berry	Computer	(ABC)	was	a	binary	machine	built	from	vacuum	tubes.	Because
this	system	was	built	specifically	to	solve	systems	of	linear	equations,	we	cannot	call	it	a	general-purpose
computer.	There	were,	however,	some	features	that	the	ABC	had	in	common	with	the	general-purpose
ENIAC	(Electronic	Numerical	Integrator	and	Computer),	which	was	invented	a	few	years	later.	These
common	features	caused	considerable	controversy	as	to	who	should	be	given	the	credit	(and	patent	rights)
for	the	invention	of	the	electronic	digital	computer.	(The	interested	reader	can	find	more	details	on	a
rather	lengthy	lawsuit	involving	Atanasoff	and	the	ABC	in	Mollenhoff	[1988].)
John	Mauchly	(1907–1980)	and	J.	Presper	Eckert	(1929–1995)	were	the	two	principal	inventors	of
the	 ENIAC,	 introduced	 to	 the	 public	 in	 1946.	 The	 ENIAC	 is	 recognized	 as	 the	 first	 all-electronic,
general-purpose	digital	computer.	This	machine	used	17,468	vacuum	tubes,	occupied	1800	square	feet	of
floor	space,	weighed	30	tons,	and	consumed	174	kilowatts	of	power.	The	ENIAC	had	a	memory	capacity
of	about	1000	information	bits	(about	20	10-digit	decimal	numbers)	and	used	punched	cards	to	store	data.
John	Mauchly’s	vision	for	an	electronic	calculating	machine	was	born	from	his	lifelong	interest	in
predicting	the	weather	mathematically.	While	a	professor	of	physics	at	Ursinus	College	near	Philadelphia,
Mauchly	 engaged	 dozens	 of	 adding	 machines	 and	 student	 operators	 to	 crunch	 mounds	 of	 data	 that	 he
believed	would	reveal	mathematical	relationships	behind	weather	patterns.	He	felt	that	if	he	could	have
only	 a	 little	 more	 computational	 power,	 he	 could	 reach	 the	 goal	 that	 seemed	 just	 beyond	 his	 grasp.
Pursuant	 to	 the	 Allied	 war	 effort,	 and	 with	 ulterior	 motives	 to	 learn	 about	 electronic	 computation,
Mauchly	 volunteered	 for	 a	 crash	 course	 in	 electrical	 engineering	 at	 the	 University	 of	 Pennsylvania’s
Moore	School	of	Engineering.	Upon	completion	of	this	program,	Mauchly	accepted	a	teaching	position	at
the	Moore	School,	where	he	taught	a	brilliant	young	student,	J.	Presper	Eckert.	Mauchly	and	Eckert	found
a	mutual	interest	in	building	an	electronic	calculating	device.	In	order	to	secure	the	funding	they	needed	to
build	their	machine,	they	wrote	a	formal	proposal	for	review	by	the	school.	They	portrayed	their	machine
as	conservatively	as	they	could,	billing	it	as	an	“automatic	calculator.”	Although	they	probably	knew	that
computers	would	be	able	to	function	most	efficiently	using	the	binary	numbering	system,	Mauchly	and
Eckert	designed	their	system	to	use	base	10	numbers,	in	keeping	with	the	appearance	of	building	a	huge
electronic	adding	machine.	The	university	rejected	Mauchly	and	Eckert’s	proposal.	Fortunately,	the	U.S.
Army	was	more	interested.

U.S.	Army,	1946.
During	 World	 War	 II,	 the	 army	 had	 an	 insatiable	 need	 for	 calculating	 the	 trajectories	 of	 its	 new

ballistic	armaments.	Thousands	of	human	“computers”	were	engaged	around	the	clock	cranking	through
the	arithmetic	required	for	these	firing	tables.	Realizing	that	an	electronic	device	could	shorten	ballistic
table	calculation	from	days	to	minutes,	the	army	funded	the	ENIAC.	And	the	ENIAC	did	indeed	shorten
the	time	to	calculate	a	table	from	20	hours	to	30	seconds.	Unfortunately,	the	machine	wasn’t	ready	before
the	end	of	the	war.	But	the	ENIAC	had	shown	that	vacuum	tube	computers	were	fast	and	feasible.	During
the	next	decade,	vacuum	tube	systems	continued	to	improve	and	were	commercially	successful.
What	Is	a	Vacuum	Tube?
The	wired	world	that	we	know	today	was	born	from	the	invention	of	a	single	electronic	device	called
a	vacuum	tube	by	Americans	and—more	accurately—a	valve	by	the	British.	Vacuum	tubes	should	be
called	valves	because	they	control	the	flow	of	electrons	in	electrical	systems	in	much	the	same	way	as
valves	control	the	flow	of	water	in	a	plumbing	system.	In	fact,	some	mid-twentieth-century	breeds	of
these	electron	tubes	contain	no	vacuum	at	all,	but	are	filled	with	conductive	gases,	such	as	mercury
vapor,	which	can	provide	desirable	electrical	behavior.
The	electrical	phenomenon	that	makes	tubes	work	was	discovered	by	Thomas	A.	Edison	in	1883
while	 he	 was	 trying	 to	 find	 ways	 to	 keep	 the	 filaments	 of	 his	 light	 bulbs	 from	 burning	 away	 (or
oxidizing)	a	few	minutes	after	electrical	current	was	applied.	Edison	reasoned	correctly	that	one	way
to	prevent	filament	oxidation	would	be	to	place	the	filament	in	a	vacuum.	Edison	didn’t	immediately
understand	that	air	not	only	supports	combustion,	but	also	is	a	good	insulator.	When	he	energized	the
electrodes	holding	a	new	tungsten	filament,	the	filament	soon	became	hot	and	burned	out	as	the	others
had	before	it.	This	time,	however,	Edison	noticed	that	electricity	continued	to	flow	from	the	warmed
negative	terminal	to	the	cool	positive	terminal	within	the	light	bulb.	In	1911,	Owen	Willans	Richardson
analyzed	this	behavior.	He	concluded	that	when	a	negatively	charged	filament	was	heated,	electrons
“boiled	 off”	 as	 water	 molecules	 can	 be	 boiled	 to	 create	 steam.	 He	 aptly	 named	 this	 phenomenon
thermionic	emission.
Thermionic	emission,	as	Edison	had	documented	it,	was	thought	by	many	to	be	only	an	electrical
curiosity.	But	in	1905,	a	British	former	assistant	to	Edison,	John	A.	Fleming,	saw	Edison’s	discovery

as	much	more	than	a	novelty.	He	knew	that	thermionic	emission	supported	the	flow	of	electrons	in	only
one	direction:	from	the	negatively	charged	cathode	to	the	positively	charged	anode,	also	called	a
plate.	 He	 realized	 that	 this	 behavior	 could	rectify	 alternating	 current.	 That	 is,	 it	 could	 change
alternating	 current	 into	 the	 direct	 current	 that	 was	 essential	 for	 the	 proper	 operation	 of	 telegraph
equipment.	Fleming	used	his	ideas	to	invent	an	electronic	valve	later	called	a	diode	tube	or	rectifier.
The	 diode	 was	 well	 suited	 for	 changing	 alternating	 current	 into	 direct	 current,	 but	 the	 greatest
power	of	the	electron	tube	was	yet	to	be	discovered.	In	1907,	an	American	named	Lee	DeForest	added
a	third	element,	called	a	control	grid.	The	control	grid,	when	carrying	a	negative	charge,	can	reduce	or
prevent	electron	flow	from	the	cathode	to	the	anode	of	a	diode.
When	DeForest	patented	his	device,	he	called	it	an	audion	tube.	It	was	later	known	as	a	triode.
The	schematic	symbol	for	the	triode	is	shown	at	the	left.
A	triode	can	act	as	either	a	switch	or	an	amplifier.	Small	changes	in	the	charge	of	the	control	grid
can	cause	much	larger	changes	in	the	flow	of	electrons	between	the	cathode	and	the	anode.	Therefore,
a	weak	signal	applied	to	the	grid	results	in	a	much	stronger	signal	at	the	plate	output.	A	sufficiently
large	negative	charge	applied	to	the	grid	stops	all	electrons	from	leaving	the	cathode.
Additional	control	grids	were	eventually	added	to	the	triode	to	allow	more	exact	control	of	the
electron	flow.	Tubes	with	two	grids	(four	elements)	are	called	tetrodes;	tubes	with	three	grids	are
called	pentodes.	Triodes	and	pentodes	were	the	tubes	most	commonly	used	in	communications	and
computer	 applications.	 Often,	 two	 or	 three	 triodes	 or	 pentodes	 would	 be	 combined	 within	 one
envelope	so	they	could	share	a	single	heater,	thereby	reducing	the	power	consumption	of	a	particular
device.	These	latter-day	devices	were	called	“miniature”	tubes	because	many	were	about	2	inches
(5cm)	high	and	0.5	inch	(1.5cm)	in	diameter.	Equivalent	full-sized	diodes,	triodes,	and	pentodes	were
a	little	smaller	than	a	household	light	bulb.

Vacuum	 tubes	 were	 not	 well	 suited	 for	 building	 computers.	 Even	 the	 simplest	 vacuum	 tube
computer	system	required	thousands	of	tubes.	Enormous	amounts	of	electrical	power	were	required	to
heat	the	cathodes	of	these	devices.	To	prevent	a	meltdown,	this	heat	had	to	be	removed	from	the	system
as	 quickly	 as	 possible.	 Power	 consumption	 and	 heat	 dissipation	 could	 be	 reduced	 by	 running	 the
cathode	 heaters	 at	 lower	 voltages,	 but	 this	 reduced	 the	 already	 slow	 switching	 speed	 of	 the	 tube.
Despite	 their	 limitations	 and	 power	 consumption,	 vacuum	 tube	 computer	 systems,	 both	 analog	 and
digital,	 served	 their	 purpose	 for	 many	 years	 and	 are	 the	 architectural	 foundation	 for	 all	 modern
computer	systems.
Although	decades	have	passed	since	the	last	vacuum	tube	computer	was	manufactured,	vacuum
tubes	are	still	used	in	audio	amplifiers.	These	“high-end”	amplifiers	are	favored	by	musicians	who
believe	that	tubes	provide	a	resonant	and	pleasing	sound	unattainable	by	solid-state	devices.
1.5.3		The	Second	Generation:	Transistorized	Computers	(1954–1965)
The	 vacuum	 tube	 technology	 of	 the	 first	 generation	 was	 not	 very	 dependable.	 In	 fact,	 some	 ENIAC
detractors	believed	that	the	system	would	never	run	because	the	tubes	would	burn	out	faster	than	they
could	be	replaced.	Although	system	reliability	wasn’t	as	bad	as	the	doomsayers	predicted,	vacuum	tube
systems	often	experienced	more	downtime	than	uptime.
In	 1948,	 three	 researchers	 with	 Bell	 Laboratories—John	 Bardeen,	 Walter	 Brattain,	 and	 William
Shockley—invented	 the	 transistor.	 This	 new	 technology	 not	 only	 revolutionized	 devices	 such	 as
televisions	and	radios,	but	also	pushed	the	computer	industry	into	a	new	generation.	Because	transistors
consume	less	power	than	vacuum	tubes,	are	smaller,	and	work	more	reliably,	the	circuitry	in	computers
consequently	became	smaller	and	more	reliable.	Despite	using	transistors,	computers	of	this	generation
were	still	bulky	and	quite	costly.	Typically	only	universities,	governments,	and	large	businesses	could
justify	the	expense.	Nevertheless,	a	plethora	of	computer	makers	emerged	in	this	generation;	IBM,	Digital
Equipment	Corporation	(DEC),	and	Univac	(now	Unisys)	dominated	the	industry.	IBM	marketed	the	7094
for	scientific	applications	and	the	1401	for	business	applications.	DEC	was	busy	manufacturing	the	PDP-
1.	 A	 company	 founded	 (but	 soon	 sold)	 by	 Mauchly	 and	 Eckert	 built	 the	 Univac	 systems.	 The	 most
successful	Unisys	systems	of	this	generation	belonged	to	its	1100	series.	Another	company,	Control	Data
Corporation	 (CDC),	 under	 the	 supervision	 of	 Seymour	 Cray,	 built	 the	 CDC	 6600,	 the	 world’s	 first

supercomputer.	The	$10	million	CDC	6600	could	perform	10	million	instructions	per	second,	used	60-bit
words,	and	had	an	astounding	128	kilowords	of	main	memory.
What	Is	a	Transistor?
The	transistor,	short	for	transfer	resistor,	is	the	solid-state	version	of	the	triode.	There	is	no	such
thing	as	a	solid-state	version	of	the	tetrode	or	pentode.	Electrons	are	better	behaved	in	a	solid	medium
than	in	the	open	void	of	a	vacuum	tube,	so	there	is	no	need	for	the	extra	controlling	grids.	Either
germanium	or	silicon	can	be	the	basic	“solid”	used	in	these	solid-state	devices.	In	their	pure	form,
neither	of	these	elements	is	a	good	conductor	of	electricity.	But	when	they	are	combined	with	trace
amounts	 of	 elements	 that	 are	 their	 neighbors	 in	 the	 Periodic	 Chart	 of	 the	 Elements,	 they	 conduct
electricity	in	an	effective	and	easily	controlled	manner.
Boron,	aluminum,	and	gallium	can	be	found	to	the	left	of	silicon	and	germanium	on	the	Periodic
Chart.	Because	they	lie	to	the	left	of	silicon	and	germanium,	they	have	one	less	electron	in	their	outer
electron	shell,	or	valence.	So	if	you	add	a	small	amount	of	aluminum	to	silicon,	the	silicon	ends	up
with	a	slight	imbalance	in	its	outer	electron	shell,	and	therefore	attracts	electrons	from	any	pole	that
has	a	negative	potential	(an	excess	of	electrons).	When	modified	(or	doped)	in	this	way,	silicon	or
germanium	becomes	a	P-type	material.
Similarly,	if	we	add	a	little	boron,	arsenic,	or	gallium	to	silicon,	we’ll	have	extra	electrons	in
valences	of	the	silicon	crystals.	This	gives	us	an	N-type	material.	A	small	amount	of	current	will	flow
through	the	N-type	material	if	we	provide	the	loosely	bound	electrons	in	the	N-type	material	with	a
place	to	go.	In	other	words,	if	we	apply	a	positive	potential	to	N-type	material,	electrons	will	flow
from	the	negative	pole	to	the	positive	pole.	If	the	poles	are	reversed,	that	is,	if	we	apply	a	negative

potential	to	the	N-type	material	and	a	positive	potential	to	the	P-type	material,	no	current	will	flow.
This	means	we	can	make	a	solid-state	diode	from	a	simple	junction	of	N-	and	P-type	materials.
The	solid-state	triode,	the	transistor,	consists	of	three	layers	of	semiconductor	material.	Either	a
slice	of	P-type	material	is	sandwiched	between	two	N-type	materials,	or	a	slice	of	N-type	material	is
sandwiched	between	two	P-type	materials.	The	former	is	called	an	NPN	transistor,	the	latter	a	PNP
transistor.	 The	 inner	 layer	 of	 the	 transistor	 is	 called	 the	 base;	 the	 other	 two	 layers	 are	 called	 the
collector	and	the	emitter.
The	figure	at	the	left	shows	how	current	flows	through	NPN	and	PNP	transistors.	The	base	in	a
transistor	works	just	like	the	control	grid	in	a	triode	tube:	Small	changes	in	the	current	at	the	base	of	a
transistor	result	in	a	large	electron	flow	from	the	emitter	to	the	collector.
A	discrete-component	transistor	is	shown	in	“TO-50”	packaging	in	the	figure	at	the	top	of	this
sidebar.	There	are	only	three	wires	(leads)	that	connect	the	base,	emitter,	and	collector	of	the	transistor
to	the	rest	of	the	circuit.	Transistors	are	not	only	smaller	than	vacuum	tubes,	but	they	also	run	cooler
and	are	much	more	reliable.	Vacuum	tube	filaments,	like	light	bulb	filaments,	run	hot	and	eventually
burn	out.	Computers	using	transistorized	components	will	naturally	be	smaller	and	run	cooler	than	their
vacuum	 tube	 predecessors.	 The	 ultimate	 miniaturization,	 however,	 is	 not	 realized	 by	 replacing
individual	triodes	with	discrete	transistors,	but	in	shrinking	entire	circuits	onto	one	piece	of	silicon.
Integrated	 circuits,	 or	chips,	 contain	 hundreds	 to	 billions	 of	 microscopic	 transistors.	 Several
different	techniques	are	used	to	manufacture	integrated	circuits.	One	of	the	simplest	methods	involves
creating	a	circuit	using	computer-aided	design	software	that	can	print	large	maps	of	each	of	the	several
silicon	layers	forming	the	chip.	Each	map	is	used	like	a	photographic	negative	where	light-induced
changes	in	a	photoresistive	substance	on	the	chip’s	surface	produce	the	delicate	patterns	of	the	circuit
when	the	silicon	chip	is	immersed	in	a	chemical	that	washes	away	the	exposed	areas	of	the	silicon.
This	technique	is	called	photomicrolithography.	After	the	etching	is	completed,	a	layer	of	N-type	or
P-type	 material	 is	 deposited	 on	 the	 bumpy	 surface	 of	 the	 chip.	 This	 layer	 is	 then	 treated	 with	 a
photoresistive	 substance,	 exposed	 to	 light,	 and	 etched	 as	 was	 the	 layer	 before	 it.	 This	 process
continues	 until	 all	 the	 layers	 have	 been	 etched.	 The	 resulting	 peaks	 and	 valleys	 of	 P-	 and	 N-type
material	form	microscopic	electronic	components,	including	transistors,	that	behave	just	like	larger

versions	fashioned	from	discrete	components,	except	that	they	run	a	lot	faster	and	consume	a	small
fraction	of	the	power.
1.5.4		The	Third	Generation:	Integrated	Circuit	Computers	(1965–
1980)
The	real	explosion	in	computer	use	came	with	the	integrated	circuit	generation.	Jack	Kilby	invented	the
integrated	circuit	(IC),	or	microchip,	made	of	germanium.	Six	months	later,	Robert	Noyce	(who	had	also
been	working	on	integrated	circuit	design)	created	a	similar	device	using	silicon	instead	of	germanium.
This	 is	 the	 silicon	 chip	 upon	 which	 the	 computer	 industry	 was	 built.	 Early	 ICs	 allowed	 dozens	 of
transistors	to	exist	on	a	single	silicon	chip	that	was	smaller	than	a	single	“discrete	component”	transistor.
Computers	 became	 faster,	 smaller,	 and	 cheaper,	 bringing	 huge	 gains	 in	 processing	 power.	 The	 IBM
System/360	family	of	computers	was	among	the	first	commercially	available	systems	to	be	built	entirely
of	solid-state	components.	The	360	product	line	was	also	IBM’s	first	offering	in	which	all	the	machines	in
the	 family	 were	 compatible,	 meaning	 they	 all	 used	 the	 same	 assembly	 language.	 Users	 of	 smaller
machines	could	upgrade	to	larger	systems	without	rewriting	all	their	software.	This	was	a	revolutionary
new	concept	at	the	time.
The	IC	generation	also	saw	the	introduction	of	time-sharing	and	multiprogramming	(the	ability	for
more	 than	 one	 person	 to	 use	 the	 computer	 at	 a	 time).	 Multiprogramming,	 in	 turn,	 necessitated	 the
introduction	of	new	operating	systems	for	these	computers.	Time-sharing	minicomputers	such	as	DEC’s
PDP-8	 and	 PDP-11	 made	 computing	 affordable	 to	 smaller	 businesses	 and	 more	 universities.	IC
technology	also	allowed	for	the	development	of	more	powerful	supercomputers.	Seymour	Cray	took	what
he	 had	 learned	 while	 building	 the	 CDC	 6600	 and	 started	 his	 own	 company,	 the	 Cray	 Research
Corporation.	This	company	produced	a	number	of	supercomputers,	starting	with	the	$8.8	million	Cray-1,
in	1976.	The	Cray-1,	in	stark	contrast	to	the	CDC	6600,	could	execute	more	than	160	million	instructions
per	second	and	could	support	8MB	of	memory.	See	Figure	1.2	for	a	size	comparison	of	vacuum	tubes,
transistors,	and	integrated	circuits.
1.5.5		The	Fourth	Generation:	VLSI	Computers	(1980–????)
In	the	third	generation	of	electronic	evolution,	multiple	transistors	were	integrated	onto	one	chip.	As
manufacturing	techniques	and	chip	technologies	advanced,	increasing	numbers	of	transistors	were	packed
onto	one	chip.	There	are	now	various	levels	of	integration:	SSI	(small-scale	integration),	in	which	there
are	10	to	100	components	per	chip;	MSI	(medium-scale	integration),	in	which	there	are	100	to	1000
components	per	chip;	LSI	(large-scale	integration),	in	which	there	are	1000	to	10,000	components	per
chip;	and	finally,	VLSI	(very-large-scale	integration),	in	which	there	are	more	than	10,000	components
per	chip.	This	last	level,	VLSI,	marks	the	beginning	of	the	fourth	generation	of	computers.	The	complexity
of	integraged	circuits	continues	to	grow,	with	more	transistors	being	added	all	the	time.	The	term	ULSI
(ultra-large-scale	integration)	has	been	suggested	for	integrated	circuits	containing	more	than	1	million
transistors.	In	2005,	billions	of	transistors	were	put	on	a	single	chip.	Other	useful	terminology	includes:
(1)	WSI	(wafer-scale	integration,	building	superchip	ICs	from	an	entire	silicon	wafer;	(2)	3D-IC	(three-
dimensional	integrated	circuit);	and	(3)	SOC	(system-on-a-chip),	an	IC	that	includes	all	the	necessary
components	for	the	entire	computer.

FIGURE	1.2	Comparison	of	Computer	Components	Clockwise,	starting	from	the	top:
1)	Vacuum	tube
2)	Transistor
3)	Chip	containing	3200	2-input	NAND	gates
4)	Integrated	circuit	package	(the	small	silver	square	in	the	lower	left-hand	corner	is	an	integrated	circuit)
Courtesy	of	Linda	Null.
To	 give	 some	 perspective	 to	 these	 numbers,	 consider	 the	 ENIAC-on-a-chip	 project.	 In	 1997,	 to
commemorate	 the	 fiftieth	 anniversary	 of	 its	 first	 public	 demonstration,	 a	 group	 of	 students	 at	 the
University	of	Pennsylvania	constructed	a	single-chip	equivalent	of	the	ENIAC.	The	1800-square-foot,	30-
ton	beast	that	devoured	174	kilowatts	of	power	the	minute	it	was	turned	on	had	been	reproduced	on	a	chip
the	size	of	a	thumbnail.	This	chip	contained	approximately	174,569	transistors—an	order	of	magnitude
fewer	than	the	number	of	components	typically	placed	on	the	same	amount	of	silicon	in	the	late	1990s.
VLSI	allowed	Intel,	in	1971,	to	create	the	world’s	first	microprocessor,	the	4004,	which	was	a	fully
functional,	4-bit	system	that	ran	at	108KHz.	Intel	also	introduced	the	random	access	memory	(RAM)	chip,
accommodating	four	kilobits	of	memory	on	a	single	chip.	This	allowed	computers	of	the	fourth	generation
to	become	smaller	and	faster	than	their	solid-state	predecessors.
VLSI	technology,	and	its	incredible	shrinking	circuits,	spawned	the	development	of	microcomputers.
These	systems	were	small	enough	and	inexpensive	enough	to	make	computers	available	and	affordable	to
the	general	public.	The	premiere	microcomputer	was	the	Altair	8800,	released	in	1975	by	the	Micro
Instrumentation	and	Telemetry	(MITS)	corporation.	The	Altair	8800	was	soon	followed	by	the	Apple	I
and	Apple	II,	and	Commodore’s	PET	and	Vic	20.	Finally,	in	1981,	IBM	introduced	its	PC	(Personal
Computer).
The	Personal	Computer	was	IBM’s	third	attempt	at	producing	an	“entry-level”	computer	system.	Its

Datamaster	and	its	5100	Series	desktop	computers	flopped	miserably	in	the	marketplace.	Despite	these
early	failures,	IBM’s	John	Opel	convinced	his	management	to	try	again.	He	suggested	forming	a	fairly
autonomous	“independent	business	unit”	in	Boca	Raton,	Florida,	far	from	IBM’s	headquarters	in	Armonk,
New	York.	Opel	picked	Don	Estridge,	an	energetic	and	capable	engineer,	to	champion	the	development	of
the	 new	 system,	 code-named	 the	 Acorn.	 In	 light	 of	 IBM’s	 past	 failures	 in	 the	 small-systems	 area,
corporate	management	held	tight	rein	on	the	Acorn’s	timeline	and	finances.	Opel	could	get	his	project	off
the	ground	only	after	promising	to	deliver	it	within	a	year,	a	seemingly	impossible	feat.
Estridge	 knew	 that	 the	 only	 way	 he	 could	 deliver	 the	 PC	 within	 the	 wildly	 optimistic	 12-month
schedule	would	be	to	break	with	IBM	convention	and	use	as	many	“off-the-shelf”	parts	as	possible.	Thus,
from	the	outset,	the	IBM	PC	was	conceived	with	an	“open”	architecture.	Although	some	people	at	IBM
may	have	later	regretted	the	decision	to	keep	the	architecture	of	the	PC	as	nonproprietary	as	possible,	it
was	this	very	openness	that	allowed	IBM	to	set	the	standard	for	the	industry.	While	IBM’s	competitors
were	busy	suing	companies	for	copying	their	system	designs,	PC	clones	proliferated.	Before	long,	the
price	of	“IBM-compatible”	microcomputers	came	within	reach	for	just	about	every	small	business.	Also,
thanks	to	the	clone	makers,	large	numbers	of	these	systems	soon	began	finding	true	“personal	use”	in
people’s	homes.
IBM	eventually	lost	its	microcomputer	market	dominance,	but	the	genie	was	out	of	the	bottle.	For
better	or	worse,	the	IBM	architecture	continues	to	be	the	de	facto	standard	for	microcomputing,	with	each
year	 heralding	 bigger	 and	 faster	 systems.	 Today,	 the	 average	 desktop	 computer	 has	 many	 times	 the
computational	power	of	the	mainframes	of	the	1960s.
Since	the	1960s,	mainframe	computers	have	seen	stunning	improvements	in	price–performance	ratios
owing	to	VLSI	technology.	Although	the	IBM	System/360	was	an	entirely	solid-state	system,	it	was	still	a
water-cooled,	power-gobbling	behemoth.	It	could	perform	only	about	50,000	instructions	per	second	and
supported	only	16MB	of	memory	(while	usually	having	kilobytes	of	physical	memory	installed).	These
systems	were	so	costly	that	only	the	largest	businesses	and	universities	could	afford	to	own	or	lease	one.
Today’s	mainframes—now	called	“enterprise	servers”—are	still	priced	in	the	millions	of	dollars,	but
their	processing	capabilities	have	grown	several	thousand	times	over,	passing	the	billion-instructions-
per-second	mark	in	the	late	1990s.	These	systems,	often	used	as	Web	servers,	routinely	support	hundreds
of	thousands	of	transactions	per	minute!
The	 processing	 power	 brought	 by	 VLSI	 to	 supercomputers	 defies	 comprehension.	 The	 first
supercomputer,	the	CDC	6600,	could	perform	10	million	instructions	per	second,	and	had	128KB	of	main
memory.	By	contrast,	supercomputers	of	today	contain	thousands	of	processors,	can	address	terabytes	of
memory,	and	will	soon	be	able	to	perform	a	quadrillion	instructions	per	second.
What	technology	will	mark	the	beginning	of	the	fifth	generation?	Some	say	the	fifth	generation	will
mark	the	acceptance	of	parallel	processing	and	the	use	of	networks	and	single-user	workstations.	Many
people	believe	we	have	already	crossed	into	this	generation.	Some	believe	it	will	be	quantum	computing.
Some	people	characterize	the	fifth	generation	as	being	the	generation	of	neural	network,	DNA,	or	optical
computing	 systems.	 It’s	 possible	 that	 we	 won’t	 be	 able	 to	 define	 the	 fifth	 generation	 until	 we	 have
advanced	into	the	sixth	or	seventh	generations,	and	whatever	those	eras	will	bring.
The	Integrated	Circuit	and	Its	Production
Integrated	circuits	are	found	all	around	us,	from	computers	to	cars	to	refrigerators	to	cell	phones.	The

most	advanced	circuits	contain	hundreds	of	millions	(and	even	billions)	of	components	in	an	area
about	the	size	of	your	thumbnail.	The	transistors	in	these	advanced	circuits	can	be	as	small	as	45nm,	or
0.000045	millimeters,	in	size.	Thousands	of	these	transistors	would	fit	in	a	circle	the	diameter	of	a
human	hair.
How	 are	 these	 circuits	 made?	 They	 are	 manufactured	 in	 semiconductor	 fabrication	 facilities.
Because	the	components	are	so	small,	all	precautions	must	be	taken	to	ensure	a	sterile,	particle-free
environment,	so	manufacturing	is	done	in	a	“clean	room.”	There	can	be	no	dust,	no	skin	cells,	no	smoke
—not	even	bacteria.	Workers	must	wear	clean	room	suits,	often	called	“bunny	suits,”	to	ensure	that
even	the	tiniest	particle	does	not	escape	into	the	air.
The	 process	 begins	 with	 the	 chip	 design,	 which	 eventually	 results	 in	 a	 mask,	 the	 template	 or
blueprint	that	contains	the	circuit	patterns.	A	silicon	wafer	is	then	covered	by	an	insulating	layer	of
oxide,	followed	by	a	layer	of	photosensitive	film	called	photo-resist.	This	photo-resist	has	regions	that
break	down	under	UV	light	and	other	regions	that	do	not.	A	UV	light	is	then	shone	through	the	mask	(a
process	called	photolithography).	Bare	oxide	is	left	on	portions	where	the	photo-resist	breaks	down
under	the	UV	light.	Chemical	“etching”	is	then	used	to	dissolve	the	revealed	oxide	layer	and	also	to
remove	the	remaining	photo-resist	not	affected	by	the	UV	light.	The	“doping”	process	embeds	certain
impurities	 into	 the	 silicon	 that	 alters	 the	 electrical	 properties	 of	 the	 unprotected	 areas,	 basically
creating	 the	 transistors.	 The	 chip	 is	 then	 covered	 with	 another	 layer	 of	 both	 the	 insulating	 oxide
material	 and	 the	 photo-resist,	 and	 the	 entire	 process	 is	 repeated	 hundreds	 of	 times,	 each	 iteration
creating	a	new	layer	of	the	chip.	Different	masks	are	used	with	a	similar	process	to	create	the	wires
that	connect	the	components	on	the	chip.	The	circuit	is	finally	encased	in	a	protective	plastic	cover,
tested,	and	shipped	out.
As	 components	 become	 smaller	 and	 smaller,	 the	 equipment	 used	 to	 make	 them	 must	 be	 of
continually	higher	quality.	This	has	resulted	in	a	dramatic	increase	in	the	cost	of	manufacturing	ICs
over	the	years.	In	the	early	1980s,	the	cost	to	build	a	semiconductor	factory	was	roughly	$10	million.
By	the	late	1980s,	that	cost	had	risen	to	approximately	$200	million,	and	by	the	late	1990s,	an	IC
fabrication	factory	cost	more	or	less	around	$1	billion.	In	2005,	Intel	spent	approximately	$2	billion
for	a	single	fabrication	facility	and,	in	2007,	invested	roughly	$7	billion	to	retool	three	plants	in	order
to	 allow	 them	 to	 produce	 a	 smaller	 processor.	 In	 2009,	 AMD	 begin	 building	 a	 $4.2	 billion	 chip
manufacturing	facility	in	upstate	New	York.
The	manufacturing	facility	is	not	the	only	high-dollar	item	when	it	comes	to	making	ICs.	The	cost	to
design	a	chip	and	create	the	mask	can	run	anywhere	from	$1	million	to	$3	million—more	for	smaller
chips	and	less	for	larger	ones.	Considering	the	costs	of	both	the	chip	design	and	the	fabrication	facility,
it	 truly	 is	 amazing	 that	 we	 can	 walk	 into	 our	 local	 computer	 store	 and	 buy	 a	 new	 Intel	 i3
microprocessor	chip	for	around	$100.
1.5.6		Moore’s	Law
So	where	does	it	end?	How	small	can	we	make	transistors?	How	densely	can	we	pack	chips?	No	one	can
say	for	sure.	Every	year,	scientists	continue	to	thwart	prognosticators’	attempts	to	define	the	limits	of
integration.	In	fact,	more	than	one	skeptic	raised	an	eyebrow	when,	in	1965,	Intel	founder	Gordon	Moore
stated,	“The	density	of	transistors	in	an	integrated	circuit	will	double	every	year.”	The	current	version	of
this	 prediction	 is	 usually	 conveyed	 as	 “the	 density	 of	 silicon	 chips	 doubles	 every	 18	 months.”	 This
assertion	has	become	known	as	Moore’s	Law.	Moore	intended	this	postulate	to	hold	for	only	10	years.
However,	advances	in	chip	manufacturing	processes	have	allowed	this	assertion	to	hold	for	almost	40

years	(and	many	believe	it	will	continue	to	hold	well	into	the	2010s).
Yet,	using	current	technology,	Moore’s	Law	cannot	hold	forever.	There	are	physical	and	financial
limitations	that	must	ultimately	come	into	play.	At	the	current	rate	of	miniaturization,	it	would	take	about
500	years	to	put	the	entire	solar	system	on	a	chip!	Clearly,	the	limit	lies	somewhere	between	here	and
there.	Cost	may	be	the	ultimate	constraint.	Rock’s	Law,	proposed	by	early	Intel	capitalist	Arthur	Rock,	is
a	corollary	to	Moore’s	Law:	“The	cost	of	capital	equipment	to	build	semiconductors	will	double	every
four	years.”	Rock’s	Law	arises	from	the	observations	of	a	financier	who	saw	the	price	tag	of	new	chip
facilities	escalate	from	about	$12,000	in	1968	to	$12	million	in	the	mid-1990s.	In	2005,	the	cost	of
building	a	new	chip	plant	was	nearing	$3	billion.	At	this	rate,	by	the	year	2035,	not	only	will	the	size	of	a
memory	element	be	smaller	than	an	atom,	but	it	would	also	require	the	entire	wealth	of	the	world	to	build
a	single	chip!	So	even	if	we	continue	to	make	chips	smaller	and	faster,	the	ultimate	question	may	be
whether	we	can	afford	to	build	them.
Certainly,	if	Moore’s	Law	is	to	hold,	Rock’s	Law	must	fall.	It	is	evident	that	for	these	two	things	to
happen,	computers	must	shift	to	a	radically	different	technology.	Research	into	new	computing	paradigms
has	 been	 proceeding	 in	 earnest	 during	 the	 last	 half	 decade.	 Laboratory	 prototypes	 fashioned	 around
organic	computing,	superconducting,	molecular	physics,	and	quantum	computing	have	been	demonstrated.
Quantum	computers,	which	leverage	the	vagaries	of	quantum	mechanics	to	solve	computational	problems,
are	 particularly	 exciting.	 Not	 only	 would	 quantum	 systems	 compute	 exponentially	 faster	 than	 any
previously	used	method,	but	they	would	also	revolutionize	the	way	in	which	we	define	computational
problems.	Problems	that	today	are	considered	ludicrously	infeasible	could	be	well	within	the	grasp	of	the
next	generation’s	schoolchildren.	These	school-children	may,	in	fact,	chuckle	at	our	“primitive”	systems
in	the	same	way	that	we	are	tempted	to	chuckle	at	the	ENIAC.
1.6			THE	COMPUTER	LEVEL	HIERARCHY
If	a	machine	is	to	be	capable	of	solving	a	wide	range	of	problems,	it	must	be	able	to	execute	programs
written	in	different	languages,	from	Fortran	and	C	to	Lisp	and	Prolog.	As	we	shall	see	in	Chapter	3,	the
only	 physical	 components	 we	 have	 to	 work	 with	 are	 wires	 and	 gates.	 A	 formidable	 open	 space—a
semantic	gap—exists	between	these	physical	components	and	a	high-level	language	such	as	C++.	For	a
system	to	be	practical,	the	semantic	gap	must	be	invisible	to	most	of	the	users	of	the	system.
Programming	experience	teaches	us	that	when	a	problem	is	large,	we	should	break	it	down	and	use	a
“divide	and	conquer”	approach.	In	programming,	we	divide	a	problem	into	modules	and	then	design	each
module	separately.	Each	module	performs	a	specific	task,	and	modules	need	only	know	how	to	interface
with	other	modules	to	make	use	of	them.
Computer	 system	 organization	 can	 be	 approached	 in	 a	 similar	 manner.	 Through	 the	 principle	 of
abstraction,	we	can	imagine	the	machine	to	be	built	from	a	hierarchy	of	levels,	in	which	each	level	has	a
specific	function	and	exists	as	a	distinct	hypothetical	machine.	We	call	the	hypothetical	computer	at	each
level	a	virtual	machine.	 Each	 level’s	 virtual	 machine	 executes	 its	 own	 particular	 set	 of	 instructions,
calling	 upon	 machines	 at	 lower	 levels	 to	 carry	 out	 the	 tasks	 when	 necessary.	 By	 studying	 computer
organization,	you	will	see	the	rationale	behind	the	hierarchy’s	partitioning,	as	well	as	how	these	layers
are	 implemented	 and	 interface	 with	 each	 other.	Figure	 1.3	 shows	 the	 commonly	 accepted	 layers
representing	the	abstract	virtual	machines.
Level	6,	the	User	Level,	is	composed	of	applications	and	is	the	level	with	which	everyone	is	most
familiar.	At	this	level,	we	run	programs	such	as	word	processors,	graphics	packages,	or	games.	The

lower	levels	are	nearly	invisible	from	the	User	Level.
Level	5,	the	High-Level	Language	Level,	consists	of	languages	such	as	C,	C++,	Fortran,	Lisp,	Pascal,
and	Prolog.	These	languages	must	be	translated	(using	either	a	compiler	or	an	interpreter)	to	a	language
the	 machine	 can	 understand.	 Compiled	 languages	 are	 translated	 into	 assembly	 language	 and	 then
assembled	into	machine	code.	(They	are	translated	to	the	next	lower	level.)	The	user	at	this	level	sees
very	little	of	the	lower	levels.	Even	though	a	programmer	must	know	about	data	types	and	the	instructions
available	for	those	types,	he	or	she	need	not	know	about	how	those	types	are	actually	implemented.
FIGURE	1.3	The	Abstract	Levels	of	Modern	Computing	Systems
Level	4,	the	Assembly	Language	Level,	encompasses	some	type	of	assembly	language.	As	previously
mentioned,	 compiled	 higher-level	 languages	 are	 first	 translated	 to	 assembly,	 which	 is	 then	 directly
translated	 to	 machine	 language.	 This	 is	 a	 one-to-one	 translation,	 meaning	 that	 one	 assembly	 language
instruction	is	translated	to	exactly	one	machine	language	instruction.	By	having	separate	levels,	we	reduce
the	semantic	gap	between	a	high-level	language,	such	as	C++,	and	the	actual	machine	language	(which
consists	of	0s	and	1s).
Level	 3,	 the	 System	 Software	 Level,	 deals	 with	 operating	 system	 instructions.	 This	 level	 is
responsible	 for	 multiprogramming,	 protecting	 memory,	 synchronizing	 processes,	 and	 various	 other
important	functions.	Often,	instructions	translated	from	assembly	language	to	machine	language	are	passed
through	this	level	unmodified.

Level	2,	the	Instruction	Set	Architecture	(ISA),	or	Machine	Level,	consists	of	the	machine	language
recognized	by	the	particular	architecture	of	the	computer	system.	Programs	written	in	a	computer’s	true
machine	 language	 on	 a	 hardwired	 computer	 (see	 below)	 can	 be	 executed	 directly	 by	 the	 electronic
circuits	without	any	interpreters,	translators,	or	compilers.	We	will	study	ISAs	in	depth	in	Chapters	4	and
5.
Level	1,	the	Control	Level,	is	where	a	control	unit	makes	sure	that	instructions	are	decoded	and
executed	properly	and	that	data	is	moved	where	and	when	it	should	be.	The	control	unit	interprets	the
machine	instructions	passed	to	it,	one	at	a	time,	from	the	level	above,	causing	the	required	actions	to	take
place.
Control	 units	 can	 be	 designed	 in	 one	 of	 two	 ways:	 They	 can	 be	hardwired	 or	 they	 can	 be
microprogrammed.	 In	 hardwired	 control	 units,	 control	 signals	 emanate	 from	 blocks	 of	 digital	 logic
components.	These	signals	direct	all	the	data	and	instruction	traffic	to	appropriate	parts	of	the	system.
Hardwired	control	units	are	typically	very	fast	because	they	are	actually	physical	components.	However,
once	implemented,	they	are	very	difficult	to	modify	for	the	same	reason.
The	other	option	for	control	is	to	implement	instructions	using	a	microprogram.	A	microprogram	is	a
program	 written	 in	 a	 low-level	 language	 that	 is	 implemented	 directly	 by	 the	 hardware.	 Machine
instructions	produced	in	Level	2	are	fed	into	this	microprogram,	which	then	interprets	the	instructions	by
activating	 hardware	 suited	 to	 execute	 the	 original	 instruction.	 One	 machine-level	 instruction	 is	 often
translated	into	several	microcode	instructions.	This	is	not	the	one-to-one	correlation	that	exists	between
assembly	 language	 and	 machine	 language.	 Microprograms	 are	 popular	 because	 they	 can	 be	 modified
relatively	 easily.	 The	 disadvantage	 of	 microprogramming	 is,	 of	 course,	 that	 the	 additional	 layer	 of
translation	typically	results	in	slower	instruction	execution.
Level	0,	the	Digital	Logic	Level,	is	where	we	find	the	physical	components	of	the	computer	system:
the	gates	and	wires.	These	are	the	fundamental	building	blocks,	the	implementations	of	the	mathematical
logic,	that	are	common	to	all	computer	systems.	Chapter	3	presents	the	Digital	Logic	Level	in	detail.
1.7			CLOUD	COMPUTING:	COMPUTING	AS	A	SERVICE
We	must	never	forget	that	the	ultimate	aim	of	every	computer	system	is	to	deliver	functionality	to	its	users.
Computer	users	typically	do	not	care	about	terabytes	of	storage	and	gigahertz	of	processor	speed.	In	fact,
many	 companies	 and	 government	 agencies	 have	 “gotten	 out	 of	 the	 technology	 business”	 entirely	 by
outsourcing	their	data	centers	to	third-party	specialists.	These	outsourcing	agreements	tend	to	be	highly
complex	and	prescribe	every	aspect	of	the	hardware	configuration.	Along	with	the	detailed	hardware
specifications,	service-level	 agreements	 (SLAs)	 provide	 penalties	 if	 certain	 parameters	 of	 system
performance	and	availability	are	not	met.	Both	contracting	parties	employ	individuals	whose	main	job	is
to	 monitor	 the	 contract,	 calculate	 bills,	 and	 determine	 SLA	 penalties	 when	 needed.	 Thus,	 with	 the
additional	administrative	overhead,	data	center	outsourcing	is	neither	a	cheap	nor	an	easy	solution	for
companies	that	want	to	avoid	the	problems	of	technology	management.

FIGURE	1.4	Levels	of	Computing	as	a	Service
A	 somewhat	 easier	 approach	 may	 be	 found	 in	 the	 emerging	 field	 of	 Cloud	 computing.	Cloud
computing	is	the	general	term	for	any	type	of	virtual	computing	platform	provided	over	the	Internet.	A
Cloud	computing	platform	is	defined	in	terms	of	the	services	that	it	provides	rather	than	its	physical
configuration.	Its	name	derives	from	the	cloud	icon	that	symbolizes	the	Internet	on	schematic	diagrams.
But	the	metaphor	carries	well	into	the	actual	Cloud	infrastructure,	because	the	computer	is	more	abstract
than	real.	The	“computer”	and	“storage”	appear	to	the	user	as	a	single	entity	in	the	Cloud	but	usually	span
several	 physical	 servers.	 The	 storage	 is	 usually	 located	 on	 an	 array	 of	 disks	 that	 are	 not	 directly
connected	to	any	particular	server.	System	software	is	designed	to	give	this	configuration	the	illusion	of
being	a	single	system;	thus,	we	say	that	it	presents	a	virtual	machine	to	the	user.
Cloud	computing	services	can	be	defined	and	delivered	in	a	number	of	ways	based	on	levels	of	the
computer	hierarchy	shown	again	in	Figure	1.4.	At	the	top	of	the	hierarchy,	where	we	have	executable
programs,	 a	 Cloud	 provider	 might	 offer	 an	 entire	 application	 over	 the	 Internet,	 with	 no	 components
installed	locally.	This	is	called	Software	as	a	Service,	or	SaaS.	The	consumer	of	this	service	does	not
maintain	 the	 application	 or	 need	 to	 be	 at	 all	 concerned	 with	 the	 infrastructure	 in	 any	 way.	 SaaS
applications	tend	to	focus	on	narrow,	non-business-critical	applications.	Well-known	examples	include
Gmail,	 Dropbox,	 GoToMeeting,	 and	 Netflix.	 Specialized	 products	 are	 available	 for	 tax	 return
preparation,	payroll,	fleet	management,	and	case	management,	to	name	only	a	few.	Salesforce.com	is	a

pioneering,	full-featured	SaaS	offering	designed	for	customer	relationship	management.	Fee-based	SaaS
is	typically	billed	monthly	according	to	the	number	of	users,	sometimes	with	per-transaction	fees	added
on	as	well.
A	great	disadvantage	of	SaaS	is	that	the	consumer	has	little	control	over	the	behavior	of	the	product.
This	may	be	problematic	if	a	company	has	to	make	radical	changes	to	its	processes	or	policies	in	order	to
use	a	SaaS	product.	Companies	that	desire	to	have	more	control	over	their	applications,	or	that	need
applications	 for	 which	 SaaS	 is	 unavailable,	 might	 instead	 opt	 to	 deploy	 their	 own	 applications	 on	 a
Cloud-hosted	 environment	 called	Platform	as	a	Service,	 or	PaaS.	 PaaS	 provides	 server	 hardware,
operating	systems,	database	services,	security	components,	and	backup	and	recovery	services.	The	PaaS
provider	manages	performance	and	availability	of	the	environment,	whereas	the	customer	manages	the
applications	hosted	in	the	PaaS	Cloud.	The	customer	is	typically	billed	monthly	per	megabytes	of	storage,
processor	utilization,	and	megabytes	of	data	transferred.	Well-known	PaaS	providers	include	Google	App
Engine	 and	 Microsoft	 Windows	 Azure	 Cloud	 Services	 [as	 well	 as	Force.com	 (PaaS	 provided	 by
Salesforce.com)].
PaaS	is	not	a	good	fit	in	situations	where	rapid	configuration	changes	are	required.	This	would	be	the
case	if	a	company’s	main	business	is	software	development.	The	formality	of	change	processes	necessary
to	a	well-run	PaaS	operation	impedes	rapid	software	deployment	[by	forcing	a	company	to	play	by	the
service	provider’s	rules].	Indeed,	in	any	company	where	staff	is	capable	of	managing	operating	system
and	database	software,	the	Infrastructure	as	a	Service	(IaaS)	Cloud	model	might	be	the	best	option.
IaaS,	 [the	 most	 basic	 of	 the	 models,]	provides	 only	 server	 hardware,	 secure	 network	 access	 to	 the
servers,	and	backup	and	recovery	services.	The	customer	is	responsible	for	all	system	software	including
the	 operating	 system	 and	 databases.	 IaaS	 is	 typically	 billed	 by	 the	 number	 of	 virtual	 machines	 used,
megabytes	of	storage,	and	megabytes	of	data	transferred,	but	at	a	lower	rate	than	PaaS.	The	biggest	names
in	IaaS	include	Amazon	EC2,	Google	Compute	Engine,	Microsoft	Azure	Services	Platform,	Rackspace,
and	HP	Cloud.
Not	only	do	PaaS	and	IaaS	liberate	the	customer	from	the	difficulties	of	data	center	management,	they
also	provide	elasticity:	the	ability	to	add	and	remove	resources	based	on	demand.	A	customer	pays	for
only	as	much	infrastructure	as	is	needed.	So	if	a	business	has	a	peak	season,	extra	capacity	needs	to	be
allocated	only	for	the	duration	of	the	peak	period.	This	flexibility	can	save	a	company	a	great	deal	of
money	when	it	has	large	variations	in	computing	demands.
Cloud	storage	 is	 a	 limited	 type	 of	 IaaS.	 The	 general	 public	 can	 obtain	 small	 amounts	 of	 Cloud
storage	inexpensively	through	services	such	as	Dropbox,	Google	Drive,	and	Amazon.com’s	Cloud	Drive
—to	name	only	a	few	among	a	crowded	field.	Google,	Amazon,	HP,	IBM,	and	Microsoft	are	among
several	 vendors	 that	 provide	 Cloud	 storage	 for	 the	 enterprise.	 As	 with	 Cloud	 computing	 in	 general,
enterprise-grade	Cloud	storage	also	requires	careful	management	of	performance	and	availability.
The	question	that	all	potential	Cloud	computing	customers	must	ask	themselves	is	whether	it	is	less
expensive	to	maintain	their	own	data	center	or	to	buy	Cloud	services—including	the	allowances	for	peak
periods.	 Moreover,	 as	 with	 traditional	 outsourcing,	 vendor-provided	 Cloud	 computing	 still	 involves
considerable	contract	negotiation	and	management	on	the	part	of	both	parties.	SLA	management	remains
an	 important	 activity	 in	 the	 relationship	 between	 the	 service	 provider	 and	 the	 service	 consumer.
Moreover,	once	an	enterprise	moves	its	assets	to	the	Cloud,	it	might	be	difficult	to	transition	back	to	a
company-owned	data	center,	should	the	need	arise.	Thus,	any	notion	of	moving	assets	to	the	Cloud	must
be	carefully	considered,	and	the	risks	clearly	understood.
The	 Cloud	 also	 presents	 a	 number	 of	 challenges	 to	 computer	 scientists.	 First	 and	 foremost	 is	 the
technical	configuration	of	the	data	center.	The	infrastructure	must	provide	for	uninterrupted	service,	even

during	 maintenance	 activities.	 It	 must	 permit	 expedient	 allocation	 of	 capacity	 to	 where	 it	 is	 needed
without	degrading	or	interrupting	services.	Performance	of	the	infrastructure	must	be	carefully	monitored
and	 interventions	 taken	 whenever	 performance	 falls	 below	 certain	 defined	 thresholds;	 otherwise,
monetary	SLA	penalties	may	be	incurred.
On	the	consumer	side	of	the	Cloud,	software	architects	and	programmers	must	be	mindful	of	resource
consumption,	 because	 the	 Cloud	 model	 charges	 fees	 in	 proportion	 to	 the	 resources	 consumed.	 These
resources	 include	 communications	 bandwidth,	 processor	 cycles,	 and	 storage.	 Thus,	 to	 save	 money,
application	programs	should	be	designed	to	reduce	trips	over	the	network,	economize	machine	cycles,
and	minimize	bytes	of	storage.	Meticulous	testing	is	crucial	prior	to	deploying	a	program	in	the	Cloud:	An
errant	module	that	consumes	resources,	say,	in	an	infinite	loop,	could	result	in	a	“surprising”	Cloud	bill	at
the	end	of	the	month.
With	 the	 cost	 and	 complexity	 of	 data	 centers	 continuing	 to	 rise—with	 no	 end	 in	 sight—Cloud
computing	is	almost	certain	to	become	the	platform	of	choice	for	medium-	to	small-sized	businesses.	But
the	 Cloud	 is	 not	 worry-free.	 A	 company	 might	 end	 up	 trading	 its	 technical	 challenges	 for	 even	 more
vexing	supplier	management	challenges.
1.8			THE	VON	NEUMANN	MODEL
In	the	earliest	electronic	computing	machines,	programming	was	synonymous	with	connecting	wires	to
plugs.	No	layered	architecture	existed,	so	programming	a	computer	was	as	much	of	a	feat	of	electrical
engineering	as	it	was	an	exercise	in	algorithm	design.	Before	their	work	on	the	ENIAC	was	complete,
John	 W.	 Mauchly	 and	 J.	 Presper	 Eckert	 conceived	 of	 an	 easier	 way	 to	 change	 the	 behavior	 of	 their
calculating	 machine.	 They	 reckoned	 that	 memory	 devices,	 in	 the	 form	 of	 mercury	 delay	 lines,	 could
provide	a	way	to	store	program	instructions.	This	would	forever	end	the	tedium	of	rewiring	the	system
each	time	it	had	a	new	problem	to	solve,	or	an	old	one	to	debug.	Mauchly	and	Eckert	documented	their
idea,	proposing	it	as	the	foundation	for	their	next	computer,	the	EDVAC.	Unfortunately,	while	they	were
involved	in	the	top	secret	ENIAC	project	during	World	War	II,	Mauchly	and	Eckert	could	not	immediately
publish	their	insight.
No	 such	 proscriptions,	 however,	 applied	 to	 a	 number	 of	 people	 working	 at	 the	 periphery	 of	 the
ENIAC	project.	One	of	these	people	was	a	famous	Hungarian	mathematician	named	John	von	Neumann
(pronounced	von	noy-man).	After	reading	Mauchly	and	Eckert’s	proposal	for	the	EDVAC,	von	Neumann
published	and	publicized	the	idea.	So	effective	was	he	in	the	delivery	of	this	concept	that	history	has
credited	him	with	its	invention.	All	stored-program	computers	have	come	to	be	known	as	von	Neumann
systems	using	the	von	Neumann	architecture.	Although	we	are	compelled	by	tradition	to	say	that	stored-
program	computers	use	the	von	Neumann	architecture,	we	shall	not	do	so	without	paying	proper	tribute	to
its	true	inventors:	John	W.	Mauchly	and	J.	Presper	Eckert.
Today’s	 version	 of	 the	 stored-program	 machine	 architecture	 satisfies	 at	 least	 the	 following
characteristics:
•	 	 	 Consists	 of	 three	 hardware	 systems:	 A	central	 processing	 unit	 (CPU)	 with	 a	 control	 unit,	 an
arithmetic	logic	unit	(ALU),	registers	(small	storage	areas),	and	a	program	counter;	a	main	memory
system,	which	holds	programs	that	control	the	computer’s	operation;	and	an	I/O	system.
•			Capacity	to	carry	out	sequential	instruction	processing.
•			Contains	a	single	path,	either	physically	or	logically,	between	the	main	memory	system	and	the	control

unit	 of	 the	 CPU,	 forcing	 alternation	 of	 instruction	 and	 execution	 cycles.	 This	 single	 path	 is	 often
referred	to	as	the	von	Neumann	bottleneck.
Figure	1.5	shows	how	these	features	work	together	in	modern	computer	systems.	Notice	that	the	system
shown	in	the	figure	passes	all	of	its	I/O	through	the	arithmetic	logic	unit	(actually,	it	passes	through	the
accumulator,	which	is	part	of	the	ALU).	This	architecture	runs	programs	in	what	is	known	as	the	von
Neumann	execution	cycle	 (also	 called	 the	fetch-decode-execute	cycle),	 which	 describes	 how	 the
machine	works.	One	iteration	of	the	cycle	is	as	follows:
1.		The	control	unit	fetches	the	next	program	instruction	from	the	memory,	using	the	program	counter	to
determine	where	the	instruction	is	located.
2.		The	instruction	is	decoded	into	a	language	the	ALU	can	understand.
3.		Any	data	operands	required	to	execute	the	instruction	are	fetched	from	memory	and	placed	in	registers
in	the	CPU.
4.		The	ALU	executes	the	instruction	and	places	the	results	in	registers	or	memory.
The	ideas	present	in	the	von	Neumann	architecture	have	been	extended	so	that	programs	and	data
stored	in	a	slow-to-access	storage	medium,	such	as	a	hard	disk,	can	be	copied	to	a	fast-access,	volatile
storage	medium	such	as	RAM	prior	to	execution.	This	architecture	has	also	been	streamlined	into	what	is
currently	called	the	system	bus	model,	which	is	shown	in	Figure	1.6.	The	data	bus	moves	data	from	main
memory	to	the	CPU	registers	(and	vice	versa).	The	address	bus	holds	the	address	of	the	data	that	the	data
bus	is	currently	accessing.	The	control	bus	carries	the	necessary	control	signals	that	specify	how	the
information	transfer	is	to	take	place.

FIGURE	1.5	The	von	Neumann	Architecture
FIGURE	1.6	The	Modified	von	Neumann	Architecture,	Adding	a	System	Bus
Other	enhancements	to	the	von	Neumann	architecture	include	using	index	registers	for	addressing,
adding	 floating-point	 data,	 using	 interrupts	 and	 asynchronous	 I/O,	 adding	 virtual	 memory,	 and	 adding
general	registers.	You	will	learn	a	great	deal	about	these	enhancements	in	the	chapters	that	follow.
Quantum	Leap	for	Computers:	How	Small	Can	We	Go?
VLSI	technology	has	allowed	us	to	put	billions	of	transistors	on	a	single	chip,	but	there	is	a	limit	to
how	small	we	can	go	with	current	transistor	technology.	Researchers	at	the	University	of	New	South
Wales’	 Centre	 for	 Quantum	 Computer	 Technology	 and	 the	 University	 of	 Wisconsin–Madison	 have
taken	“small”	to	an	entirely	new	level.	In	May	2010,	they	announced	the	7-atom	transistor,	a	working
transistor	embedded	in	silicon	that	is	only	7	atoms	in	size.	Transistors	1	atom	in	size	that	allowed	the
flows	of	electrons	were	reported	as	early	as	2002,	but	this	transistor	is	different	in	that	it	provides	all
the	functionality	of	a	transistor	as	we	know	it	today.
The	7-atom	transistor	was	created	by	hand,	using	a	scanning	tunneling	microscope.	It’s	a	long	way
from	being	mass	produced,	but	the	researchers	hope	to	make	it	commercially	available	by	2015.	The
transistor’s	 tiny	 size	 means	 smaller	 but	 more	 powerful	 computers.	 Experts	 estimate	 it	 may	 shrink
microchips	by	a	factor	of	100,	while	enabling	an	exponential	speedup	in	processing.	This	means	our
computers	could	become	one	hundred	times	smaller,	but	at	the	same	time,	also	one	hundred	times
faster.
In	addition	to	replacing	traditional	transistors,	this	discovery	may	be	fundamental	in	the	efforts	to
build	a	quantum	computer	in	silicon.	Quantum	computing	is	expected	to	be	the	next	significant	leap	in
computer	technology.	Small	quantum	computers	now	exist	that	perform	calculations	millions	of	times
faster	than	conventional	computers,	but	these	computers	are	too	small	to	be	of	much	use.	A	large-scale,
working	quantum	computer	would	enable	us	to	perform	calculations	and	solve	problems	that	would
take	a	conventional	computer	more	than	13	billion	years.	That	could	change	the	way	we	view	the
world.	For	one	thing,	every	encryption	algorithm	employed	today	would	be	useless	against	that	kind	of
computing	 power.	 On	 the	 other	 hand,	 ultra-secure	 communications	 would	 be	 possible	 using	 new

quantum	technologies.
Quantum	computers	have	significant	potential.	Current	applications,	including	special	effects	for
movies,	cryptography,	searching	large	data	files,	factoring	large	numbers,	simulating	various	systems
(such	as	nuclear	explosions	and	weather	patterns),	military	and	intelligence	gathering,	and	intensive,
time-consuming	computations	(such	as	those	found	in	astronomy,	physics,	and	chemistry),	would	all
see	tremendous	performance	increases	if	quantum	computing	were	used.	New	applications	we	have	not
yet	discovered	are	likely	to	evolve	as	well.
In	addition	to	its	potential	to	change	computing	as	we	know	it	today,	this	new	7-atom	transistor	is
significant	for	another	reason.	Recall	Moore’s	Law;	this	law	is	not	so	much	a	law	of	nature,	but	rather
an	expectation	of	innovation	and	a	significant	driving	force	in	chip	design.	Moore’s	Law	has	held	since
1965,	but	in	order	to	do	so,	chip	manufacturers	have	jumped	from	one	technology	to	another.	Gordon
Moore	himself	has	predicted	that,	if	restricted	to	CMOS	silicon,	his	law	will	fail	sometime	around
2020.	The	discovery	of	this	7-atom	transistor	gives	new	life	to	Moore’s	Law—and	we	suspect	that
Gordon	 Moore	 is	 breathing	 a	 sigh	 of	 relief	 over	 its	 discovery.	 However,	 noted	 physicist	 Stephen
Hawking	has	explained	that	chip	manufacturers	are	limited	in	their	quest	to	“enforce”	Moore’s	Law	by
two	fundamental	constraints:	the	speed	of	light	and	the	atomic	nature	of	matter,	implying	that	Moore’s
Law	will	eventually	fail,	regardless	of	the	technology	being	used.
1.9			NON–VON	NEUMANN	MODELS
Until	 recently,	 almost	 all	 general-purpose	 computers	 followed	 the	 von	 Neumann	 design.	 That	 is,	 the
architecture	consisted	of	a	CPU,	memory,	and	I/O	devices,	and	they	had	single	storage	for	instructions	and
data,	as	well	as	a	single	bus	used	for	fetching	instructions	and	transferring	data.	von	Neumann	computers
execute	 instructions	 sequentially	 and	 are	 therefore	 extremely	 well	 suited	 to	 sequential	 processing.
However,	 the	 von	 Neumann	 bottleneck	 continues	 to	 baffle	 engineers	 looking	 for	 ways	 to	 build	 fast
systems	that	are	inexpensive	and	compatible	with	the	vast	body	of	commercially	available	software.
Engineers	who	are	not	constrained	by	the	need	to	maintain	compatibility	with	von	Neumann	systems
are	free	to	use	many	different	models	of	computing.	Non–von	Neumann	architectures	are	those	in	which
the	model	of	computation	varies	from	the	characteristics	listed	for	the	von	Neumann	architecture.	For
example,	an	architecture	that	does	not	store	programs	and	data	in	memory	or	does	not	process	a	program
sequentially	would	be	considered	a	non–von	Neumann	machine.	Also,	a	computer	that	has	two	buses,	one
for	 data	 and	 a	 separate	 one	 for	 instructions,	 would	 be	 considered	 a	 non–von	 Neumann	 machine.
Computers	designed	using	the	Harvard	architecture	have	two	buses,	thus	allowing	data	and	instructions
to	be	transferred	simultaneously,	but	also	have	separate	storage	for	data	and	instructions.	Many	modern
general-purpose	 computers	 use	 a	 modified	 version	 of	 the	 Harvard	 architecture	 in	 which	 they	 have
separate	 pathways	 for	 data	 and	 instructions	 but	 not	 separate	 storage.	 Pure	 Harvard	 architectures	 are
typically	used	in	microcontrollers	(an	entire	computer	system	on	a	chip),	such	as	those	found	in	embedded
systems,	as	in	appliances,	toys,	and	cars.
Many	non–von	Neumann	machines	are	designed	for	special	purposes.	The	first	recognized	non–von
Neumann	processing	chip	was	designed	strictly	for	image	processing.	Another	example	is	a	reduction
machine	 (built	 to	 perform	 combinatory	 logic	 calculations	 using	 graph	 reduction).	 Other	 non–von
Neumann	computers	include	digital	signal	processors	(DSPs)	and	media	processors,	which	can	execute
a	single	instruction	on	a	set	of	data	(instead	of	executing	a	single	instruction	on	a	single	piece	of	data).
A	number	of	different	subfields	fall	into	the	non–von	Neumann	category,	including	neural	networks

(using	 ideas	 from	 models	 of	 the	 brain	 as	 a	 computing	 paradigm)	 implemented	 in	 silicon,	cellular
automata,	cognitive	computers	(machines	that	learn	by	experience	rather	than	through	programming,
including	IBM’s	SyNAPSE	computer,	a	machine	that	models	the	human	brain),	quantum	computation	(a
combination	of	computing	and	quantum	physics),	dataflow	computation,	and	parallel	computers.	These
all	have	something	in	common—the	computation	is	distributed	among	different	processing	units	that	act	in
parallel.	They	differ	in	how	weakly	or	strongly	the	various	components	are	connected.	Of	these,	parallel
computing	is	currently	the	most	popular.
1.10			PARALLEL	PROCESSORS	AND	PARALLEL	COMPUTING
Today,	parallel	processing	solves	some	of	our	biggest	problems	in	much	the	same	way	that	settlers	of	the
Old	West	solved	their	biggest	problems	using	parallel	oxen.	If	they	were	using	an	ox	to	move	a	tree	and
the	ox	was	not	big	enough	or	strong	enough,	they	certainly	didn’t	try	to	grow	a	bigger	ox—they	used	two
oxen.	If	our	computer	isn’t	fast	enough	or	powerful	enough,	instead	of	trying	to	develop	a	faster,	more
powerful	computer,	why	not	simply	use	multiple	computers?	This	is	precisely	what	parallel	computing
does.	The	first	parallel	processing	systems	were	built	in	the	late	1960s	and	had	only	two	processors.	The
1970s	saw	the	introduction	of	supercomputers	with	as	many	as	32	processors,	and	the	1980s	brought	the
first	 systems	 with	 more	 than	 1000	 processors.	 Finally,	 in	 1999,	 IBM	 announced	 funding	 for	 the
development	of	a	supercomputer	architecture	called	the	Blue	Gene	series.	The	first	computer	in	this
series,	the	Blue	Gene/L,	is	a	massively	parallel	computer	containing	131,000	dual-core	processors,	each
with	 its	 own	 dedicated	 memory.	 In	 addition	 to	 allowing	 researchers	 to	 study	 the	 behavior	 of	 protein
folding	(by	using	large	simulations),	this	computer	has	also	allowed	researchers	to	explore	new	ideas	in
parallel	architectures	and	software	for	those	architectures.	IBM	has	continued	to	add	computers	to	this
series.	The	Blue	Gene/P	appeared	in	2007	and	has	quad-core	processors.	The	latest	computer	designed
for	this	series,	the	Blue	Gene/Q,	uses	16-core	processors,	with	1024	compute	nodes	per	rack,	scalable
up	to	512	racks.	Installations	of	the	Blue	Gene/Q	computer	include	Nostromo	(being	used	for	biomedical
data	in	Poland),	Sequoia	(being	used	at	Lawrence	Livermore	National	Laboratory	for	nuclear	simulations
and	scientific	research),	and	Mira	(used	at	Argonne	National	Laboratory).
Dual-core	 and	 quad-core	 processors	 (and	 higher,	 as	 we	 saw	 in	 Blue	 Gene/Q)	 are	 examples	 of
multicore	processors.	But	what	is	a	multicore	processor?	Essentially,	it	is	a	special	type	of	parallel
processor.	 Parallel	 processors	 are	 often	 classified	 as	 either	 “shared	 memory”	 processors	 (in	 which
processors	 all	 share	 the	 same	 global	 memory)	 or	 “distributed	 memory”	 computers	 (in	 which	 each
processor	has	its	own	private	memory).	Chapter	9	covers	parallel	processors	in	detail.	The	following
discussion	is	limited	to	shared	memory	multicore	architectures—the	type	used	in	personal	computers.
Multicore	architectures	are	parallel	processing	machines	that	allow	for	multiple	processing	units
(often	called	cores)	on	a	single	chip.	Dual	core	means	2	cores;	quad	core	machines	have	4	cores;	and	so
on.	But	what	is	a	core?	Instead	of	a	single	processing	unit	in	an	integrated	circuit	(as	found	in	typical	von
Neumann	machines),	independent	multiple	cores	are	“plugged	in”	and	run	in	parallel.	Each	processing
unit	has	its	own	ALU	and	set	of	registers,	but	all	processors	share	memory	and	some	other	resources.
“Dual	 core”	 is	 different	 from	 “dual	 processor.”	 Dual-processor	 machines,	 for	 example,	 have	 two
processors,	but	each	processor	plugs	into	the	motherboard	separately.	The	important	distinction	to	note	is
that	all	cores	in	multicore	machines	are	integrated	into	the	same	chip.	This	means	that	you	could,	for
example,	 replace	 a	 single-core	 (uniprocessor)	 chip	 in	 your	 computer	 with,	 for	 example,	 a	 dual-core
processor	chip	(provided	your	computer	had	the	appropriate	socket	for	the	new	chip).	Many	computers

today	are	advertised	as	dual	core,	quad	core,	or	higher.	Dual	core	is	generally	considered	the	standard	in
today’s	 computers.	 Although	 most	 desktop	 and	 laptop	 computers	 have	 limited	 cores	 (fewer	 than	 8),
machines	with	hundreds	of	cores	are	available	for	the	right	price,	of	course.
Just	because	your	computer	has	multiple	cores	does	not	mean	it	will	run	your	programs	more	quickly.
Application	 programs	 (including	 operating	 systems)	 must	 be	 written	 to	 take	 advantage	 of	 multiple
processing	units	(this	statement	is	true	for	parallel	processing	in	general).	Multicore	computers	are	very
useful	for	multitasking—when	users	are	doing	more	than	one	thing	at	a	time.	For	example,	you	may	be
reading	email,	listening	to	music,	browsing	the	Web,	and	burning	a	DVD	all	at	the	same	time.	These
“multiple	 tasks”	 can	 be	 assigned	 to	 different	 processors	 and	 carried	 out	 in	 parallel,	 provided	 the
operating	system	is	able	to	manipulate	many	tasks	at	once.
In	addition	to	multitasking,	multithreading	can	also	increase	the	performance	of	any	application	with
inherent	parallelism.	Programs	are	divided	into	threads,	which	can	be	thought	of	as	mini-processes.	For
example,	a	Web	browser	is	multithreaded;	one	thread	can	download	text,	while	each	image	is	controlled
and	 downloaded	 by	 a	 separate	 thread.	 If	 an	 application	 is	 multithreaded,	 separate	 threads	 can	 run	 in
parallel	 on	 different	 processing	 units.	 We	 should	 note	 that	 even	 on	 uniprocessors,	 multithreading	 can
improve	 performance,	 but	 this	 is	 a	 discussion	 best	 left	 for	 another	 time.	 For	 more	 information,	 see
Stallings	(2012).
To	 summarize,	 parallel	 processing	 refers	 to	 a	 collection	 of	 different	 architectures,	 from	 multiple
separate	computers	working	together,	to	multiple	processors	sharing	memory,	to	multiple	cores	integrated
onto	the	same	chip.	Parallel	processors	are	technically	not	classified	as	von	Neumann	machines	because
they	do	not	process	instructions	sequentially.	However,	many	argue	that	parallel	processing	computers
contain	CPUs,	use	program	counters,	and	store	both	programs	and	data	in	main	memory,	which	makes
them	more	like	an	extension	to	the	von	Neumann	architecture	rather	than	a	departure	from	it;	these	people
view	parallel	processing	computers	as	sets	of	cooperating	von	Neumann	machines.	In	this	regard,	perhaps
it	is	more	appropriate	to	say	that	parallel	processing	exhibits	“non–von	Neumannness.”	Regardless	of
how	parallel	processors	are	classified,	parallel	computing	allows	us	to	multitask	and	to	solve	larger	and
more	complex	problems,	and	is	driving	new	research	in	various	software	tools	and	programming.
Even	parallel	computing	has	its	limits,	however.	As	the	number	of	processors	increases,	so	does	the
overhead	of	managing	how	tasks	are	distributed	to	those	processors.	Some	parallel	processing	systems
require	extra	processors	just	to	manage	the	rest	of	the	processors	and	the	resources	assigned	to	them.	No
matter	how	many	processors	we	place	in	a	system,	or	how	many	resources	we	assign	to	them,	somehow,
somewhere,	a	bottleneck	is	bound	to	develop.	The	best	we	can	do,	however,	is	make	sure	the	slowest
parts	of	the	system	are	the	ones	that	are	used	the	least.	This	is	the	idea	behind	Amdahl’s	Law.	This	law
states	that	the	performance	enhancement	possible	with	a	given	improvement	is	limited	by	the	amount	that
the	improved	feature	is	used.	The	underlying	premise	is	that	every	algorithm	has	a	sequential	part	that
ultimately	limits	the	speedup	that	can	be	achieved	by	multiprocessor	implementation.
If	parallel	machines	and	other	non–von	Neumann	architectures	give	such	huge	increases	in	processing
speed	and	power,	why	isn’t	everyone	using	them	everywhere?	The	answer	lies	in	their	programmability.
Advances	in	operating	systems	that	can	utilize	multiple	cores	have	put	these	chips	in	laptops	and	desktops
that	 we	 can	 buy	 today;	 however,	 true	 multiprocessor	 programming	 is	 more	 complex	 than	 both
uniprocessor	and	multicore	programming	and	requires	people	to	think	about	problems	in	a	different	way,
using	new	algorithms	and	programming	tools.
One	of	these	programming	tools	is	a	set	of	new	programming	languages.	Most	of	our	programming
languages	 are	 von	 Neumann	 languages,	 created	 for	 the	 von	 Neumann	 architecture.	 Many	 common
languages	have	been	extended	with	special	libraries	to	accommodate	parallel	programming,	and	many

new	languages	have	been	designed	specifically	for	the	parallel	programming	environment.	We	have	very
few	 programming	 languages	 for	 the	 remaining	 (nonparallel)	 non–von	 Neumann	 platforms,	 and	 fewer
people	who	really	understand	how	to	program	in	these	environments	efficiently.	Examples	of	non–von
Neumann	languages	include	Lucid	(for	dataflow)	and	QCL	(Quantum	Computation	Language)	for	quantum
computers,	as	well	as	VHDL	or	Verilog	(languages	used	to	program	FPGAs).	However,	even	with	the
inherent	difficulties	in	programming	parallel	machines,	we	see	in	the	next	section	that	significant	progress
is	being	made.
1.11		PARALLELISM:	ENABLER	OF	MACHINE	INTELLIGENCE
—DEEP	BLUE	AND	WATSON
It	 is	 evident	 by	 our	 sidebar	 on	 the	 Mechanical	 Turk	 that	 chess	 playing	 has	 long	 been	 considered	 the
ultimate	demonstration	of	a	“thinking	machine.”	The	chess-board	is	a	battlefield	where	human	can	meet
machine	on	more-or-less	equal	terms—with	the	human	always	having	the	edge,	of	course.	Real	chess-
playing	computers	have	been	around	since	the	late	1950s.	Over	the	decades,	they	gradually	improved
their	hardware	and	software	to	eventually	become	formidable	opponents	for	reasonably	skilled	players.
The	 problem	 of	championship	 chess	 playing,	 however,	 had	 long	 been	 considered	 so	 hard	 that	 many
believed	a	machine	could	never	beat	a	human	Grandmaster.	On	May	11,	1997,	a	machine	called	Deep
Blue	did	just	that.
Deep	Blue’s	principal	designers	were	IBM	researchers	Feng-hsiung	Hsu,	Thomas	Anantharaman,	and
Murray	Campbell.	Reportedly	costing	more	than	$6	million	and	taking	six	years	to	build,	Deep	Blue	was
a	massively	parallel	system	consisting	of	30	RS/6000-based	nodes	supplemented	with	480	chips	built
especially	 to	 play	 chess.	 Deep	 Blue	 included	 a	 database	 of	 700,000	 complete	 games	 with	 separate
systems	 for	 opening	 and	 endgames.	 It	 evaluated	 200	 million	 positions	 per	 second	 on	 average.	 This
enabled	Deep	Blue	to	produce	a	12-move	look	ahead.
Having	soundly	beat	an	earlier	version	of	Deep	Blue,	world	chess	champion	Garry	Kasparov	was
overwhelmingly	favored	to	win	a	rematch	starting	May	3,	1997.	At	the	end	of	five	games,	Kasparov	and
Deep	Blue	were	tied,	2½	to	2½.	Then	Deep	Blue	quickly	seized	upon	an	error	that	Kasparov	made	early
in	the	sixth	game.	Kasparov	had	no	choice	but	to	concede,	thus	making	Deep	Blue	the	first	machine	to
ever	defeat	a	chess	Grandmaster.
With	Deep	Blue’s	stunning	win	over	Kasparov	now	in	the	history	books,	IBM	Research	manager
Charles	Lickel	began	looking	for	a	new	challenge.	In	2004,	Lickel	was	among	the	millions	mesmerized	by
Ken	 Jennings’s	 unprecedented	 74-game	 winning	 streak	 on	 the	 American	 quiz	 show,	Jeopardy!	 As	 he
watched	 Jennings	 win	 one	 match	 after	 another,	 Lickel	 dared	 to	 think	 that	 it	 was	 possible	 to	 build	 a
machine	that	could	win	at	Jeopardy!	Moreover,	he	believed	that	IBM	Research	had	the	talent	to	build
such	a	machine.	He	tapped	Dr.	David	Ferrucci	to	lead	the	effort.
IBM	scientists	were	in	no	rush	to	sign	on	to	Lickel’s	audacious	project.	They	doubted—with	good
reason—that	 such	 a	 machine	 could	 be	 built.	 After	 all,	 creating	 Deep	 Blue	 was	 hard	 enough.	 Playing
Jeopardy!	 is	 enormously	 more	 difficult	 than	 playing	 chess.	 In	 chess,	 the	 problem	 domain	 is	 clearly
defined	 with	 fixed,	unambiguous	 rules,	 and	 a	 finite	 (although	 very	 large)	 solution	 space.	Jeopardy!
questions,	on	the	other	hand,	cover	a	nearly	infinite	problem	space	compounded	by	the	vagaries	of	human
language,	odd	relations	between	concepts,	puns,	and	vast	amounts	of	unstructured	factual	information.	For
example,	a	Jeopardy!	category	could	be	titled	“Doozy	Twos”	and	relate	to	an	African	leader,	an	article	of
clothing,	an	Al	Jolson	song,	and	an	ammunition	size	(Benjamin	Tutu,	tutu	skirt,	“Toot	Toot	Tootsie,”	and