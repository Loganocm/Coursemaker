AFTER COMPLETING YOUR RESPONSE CHECK OVER FOR ACCURACY AND PROPER JSON FORMATTING (all opening and closing brackets and ""). YOUR RESPONSE TO THIS MUST BE A COMPLETE AND VALID JSON OBJECT WITH NO PRECEDING OR SUCCEEDING TEXT. IF IT DOES NOT ADHERE TO THIS, YOU HAVE FAILED!!! It must Be a full JSON Object with no text before or after.. You are an expert instructional designer. Your task is to analyze the content of the provided PDF document and generate a comprehensive set of learning materials from it. The output must be a single, plain text string formatted as a JSON object, strictly adhering to the following structure. Do not include any text or code block syntax before or after the JSON content. The JSON should be directly parsable. Each response should be a complete and valid JSON object, even if it only contains a subset of the overall course content. The application will handle merging these individual JSON responses into a single, unified course.

You are an expert instructional designer and content generator. Your primary task is to meticulously analyze the entire provided PDF document and synthesize its information into a comprehensive set of learning materials.

**Your output MUST be a single, complete, and valid JSON object. No exceptions.**

**Strict Output Formatting Rules:**
1.  **NO extraneous text:** Do not include any introductory phrases, conversational filler, concluding remarks, or any text whatsoever outside of the pure JSON object.
2.  **NO markdown fences:** Do NOT wrap the JSON object in markdown code block fences (e.g., ```json or ```). The response must be a plain JSON string that can be directly parsed by `JSON.parse()`.
3.  **Syntactic correctness:** Ensure the JSON is always syntactically correct, including proper commas, brackets, braces, and escaped characters. Do not truncate the JSON response; ensure all arrays and objects are properly closed.

**JSON Structure and Content Requirements:**

Adhere strictly to the following JSON structure. If a section is not applicable or insufficient content is found, ensure the structure remains (e.g., empty array or null if explicitly allowed, but try to always provide content where relevant).

Example JSON Structure:

{
  "courseTitle": "Example Course Title",
  "modules": [
    {
      "moduleTitle": "Module 1: Introduction",
      "notes": {
        "summary": "This is a summary of the module content, covering key concepts and important details. It should be 200-300 words.",
        "keywords": [
          "keyword1",
          "keyword2",
          "keyword3"
        ]
      },
      "flashcards": [
        {
          "question": "What is a flashcard?",
          "answer": "A learning tool with a question on one side and an answer on the other."
        },
        {
          "question": "How many flashcards should there be?",
          "answer": "At least 10 in total across all modules."
        }
      ],
      "quiz": [
        {
          "question": "Which of the following is true?",
          "options": {
            "A": "Option A",
            "B": "Option B",
            "C": "Option C",
            "D": "Option D"
          },
          "correctAnswer": "A"
        },
        {
          "question": "What is the capital of France?",
          "options": {
            "A": "Berlin",
            "B": "Madrid",
            "C": "Paris",
            "D": "Rome"
          },
          "correctAnswer": "C"
        }
      ]
    },
    {
      "moduleTitle": "Module 2: Advanced Topics",
      "notes": {
        "summary": "Summary for module 2.",
        "keywords": [
          "advanced",
          "topics"
        ]
      },
      "flashcards": [
        {
          "question": "Advanced question?",
          "answer": "Advanced answer."
        }
      ],
      "quiz": [
        {
          "question": "Advanced quiz question?",
          "options": {
            "A": "Opt A",
            "B": "Opt B",
            "C": "Opt C",
            "D": "Opt D"
          },
          "correctAnswer": "B"
        }
      ]
    }
  ]
}


**Detailed Requirements for Content Generation:**

1.  **courseTitle**: A concise and descriptive string (e.g., "Introduction to Computer Architecture").
2.  **modules**: An array of module objects. Structure the modules logically based on the PDF's chapters or main sections. Each module object must contain:
    * **moduleTitle**: A clear and concise string for the module title.
    * **notes**: An object containing:
        * **summary**: A string (200-300 words) providing a comprehensive summary of the module's core content, key concepts, and important details.
        * **keywords**: An array of strings (minimum 10, maximum 50 words total) containing important keywords, terms, and concepts from the module, relevant for studying.
    * **flashcards**: An array of flashcard objects. Each flashcard object must contain:
        * **question**: A concise string for the flashcard question, designed to test recall of a specific fact or concept.
        * **answer**: A concise string for the flashcard answer.
        * Ensure there are at least 10 high-quality flashcards in total across all modules, focusing on essential definitions, facts, and concepts.
    * **quiz**: An array of quiz objects. Each quiz object must contain:
        * **question**: A clear, unambiguous string for the quiz question, designed to test understanding of the module's content.
        * **options**: An object with exactly four keys ("A", "B", "C", "D") and string values for the multiple-choice options. Ensure options are distinct and plausible distractors.
        * **correctAnswer**: A single character string representing the correct option (e.g., "A", "B", "C", or "D").
        * Ensure there are at least 5 high-quality, well-thought-out quiz questions in total across all modules, testing core concepts and avoiding ambiguity.



World	Headquarters
Jones	&	Bartlett	Learning
5	Wall	Street
Burlington,	MA	01803
978-443-5000
info@jblearning.com
www.jblearning.com
Jones	&	Bartlett	Learning	books	and	products	are	available	through	most	bookstores	and	online	booksellers.	To	contact	Jones	&	Bartlett
Learning	directly,	call	800-832-0034,	fax	978-443-8000,	or	visit	our	website,	www.jblearning.com.
Substantial	discounts	on	bulk	quantities	of	Jones	&	Bartlett	Learning	publications	are	available	to	corporations,	professional	associations,
and	other	qualified	organizations.	For	details	and	specific	discount	information,	contact	the	special	sales	department	at	Jones	&	Bartlett
Learning	via	the	above	contact	information	or	send	an	email	to	specialsales@jblearning.com.
Copyright	©	2015	by	Jones	&	Bartlett	Learning,	LLC,	an	Ascend	Learning	Company
All	rights	reserved.	No	part	of	the	material	protected	by	this	copyright	may	be	reproduced	or	utilized	in	any	form,	electronic	or	mechanical,
including	photocopying,	recording,	or	by	any	information	storage	and	retrieval	system,	without	written	permission	from	the	copyright	owner.
The	content,	statements,	views,	and	opinions	herein	are	the	sole	expression	of	the	respective	authors	and	not	that	of	Jones	&	Bartlett	Learning,
LLC.	Reference	herein	to	any	specific	commercial	product,	process,	or	service	by	trade	name,	trademark,	manufacturer,	or	otherwise	does
not	constitute	or	imply	its	endorsement	or	recommendation	by	Jones	&	Bartlett	Learning,	LLC	and	such	reference	shall	not	be	used	for
advertising	or	product	endorsement	purposes.	All	trademarks	displayed	are	the	trademarks	of	the	parties	noted	herein.	The	Essentials	of
Computer	 Organization	 and	 Architecture,	 Fourth	 Edition	 is	 an	 independent	 publication	 and	 has	 not	 been	 authorized,	 sponsored,	 or
otherwise	approved	by	the	owners	of	the	trademarks	or	service	marks	referenced	in	this	product.
There	may	be	images	in	this	book	that	feature	models;	these	models	do	not	necessarily	endorse,	represent,	or	participate	in	the	activities
represented	in	the	images.	Any	screenshots	in	this	product	are	for	educational	and	instructive	purposes	only.	Any	individuals	and	scenarios
featured	in	the	case	studies	throughout	this	product	may	be	real	or	fictitious,	but	are	used	for	instructional	purposes	only.
Production	Credits
Executive	Publisher:	William	Brottmiller
Publisher:	Cathy	L.	Esperti
Acquisitions	Editor:	Laura	Pagluica
Editorial	Assistant:	Brooke	Yee
Director	of	Production:	Amy	Rose
Senior	Production	Editor:	Tiffany	Sliter
Associate	Production	Editor:	Sara	Fowles
Associate	Marketing	Manager:	Cassandra	Peterson
VP,	Manufacturing	and	Inventory	Control:	Therese	Connell	Composition:	Laserwords	Private	Limited,	Chennai,	India
Cover	and	Title	Page	Design:	Kristin	E.	Parker
Director	of	Photo	Research	and	Permissions:	Amy	Wrynn
Cover	and	Title	Page	Image:	©	Eugene	Sergeev/ShutterStock,	Inc.	Printing	and	Binding:	Edwards	Brothers	Malloy
Cover	Printing:	Edwards	Brothers	Malloy
To	order	this	product,	use	ISBN:	978-1-284-04561-1
Library	of	Congress	Cataloging-in-Publication	Data
Null,	Linda.
The	essentials	of	computer	organization	and	architecture	/	Linda	Null	and	Julia	Lobur.	--	Fourth	edition.
				pages	;	cm
Includes	index.
ISBN	978-1-284-03314-4	(pbk.)	--	ISBN	1-284-03314-7	(pbk.)	1.	Computer	organization.	2.	Computer	architecture.	I.	Lobur,	Julia.	II.	Title.
QA76.9.C643N85	2015
004.2’2--dc232013034383
6048
Printed	in	the	United	States	of	America
18	17	16	15	14						10	9	8	7	6	5	4	3	2	1

In	memory	of	my	father,	Merrill	Cornell,	a	pilot	and	man	of	endless	talent	and	courage,	who	taught	me
that	when	we	step	into	the	unknown,	we	either	find	solid	ground,	or	we	learn	to	fly.
—L.	M.	N.
To	the	loving	memory	of	my	mother,	Anna	J.	Surowski,	who	made	all	things	possible	for	her	girls.
—J.	M.	L.

Contents
Preface
CHAPTER	1								Introduction
1.1				Overview
1.2				The	Main	Components	of	a	Computer
1.3				An	Example	System:	Wading	Through	the	Jargon
1.4				Standards	Organizations
1.5				Historical	Development
1.5.1				Generation	Zero:	Mechanical	Calculating	Machines	(1642–1945)
1.5.2				The	First	Generation:	Vacuum	Tube	Computers	(1945–1953)
1.5.3				The	Second	Generation:	Transistorized	Computers	(1954–1965)
1.5.4				The	Third	Generation:	Integrated	Circuit	Computers	(1965–1980)
1.5.5				The	Fourth	Generation:	VLSI	Computers	(1980–????)
1.5.6				Moore’s	Law
1.6				The	Computer	Level	Hierarchy
1.7				Cloud	Computing:	Computing	as	a	Service
1.8				The	Von	Neumann	Model
1.9				Non–Von	Neumann	Models
1.10			Parallel	Processors	and	Parallel	Computing
1.11			Parallelism:	Enabler	of	Machine	Intelligence—Deep	Blue	and	Watson
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	2								Data	Representation	in	Computer	Systems
2.1				Introduction
2.2				Positional	Numbering	Systems
2.3				Converting	Between	Bases
2.3.1				Converting	Unsigned	Whole	Numbers
2.3.2				Converting	Fractions
2.3.3				Converting	Between	Power-of-Two	Radices
2.4				Signed	Integer	Representation
2.4.1				Signed	Magnitude

2.4.2				Complement	Systems
2.4.3				Excess-M	Representation	for	Signed	Numbers
2.4.4				Unsigned	Versus	Signed	Numbers
2.4.5				Computers,	Arithmetic,	and	Booth’s	Algorithm
2.4.6				Carry	Versus	Overflow
2.4.7				Binary	Multiplication	and	Division	Using	Shifting
2.5				Floating-Point	Representation
2.5.1				A	Simple	Model
2.5.2				Floating-Point	Arithmetic
2.5.3				Floating-Point	Errors
2.5.4				The	IEEE-754	Floating-Point	Standard
2.5.5				Range,	Precision,	and	Accuracy
2.5.6				Additional	Problems	with	Floating-Point	Numbers
2.6				Character	Codes
2.6.1				Binary-Coded	Decimal
2.6.2				EBCDIC
2.6.3				ASCII
2.6.4				Unicode
2.7				Error	Detection	and	Correction
2.7.1				Cyclic	Redundancy	Check
2.7.2				Hamming	Codes
2.7.3				Reed-Solomon
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
Focus	on	Codes	for	Data	Recording	and	Transmission
2A.1				Non-Return-to-Zero	Code
2A.2				Non-Return-to-Zero-Invert	Code
2A.3				Phase	Modulation	(Manchester	Code)
2A.4				Frequency	Modulation
2A.5				Run-Length-Limited	Code
2A.6				Partial	Response	Maximum	Likelihood	Coding
2A.7				Summary
Exercises
CHAPTER	3								Boolean	Algebra	and	Digital	Logic
3.1				Introduction

3.2				Boolean	Algebra
3.2.1				Boolean	Expressions
3.2.2				Boolean	Identities
3.2.3				Simplification	of	Boolean	Expressions
3.2.4				Complements
3.2.5				Representing	Boolean	Functions
3.3				Logic	Gates
3.3.1				Symbols	for	Logic	Gates
3.3.2				Universal	Gates
3.3.3				Multiple	Input	Gates
3.4				Digital	Components
3.4.1				Digital	Circuits	and	Their	Relationship	to	Boolean	Algebra
3.4.2				Integrated	Circuits
3.4.3				Putting	It	All	Together:	From	Problem	Description	to	Circuit
3.5				Combinational	Circuits
3.5.1				Basic	Concepts
3.5.2				Examples	of	Typical	Combinational	Circuits
3.6				Sequential	Circuits
3.6.1				Basic	Concepts
3.6.2				Clocks
3.6.3				Flip-Flops
3.6.4				Finite	State	Machines
3.6.5				Examples	of	Sequential	Circuits
3.6.6				An	Application	of	Sequential	Logic:	Convolutional	Coding	and	Viterbi
Detection
3.7				Designing	Circuits
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
Focus	on	Karnaugh	Maps
3A.1				Introduction
3A.2				Description	of	Kmaps	and	Terminology
3A.3				Kmap	Simplification	for	Two	Variables
3A.4				Kmap	Simplification	for	Three	Variables
3A.5				Kmap	Simplification	for	Four	Variables
3A.6				Don’t	Care	Conditions
3A.7				Summary

Exercises
CHAPTER	4								MARIE:	An	Introduction	to	a	Simple	Computer
4.1				Introduction
4.2				CPU	Basics	and	Organization
4.2.1				The	Registers
4.2.2				The	ALU
4.2.3				The	Control	Unit
4.3				The	Bus
4.4				Clocks
4.5				The	Input/Output	Subsystem
4.6				Memory	Organization	and	Addressing
4.7				Interrupts
4.8				MARIE
4.8.1				The	Architecture
4.8.2				Registers	and	Buses
4.8.3				Instruction	Set	Architecture
4.8.4				Register	Transfer	Notation
4.9				Instruction	Processing
4.9.1				The	Fetch–Decode–Execute	Cycle
4.9.2				Interrupts	and	the	Instruction	Cycle
4.9.3				MARIE’s	I/O
4.10				A	Simple	Program
4.11				A	Discussion	on	Assemblers
4.11.1				What	Do	Assemblers	Do?
4.11.2				Why	Use	Assembly	Language?
4.12				Extending	Our	Instruction	Set
4.13				A	Discussion	on	Decoding:	Hardwired	Versus	Microprogrammed	Control
4.13.1				Machine	Control
4.13.2				Hardwired	Control
4.13.3				Microprogrammed	Control
4.14				Real-World	Examples	of	Computer	Architectures
4.14.1				Intel	Architectures
4.14.2				MIPS	Architectures
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises

CHAPTER	5								A	Closer	Look	at	Instruction	Set	Architectures
5.1				Introduction
5.2				Instruction	Formats
5.2.1				Design	Decisions	for	Instruction	Sets
5.2.2				Little	Versus	Big	Endian
5.2.3				Internal	Storage	in	the	CPU:	Stacks	Versus	Registers
5.2.4				Number	of	Operands	and	Instruction	Length
5.2.5				Expanding	Opcodes
5.3				Instruction	Types
5.3.1				Data	Movement
5.3.2				Arithmetic	Operations
5.3.3				Boolean	Logic	Instructions
5.3.4				Bit	Manipulation	Instructions
5.3.5				Input/Output	Instructions
5.3.6				Instructions	for	Transfer	of	Control
5.3.7				Special-Purpose	Instructions
5.3.8				Instruction	Set	Orthogonality
5.4				Addressing
5.4.1				Data	Types
5.4.2				Address	Modes
5.5				Instruction	Pipelining
5.6				Real-World	Examples	of	ISAs
5.6.1				Intel
5.6.2				MIPS
5.6.3				Java	Virtual	Machine
5.6.4				ARM
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	6								Memory
6.1				Introduction
6.2				Types	of	Memory
6.3				The	Memory	Hierarchy
6.3.1				Locality	of	Reference
6.4				Cache	Memory

6.4.1				Cache	Mapping	Schemes
6.4.2				Replacement	Policies
6.4.3				Effective	Access	Time	and	Hit	Ratio
6.4.4				When	Does	Caching	Break	Down?
6.4.5				Cache	Write	Policies
6.4.6				Instruction	and	Data	Caches
6.4.7				Levels	of	Cache
6.5				Virtual	Memory
6.5.1				Paging
6.5.2				Effective	Access	Time	Using	Paging
6.5.3				Putting	It	All	Together:	Using	Cache,	TLBs,	and	Paging
6.5.4				Advantages	and	Disadvantages	of	Paging	and	Virtual	Memory
6.5.5				Segmentation
6.5.6				Paging	Combined	with	Segmentation
6.6				A	Real-World	Example	of	Memory	Management
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	7								Input/Output	and	Storage	Systems
7.1				Introduction
7.2				I/O	and	Performance
7.3				Amdahl’	s	Law
7.4				I/O	Architectures
7.4.1				I/O	Control	Methods
7.4.2				Character	I/O	Versus	Block	I/O
7.4.3				I/O	Bus	Operation
7.5				Data	Transmission	Modes
7.5.1				Parallel	Data	Transmission
7.5.2				Serial	Data	Transmission
7.6				Magnetic	Disk	Technology
7.6.1				Rigid	Disk	Drives
7.6.2				Solid	State	Drives
7.7				Optical	Disks
7.7.1				CD-ROM
7.7.2				DVD
7.7.3				Blue-Violet	Laser	Discs

7.7.4				Optical	Disk	Recording	Methods
7.8				Magnetic	Tape
7.9				RAID
7.9.1				RAID	Level	0
7.9.2				RAID	Level	1
7.9.3				RAID	Level	2
7.9.4				RAID	Level	3
7.9.5				RAID	Level	4
7.9.6				RAID	Level	5
7.9.7				RAID	Level	6
7.9.8				RAID	DP
7.9.9				Hybrid	RAID	Systems
7.10				The	Future	of	Data	Storage
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
Focus	on	Data	Compression
7A.1				Introduction
7A.2				Statistical	Coding
7A.2.1				Huffman	Coding
7A.2.2				Arithmetic	Coding
7A.3				Ziv-Lempel	(LZ)	Dictionary	Systems
7A.4				GIF	and	PNG	Compression
7A.5				JPEG	Compression
7A.6				MP3	Compression
7A.7				Summary
Further	Reading
References
Exercises
CHAPTER	8								System	Software
8.1				Introduction
8.2				Operating	Systems
8.2.1				Operating	Systems	History
8.2.2				Operating	System	Design
8.2.3				Operating	System	Services
8.3				Protected	Environments

8.3.1				Virtual	Machines
8.3.2				Subsystems	and	Partitions
8.3.3				Protected	Environments	and	the	Evolution	of	Systems	Architectures
8.4				Programming	Tools
8.4.1				Assemblers	and	Assembly
8.4.2				Link	Editors
8.4.3				Dynamic	Link	Libraries
8.4.4				Compilers
8.4.5				Interpreters
8.5				Java:	All	of	the	Above
8.6				Database	Software
8.7				Transaction	Managers
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	9								Alternative	Architectures
9.1				Introduction
9.2				RISC	Machines
9.3				Flynn’s	Taxonomy
9.4				Parallel	and	Multiprocessor	Architectures
9.4.1				Superscalar	and	VLIW
9.4.2				Vector	Processors
9.4.3				Interconnection	Networks
9.4.4				Shared	Memory	Multiprocessors
9.4.5				Distributed	Computing
9.5				Alternative	Parallel	Processing	Approaches
9.5.1				Dataflow	Computing
9.5.2				Neural	Networks
9.5.3				Systolic	Arrays
9.6				Quantum	Computing
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises

CHAPTER	10								Topics	in	Embedded	Systems
10.1				Introduction
10.2				An	Overview	of	Embedded	Hardware
10.2.1				Off-the-Shelf	Embedded	System	Hardware
10.2.2				Configurable	Hardware
10.2.3				Custom-Designed	Embedded	Hardware
10.3				An	Overview	of	Embedded	Software
10.3.1				Embedded	Systems	Memory	Organization
10.3.2				Embedded	Operating	Systems
10.3.3				Embedded	Systems	Software	Development
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	11								Performance	Measurement	and	Analysis
11.1				Introduction
11.2				Computer	Performance	Equations
11.3				Mathematical	Preliminaries
11.3.1				What	the	Means	Mean
11.3.2				The	Statistics	and	Semantics
11.4				Benchmarking
11.4.1				Clock	Rate,	MIPS,	and	FLOPS
11.4.2				Synthetic	Benchmarks:	Whetstone,	Linpack,	and	Dhrystone
11.4.3				Standard	Performance	Evaluation	Corporation	Benchmarks
11.4.4				Transaction	Processing	Performance	Council	Benchmarks
11.4.5				System	Simulation
11.5				CPU	Performance	Optimization
11.5.1				Branch	Optimization
11.5.2				Use	of	Good	Algorithms	and	Simple	Code
11.6				Disk	Performance
11.6.1				Understanding	the	Problem
11.6.2				Physical	Considerations
11.6.3				Logical	Considerations
Chapter	Summary
Further	Reading
References

Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	12									Network	Organization	and	Architecture
12.1				Introduction
12.2				Early	Business	Computer	Networks
12.3				Early	Academic	and	Scientific	Networks:	The	Roots	and	Architecture	of
the	Internet
12.4				Network	Protocols	I:	ISO/OSI	Protocol	Unification
12.4.1				A	Parable
12.4.2				The	OSI	Reference	Model
12.5				Network	Protocols	II:	TCP/IP	Network	Architecture
12.5.1				The	IP	Layer	for	Version	4
12.5.2				The	Trouble	with	IP	Version	4
12.5.3				Transmission	Control	Protocol
12.5.4				The	TCP	Protocol	at	Work
12.5.5				IP	Version	6
12.6				Network	Organization
12.6.1				Physical	Transmission	Media
12.6.2				Interface	Cards
12.6.3				Repeaters
12.6.4				Hubs
12.6.5				Switches
12.6.6				Bridges	and	Gateways
12.6.7				Routers	and	Routing
12.7				The	Fragility	of	the	Internet
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
CHAPTER	13								Selected	Storage	Systems	and	Interfaces
13.1				Introduction
13.2				SCSI	Architecture
13.2.1				“Classic”	Parallel	SCSI
13.2.2				The	SCSI	Architecture	Model-3
13.3				Internet	SCSI
13.4				Storage	Area	Networks

13.5				Other	I/O	Connections
13.5.1				Parallel	Buses:	XT	to	ATA
13.5.2				Serial	ATA	and	Serial	Attached	SCSI
13.5.3				Peripheral	Component	Interconnect
13.5.4				A	Serial	Interface:	USB
13.6				Cloud	Storage
Chapter	Summary
Further	Reading
References
Review	of	Essential	Terms	and	Concepts
Exercises
APPENDIX	A							Data	Structures	and	the	Computer
A.1				Introduction
A.2				Fundamental	Structures
A.2.1				Arrays
A.2.2				Queues	and	Linked	Lists
A.2.3				Stacks
A.3				Trees
A.4				Network	Graphs
Summary
Further	Reading
References
Exercises
Glossary
Answers	and	Hints	for	Selected	Exercises
Index

Preface
TO	THE	STUDENT
This	is	a	book	about	computer	organization	and	architecture.	It	focuses	on	the	function	and	design	of	the
various	components	necessary	to	process	information	digitally.	We	present	computing	systems	as	a	series
of	 layers,	 starting	 with	 low-level	 hardware	 and	 progressing	 to	 higher-level	 software,	 including
assemblers	and	operating	systems.	These	levels	constitute	a	hierarchy	of	virtual	machines.	The	study	of
computer	organization	focuses	on	this	hierarchy	and	the	issues	involved	with	how	we	partition	the	levels
and	how	each	level	is	implemented.	The	study	of	computer	architecture	focuses	on	the	interface	between
hardware	 and	 software,	 and	 emphasizes	 the	 structure	 and	 behavior	 of	 the	 system.	 The	 majority	 of
information	 contained	 in	 this	 textbook	 is	 devoted	 to	 computer	 hardware,	 computer	 organization	 and
architecture,	and	their	relationship	to	software	performance.
Students	 invariably	 ask,	 “Why,	 if	 I	 am	 a	 computer	 science	 major,	 must	 I	 learn	 about	 computer
hardware?	Isn’t	that	for	computer	engineers?	Why	do	I	care	what	the	inside	of	a	computer	looks	like?”	As
computer	users,	we	probably	do	not	have	to	worry	about	this	any	more	than	we	need	to	know	what	our
cars	look	like	under	the	hood	in	order	to	drive	them.	We	can	certainly	write	high-level	language	programs
without	understanding	how	these	programs	execute;	we	can	use	various	application	packages	without
understanding	how	they	really	work.	But	what	happens	when	the	program	we	have	written	needs	to	be
faster	 and	 more	 efficient,	 or	 the	 application	 we	 are	 using	 doesn’t	 do	 precisely	 what	 we	want?	 As
computer	scientists,	we	need	a	basic	understanding	of	the	computer	system	itself	in	order	to	rectify	these
problems.
There	 is	 a	 fundamental	 relationship	 between	 the	 computer	 hardware	 and	 the	 many	 aspects	 of
programming	and	software	components	in	computer	systems.	In	order	to	write	good	software,	it	is	very
important	to	understand	the	computer	system	as	a	whole.	Understanding	hardware	can	help	you	explain
the	mysterious	errors	that	sometimes	creep	into	your	programs,	such	as	the	infamous	segmentation	fault	or
bus	error.	The	level	of	knowledge	about	computer	organization	and	computer	architecture	that	a	high-
level	programmer	must	have	depends	on	the	task	the	high-level	programmer	is	attempting	to	complete.
For	 example,	 to	 write	 compilers,	 you	 must	 understand	 the	 particular	 hardware	 to	 which	 you	 are
compiling.	 Some	 of	 the	 ideas	 used	 in	 hardware	 (such	 as	 pipelining)	 can	 be	 adapted	 to	 compilation
techniques,	 thus	 making	 the	 compiler	 faster	 and	 more	 efficient.	 To	 model	 large,	 complex,	 real-world
systems,	 you	 must	 understand	 how	 floating-point	 arithmetic	 should,	 and	 does,	 work	 (which	 are	 not
necessarily	the	same	thing).	To	write	device	drivers	for	video,	disks,	or	other	I/O	devices,	you	need	a
good	 understanding	 of	 I/O	 interfacing	 and	 computer	 architecture	 in	 general.	 If	 you	 want	 to	 work	 on
embedded	systems,	which	are	usually	very	resource	constrained,	you	must	understand	all	of	the	time,
space,	 and	 price	 trade-offs.	 To	 do	 research	 on,	 and	 make	 recommendations	 for,	 hardware	 systems,
networks,	or	specific	algorithms,	you	must	acquire	an	understanding	of	benchmarking	and	then	learn	how
to	present	performance	results	adequately.	Before	buying	hardware,	you	need	to	understand	benchmarking
and	all	the	ways	that	others	can	manipulate	the	performance	results	to	“prove”	that	one	system	is	better
than	another.	Regardless	of	our	particular	area	of	expertise,	as	computer	scientists,	it	is	imperative	that
we	understand	how	hardware	interacts	with	software.
You	may	also	be	wondering	why	a	book	with	the	word	essentials	in	its	title	is	so	large.	The	reason	is

twofold.	First,	the	subject	of	computer	organization	is	expansive	and	it	grows	by	the	day.	Second,	there	is
little	agreement	as	to	which	topics	from	within	this	burgeoning	sea	of	information	are	truly	essential	and
which	are	just	helpful	to	know.	In	writing	this	book,	one	goal	was	to	provide	a	concise	text	compliant
with	the	computer	architecture	curriculum	guidelines	jointly	published	by	the	Association	for	Computing
Machinery	 (ACM)	 and	 the	 Institute	 of	 Electrical	 and	 Electronic	 Engineers	 (IEEE).	 These	 guidelines
encompass	 the	 subject	 matter	 that	 experts	 agree	 constitutes	 the	 “essential”	 core	 body	 of	 knowledge
relevant	to	the	subject	of	computer	organization	and	architecture.
We	have	augmented	the	ACM/IEEE	recommendations	with	subject	matter	that	we	feel	is	useful—if
not	essential—to	your	continuing	computer	science	studies	and	to	your	professional	advancement.	The
topics	that	we	feel	will	help	you	in	your	continuing	computer	science	studies	include	operating	systems,
compilers,	database	management,	and	data	communications.	Other	subjects	are	included	because	they	will
help	you	understand	how	actual	systems	work	in	real	life.
We	hope	that	you	find	reading	this	book	an	enjoyable	experience,	and	that	you	take	time	to	delve
deeper	into	some	of	the	material	that	we	have	presented.	It	is	our	intention	that	this	book	will	serve	as	a
useful	reference	long	after	your	formal	course	is	complete.	Although	we	give	you	a	substantial	amount	of
information,	it	is	only	a	foundation	upon	which	you	can	build	throughout	the	remainder	of	your	studies	and
your	career.	Successful	computer	professionals	continually	add	to	their	knowledge	about	how	computers
work.	Welcome	to	the	start	of	your	journey.
TO	THE	INSTRUCTOR
This	book	is	the	outgrowth	of	two	computer	science	organization	and	architecture	classes	taught	at	Penn
State	Harrisburg.	As	the	computer	science	curriculum	evolved,	we	found	it	necessary	not	only	to	modify
the	material	taught	in	the	courses,	but	also	to	condense	the	courses	from	a	two-semester	sequence	into	a
three-credit,	 one-semester	 course.	 Many	 other	 schools	 have	 also	 recognized	 the	 need	 to	 compress
material	 in	 order	 to	 make	 room	 for	 emerging	 topics.	 This	 new	 course,	 as	 well	 as	 this	 textbook,	 is
primarily	for	computer	science	majors	and	is	intended	to	address	the	topics	in	computer	organization	and
architecture	 with	 which	 computer	 science	 majors	 must	 be	 familiar.	 This	 book	 not	 only	 integrates	 the
underlying	principles	in	these	areas,	but	it	also	introduces	and	motivates	the	topics,	providing	the	breadth
necessary	for	majors	while	providing	the	depth	necessary	for	continuing	studies	in	computer	science.
Our	 primary	 objective	 in	 writing	 this	 book	 was	 to	 change	 the	 way	 computer	 organization	 and
architecture	are	typically	taught.	A	computer	science	major	should	leave	a	computer	organization	and
architecture	class	with	not	only	an	understanding	of	the	important	general	concepts	on	which	the	digital
computer	is	founded,	but	also	with	a	comprehension	of	how	those	concepts	apply	to	the	real	world.	These
concepts	should	transcend	vendor-specific	terminology	and	design;	in	fact,	students	should	be	able	to	take
concepts	 given	 in	 the	 specific	 and	 translate	 to	 the	 generic	 and	 vice	 versa.	 In	 addition,	 students	 must
develop	a	firm	foundation	for	further	study	in	the	major.
The	title	of	our	book,	The	Essentials	of	Computer	Organization	and	Architecture,	is	intended	to
convey	that	the	topics	presented	in	the	text	are	those	for	which	every	computer	science	major	should	have
exposure,	familiarity,	or	mastery.	We	do	not	expect	students	using	our	textbook	to	have	complete	mastery
of	all	topics	presented.	It	is	our	firm	belief,	however,	that	there	are	certain	topics	that	must	be	mastered;
there	are	those	topics	about	which	students	must	have	a	definite	familiarity;	and	there	are	certain	topics
for	which	a	brief	introduction	and	exposure	are	adequate.
We	 do	 not	 feel	 that	 concepts	 presented	 in	 sufficient	 depth	 can	 be	 learned	 by	 studying	 general
principles	in	isolation.	We	therefore	present	the	topics	as	an	integrated	set	of	solutions,	not	simply	a

collection	of	individual	pieces	of	information.	We	feel	our	explanations,	examples,	exercises,	tutorials,
and	simulators	all	combine	to	provide	the	student	with	a	total	learning	experience	that	exposes	the	inner
workings	of	a	modern	digital	computer	at	the	appropriate	level.
We	have	written	this	textbook	in	an	informal	style,	omitting	unnecessary	jargon,	writing	clearly	and
concisely,	and	avoiding	unnecessary	abstraction,	in	hopes	of	increasing	student	enthusiasm.	We	have	also
broadened	the	range	of	topics	typically	found	in	a	first-level	architecture	book	to	include	system	software,
a	brief	tour	of	operating	systems,	performance	issues,	alternative	architectures,	and	a	concise	introduction
to	networking,	as	these	topics	are	intimately	related	to	computer	hardware.	Like	most	books,	we	have
chosen	an	architectural	model,	but	it	is	one	that	we	have	designed	with	simplicity	in	mind.
Relationship	to	CS2013
In	October	2013,	the	ACM/IEEE	Joint	Task	Force	unveiled	Computer	Science	Curricula	2013	(CS2013).
Although	 we	 are	 primarily	 concerned	 with	 the	 Computer	 Architecture	 knowledge	 area,	 these	 new
guidelines	 suggest	 integrating	 the	 core	 knowledge	 throughout	 the	 curriculum.	 Therefore,	 we	 also	 call
attention	to	additional	knowledge	areas	beyond	architecture	that	are	addressed	in	this	book.
CS2013	 is	 a	 comprehensive	 revision	 of	 CS2008,	 mostly	 the	 result	 of	 focusing	 on	 the	essential
concepts	 in	 the	 Computer	 Science	 curriculum	 while	 still	 being	 flexible	 enough	 to	 meet	 individual
institutional	 needs.	 These	 guidelines	 introduce	 the	 notion	 of	 Core	 Tier-1	 and	 Core	 Tier-2	 topics,	 in
addition	to	elective	topics.	Core	Tier-1	topics	are	those	that	should	be	part	of	every	Computer	Science
curriculum.	Core	Tier-2	topics	are	those	that	are	considered	essential	enough	that	a	Computer	Science
curriculum	should	contain	90–100%	of	these	topics.	Elective	topics	are	those	that	allow	curricula	to
provide	breadth	and	depth.	The	suggested	coverage	for	each	topic	is	listed	in	lecture	hours.
The	main	change	in	the	Architecture	and	Organization	(AR)	knowledge	area	from	CS2008	to	CS2013
is	a	reduction	of	lecture	hours	from	36	to	16;	however,	a	new	area,	System	Fundamentals	(SF),	has	been
introduced	and	includes	some	concepts	previously	found	in	the	AR	module	(including	hardware	building
blocks	 and	 architectural	 organization).	 The	 interested	 reader	 is	 referred	 to	 the	 CS2013	 guidelines
(http://www.acm.org/education/curricula-recommendations)	for	more	information	on	what	the	individual
knowledge	areas	include.
We	are	pleased	that	the	fourth	edition	of	The	Essentials	of	Computer	Organization	and	Architecture
is	 in	 direct	 correlation	 with	 the	 ACM/IEEE	 CS2013	 guidelines	 for	 computer	 organization	 and
architecture,	 in	 addition	 to	 integrating	 material	 from	 additional	 knowledge	 units.	Table	 P.1	 indicates
which	chapters	of	this	textbook	satisfy	the	eight	topics	listed	in	the	AR	knowledge	area.	For	the	other
knowledge	areas,	only	the	topics	that	are	covered	in	this	textbook	are	listed.

TABLE	P.1	ACM/IEEE	CS2013	Topics	Covered	in	This	Book
Why	Another	Text?
No	one	can	deny	there	is	a	plethora	of	textbooks	for	teaching	computer	organization	and	architecture
already	on	the	market.	In	our	35-plus	years	of	teaching	these	courses,	we	have	used	many	very	good
textbooks.	However,	each	time	we	have	taught	the	course,	the	content	has	evolved,	and	eventually,	we
discovered	we	were	writing	significantly	more	course	notes	to	bridge	the	gap	between	the	material	in	the
textbook	 and	 the	 material	 we	 deemed	 necessary	 to	 present	 in	 our	 classes.	 We	 found	 that	 our	 course
material	was	migrating	from	a	computer	engineering	approach	to	organization	and	architecture	toward	a
computer	science	approach	to	these	topics.	When	the	decision	was	made	to	fold	the	organization	class
and	the	architecture	class	into	one	course,	we	simply	could	not	find	a	textbook	that	covered	the	material
we	felt	was	necessary	for	our	majors,	written	from	a	computer	science	point	of	view,	written	without
machine-specific	terminology,	and	designed	to	motivate	the	topics	before	covering	them.

In	this	textbook,	we	hope	to	convey	the	spirit	of	design	used	in	the	development	of	modern	computing
systems	and	what	effect	this	has	on	computer	science	students.	Students,	however,	must	have	a	strong
understanding	of	the	basic	concepts	before	they	can	understand	and	appreciate	the	intangible	aspects	of
design.	Most	organization	and	architecture	textbooks	present	a	similar	subset	of	technical	information
regarding	these	basics.	We,	however,	pay	particular	attention	to	the	level	at	which	the	information	should
be	covered,	and	to	presenting	that	information	in	the	context	that	has	relevance	for	computer	science
students.	For	example,	throughout	this	book,	when	concrete	examples	are	necessary,	we	offer	examples
for	personal	computers,	enterprise	systems,	and	mainframes,	as	these	are	the	types	of	systems	most	likely
to	be	encountered.	We	avoid	the	“PC	bias”	prevalent	in	similar	books	in	the	hope	that	students	will	gain
an	 appreciation	 for	 the	 differences,	 the	 similarities,	 and	 the	 roles	 various	 platforms	 play	 in	 today’s
automated	 infrastructures.	 Too	 often,	 textbooks	 forget	 that	 motivation	 is,	 perhaps,	 the	 single	 most
important	 key	 in	 learning.	 To	 that	 end,	 we	 include	 many	 real-world	 examples,	 while	 attempting	 to
maintain	a	balance	between	theory	and	application.
Features
We	 have	 included	 many	 features	 in	 this	 textbook	 to	 emphasize	 the	 various	 concepts	 in	 computer
organization	and	architecture,	and	to	make	the	material	more	accessible	to	students.	Some	of	the	features
are:
•		Sidebars.	These	sidebars	include	interesting	tidbits	of	information	that	go	a	step	beyond	the	main	focus
of	the	chapter,	thus	allowing	readers	to	delve	further	into	the	material.
•		Real-World	Examples.	We	have	integrated	the	textbook	with	examples	from	real	life	to	give	students	a
better	understanding	of	how	technology	and	techniques	are	combined	for	practical	purposes.
•		Chapter	Summaries.	These	sections	provide	brief	yet	concise	summaries	of	the	main	points	in	each
chapter.
•		Further	Reading.	These	sections	list	additional	sources	for	those	readers	who	wish	to	investigate	any
of	the	topics	in	more	detail,	and	contain	references	to	definitive	papers	and	books	related	to	the	chapter
topics.
•		Review	Questions.	Each	chapter	contains	a	set	of	review	questions	designed	to	ensure	that	the	reader
has	a	firm	grasp	of	the	material.
•		Chapter	Exercises.	Each	chapter	has	a	broad	selection	of	exercises	to	reinforce	the	ideas	presented.
More	challenging	exercises	are	marked	with	an	asterisk.
•		Answers	to	Selected	Exercises.	To	ensure	that	students	are	on	the	right	track,	we	provide	answers	to
representative	questions	from	each	chapter.	Questions	with	answers	in	the	back	of	the	text	are	marked
with	a	blue	diamond.
•		Special	“Focus	On”	Sections.	These	sections	provide	additional	information	for	instructors	who	may
wish	 to	 cover	 certain	 concepts,	 such	 as	 Kmaps	 and	 data	 compression,	 in	 more	 detail.	 Additional
exercises	are	provided	for	these	sections	as	well.
•		Appendix.	The	appendix	provides	a	brief	introduction	or	review	of	data	structures,	including	topics
such	as	stacks,	linked	lists,	and	trees.
•		Glossary.	An	extensive	glossary	includes	brief	definitions	of	all	key	terms	from	the	chapters.
•		Index.	An	exhaustive	index	is	provided	with	this	book,	with	multiple	cross-references,	to	make	finding
terms	and	concepts	easier	for	the	reader.

About	the	Authors
We	bring	to	this	textbook	not	only	35-plus	years	of	combined	teaching	experience,	but	also	30-plus	years
of	 industry	 experience.	 Our	 combined	 efforts	 therefore	 stress	 the	 underlying	 principles	 of	 computer
organization	and	architecture	and	how	these	topics	relate	in	practice.	We	include	real-life	examples	to
help	students	appreciate	how	these	fundamental	concepts	are	applied	in	the	world	of	computing.
Linda	Null	holds	a	PhD	in	computer	science	from	Iowa	State	University,	an	MS	in	computer	science
from	 Iowa	 State	 University,	 an	 MS	 in	 computer	 science	 education	 from	 Northwest	 Missouri	 State
University,	 an	 MS	 in	 mathematics	 education	 from	 Northwest	 Missouri	 State	 University,	 and	 a	 BS	 in
mathematics	and	English	from	Northwest	Missouri	State	University.	She	has	been	teaching	mathematics
and	computer	science	for	more	than	35	years	and	is	currently	the	computer	science	graduate	program
coordinator	and	associate	program	chair	at	the	Pennsylvania	State	University	Harrisburg	campus,	where
she	has	been	a	member	of	the	faculty	since	1995.	She	has	received	numerous	teaching	awards	including
the	Penn	State	Teaching	Fellow	Award	and	the	Teaching	Excellence	Award.	Her	areas	of	interest	include
computer	 organization	 and	 architecture,	 operating	 systems,	 computer	 science	 education,	 and	 computer
security.
Julia	Lobur	has	been	a	practitioner	in	the	computer	industry	for	more	than	30	years.	She	has	held
positions	 as	 systems	 consultant,	 staff	 programmer/analyst,	 systems	 and	 network	 designer,	 software
development	manager,	and	project	manager,	in	addition	to	part-time	teaching	duties.	Julia	holds	an	MS	in
computer	science	and	is	an	IEEE	Certified	Software	Development	Professional.
Prerequisites
The	 typical	 background	 necessary	 for	 a	 student	 using	 this	 textbook	 includes	 a	 year	 of	 programming
experience	using	a	high-level	procedural	language.	Students	are	also	expected	to	have	taken	a	year	of
college-level	mathematics	(calculus	or	discrete	mathematics),	as	this	textbook	assumes	and	incorporates
these	mathematical	concepts.	This	book	assumes	no	prior	knowledge	of	computer	hardware.
A	computer	organization	and	architecture	class	is	customarily	a	prerequisite	for	an	undergraduate
operating	systems	class	(students	must	know	about	the	memory	hierarchy,	concurrency,	exceptions,	and
interrupts),	 compilers	 (students	 must	 know	 about	 instruction	 sets,	 memory	 addressing,	 and	 linking),
networking	 (students	 must	 understand	 the	 hardware	 of	 a	 system	 before	 attempting	 to	 understand	 the
network	that	ties	these	components	together),	and	of	course,	any	advanced	architecture	class.	This	text
covers	the	topics	necessary	for	these	courses.
General	Organization	and	Coverage
Our	presentation	of	concepts	in	this	textbook	is	an	attempt	at	a	concise	yet	thorough	coverage	of	the	topics
we	 feel	 are	 essential	 for	 the	 computer	 science	 major.	 We	 do	 not	 feel	 the	 best	 way	 to	 do	 this	 is	 by
“compartmentalizing”	the	various	topics;	therefore,	we	have	chosen	a	structured	yet	integrated	approach
where	each	topic	is	covered	in	the	context	of	the	entire	computer	system.
As	with	many	popular	texts,	we	have	taken	a	bottom-up	approach,	starting	with	the	digital	logic	level
and	building	to	the	application	level	that	students	should	be	familiar	with	before	starting	the	class.	The
text	is	carefully	structured	so	that	the	reader	understands	one	level	before	moving	on	to	the	next.	By	the
time	the	reader	reaches	the	application	level,	all	the	necessary	concepts	in	computer	organization	and
architecture	have	been	presented.	Our	goal	is	to	allow	the	students	to	tie	the	hardware	knowledge	covered
in	this	book	to	the	concepts	learned	in	their	introductory	programming	classes,	resulting	in	a	complete	and

thorough	 picture	 of	 how	 hardware	 and	 software	 fit	 together.	 Ultimately,	 the	 extent	 of	 hardware
understanding	has	a	significant	influence	on	software	design	and	performance.	If	students	can	build	a	firm
foundation	 in	 hardware	 fundamentals,	 this	 will	 go	 a	 long	 way	 toward	 helping	 them	 to	 become	 better
computer	scientists.
The	concepts	in	computer	organization	and	architecture	are	integral	to	many	of	the	everyday	tasks	that
computer	professionals	perform.	To	address	the	numerous	areas	in	which	a	computer	professional	should
be	educated,	we	have	taken	a	high-level	look	at	computer	architecture,	providing	low-level	coverage	only
when	deemed	necessary	for	an	understanding	of	a	specific	concept.	For	example,	when	discussing	ISAs,
many	 hardware-dependent	 issues	 are	 introduced	 in	 the	 context	 of	 different	 case	 studies	 to	 both
differentiate	and	reinforce	the	issues	associated	with	ISA	design.
The	text	is	divided	into	13	chapters	and	an	appendix,	as	follows:
•		Chapter	1	provides	a	historical	overview	of	computing	in	general,	pointing	out	the	many	milestones	in
the	development	of	computing	systems	and	allowing	the	reader	to	visualize	how	we	arrived	at	the
current	state	of	computing.	This	chapter	introduces	the	necessary	terminology,	the	basic	components	in	a
computer	 system,	 the	 various	 logical	 levels	 of	 a	 computer	 system,	 and	 the	 von	 Neumann	 computer
model.	It	provides	a	high-level	view	of	the	computer	system,	as	well	as	the	motivation	and	necessary
concepts	for	further	study.
•		Chapter	2	provides	thorough	coverage	of	the	various	means	computers	use	to	represent	both	numerical
and	 character	 information.	 Addition,	 subtraction,	 multiplication,	 and	 division	 are	 covered	 once	 the
reader	has	been	exposed	to	number	bases	and	the	typical	numeric	representation	techniques,	including
one’s	complement,	two’s	complement,	and	BCD.	In	addition,	EBCDIC,	ASCII,	and	Unicode	character
representations	are	addressed.	Fixed-	and	floating-point	representation	are	also	introduced.	Codes	for
data	recording	and	error	detection	and	correction	are	covered	briefly.	Codes	for	data	transmission	and
recording	are	described	in	a	special	“Focus	On”	section.
•		Chapter	3	is	a	classic	presentation	of	digital	logic	and	how	it	relates	to	Boolean	algebra.	This	chapter
covers	both	combinational	and	sequential	logic	in	sufficient	detail	to	allow	the	reader	to	understand	the
logical	makeup	of	more	complicated	MSI	(medium-scale	integration)	circuits	(such	as	decoders).	More
complex	circuits,	such	as	buses	and	memory,	are	also	included.	We	have	included	optimization	and
Kmaps	in	a	special	“Focus	On”	section.
•		Chapter	4	illustrates	basic	computer	organization	and	introduces	many	fundamental	concepts,	including
the	 fetch–decode–execute	 cycle,	 the	 data	 path,	 clocks	 and	 buses,	 register	 transfer	 notation,	 and,	 of
course,	the	CPU.	A	very	simple	architecture,	MARIE,	and	its	ISA	are	presented	to	allow	the	reader	to
gain	a	full	understanding	of	the	basic	architectural	organization	involved	in	program	execution.	MARIE
exhibits	the	classic	von	Neumann	design	and	includes	a	program	counter,	an	accumulator,	an	instruction
register,	 4096	 bytes	 of	 memory,	 and	 two	 addressing	 modes.	 Assembly	 language	 programming	 is
introduced	to	reinforce	the	concepts	of	instruction	format,	instruction	mode,	data	format,	and	control
that	are	presented	earlier.	This	is	not	an	assembly	language	textbook	and	was	not	designed	to	provide	a
practical	course	in	assembly	language	programming.	The	primary	objective	in	introducing	assembly	is
to	further	the	understanding	of	computer	architecture	in	general.	However,	a	simulator	for	MARIE	is
provided	 so	 assembly	 language	 programs	 can	 be	 written,	 assembled,	 and	 run	 on	 the	 MARIE
architecture.	 The	 two	 methods	 of	 control,	 hardwiring	 and	 microprogramming,	 are	 introduced	 and
compared	in	this	chapter.	Finally,	Intel	and	MIPS	architectures	are	compared	to	reinforce	the	concepts
in	the	chapter.

•	 	Chapter	 5	 provides	 a	 closer	 look	 at	 instruction	 set	 architectures,	 including	 instruction	 formats,
instruction	types,	and	addressing	modes.	Instruction-level	pipelining	is	introduced	as	well.	Real-world
ISAs	 (including	 Intel®,	 MIPS®	 Technologies,	 ARM,	 and	 Java™)	 are	 presented	 to	 reinforce	 the
concepts	presented	in	the	chapter.
•		Chapter	6	covers	basic	memory	concepts,	such	as	RAM	and	the	various	memory	devices,	and	also
addresses	the	more	advanced	concepts	of	the	memory	hierarchy,	including	cache	memory	and	virtual
memory.	This	chapter	gives	a	thorough	presentation	of	direct	mapping,	associative	mapping,	and	set-
associative	mapping	techniques	for	cache.	It	also	provides	a	detailed	look	at	paging	and	segmentation,
TLBs,	and	the	various	algorithms	and	devices	associated	with	each.	A	tutorial	and	simulator	for	this
chapter	is	available	on	the	book’s	website.
•		Chapter	7	provides	a	detailed	overview	of	I/O	fundamentals,	bus	communication	and	protocols,	and
typical	external	storage	devices,	such	as	magnetic	and	optical	disks,	as	well	as	the	various	formats
available	for	each.	DMA,	programmed	I/O,	and	interrupts	are	covered	as	well.	In	addition,	various
techniques	for	exchanging	information	between	devices	are	introduced.	RAID	architectures	are	covered
in	detail.	Various	data	compression	formats	are	introduced	in	a	special	“Focus	On”	section.
•		Chapter	8	discusses	the	various	programming	tools	available	(such	as	compilers	and	assemblers)	and
their	relationship	to	the	architecture	of	the	machine	on	which	they	are	run.	The	goal	of	this	chapter	is	to
tie	 the	 programmer’s	 view	 of	 a	 computer	 system	 with	 the	 actual	 hardware	 and	 architecture	 of	 the
underlying	machine.	In	addition,	operating	systems	are	introduced,	but	only	covered	in	as	much	detail
as	applies	to	the	architecture	and	organization	of	a	system	(such	as	resource	use	and	protection,	traps
and	interrupts,	and	various	other	services).
•		Chapter	9	provides	an	overview	of	alternative	architectures	that	have	emerged	in	recent	years.	RISC,
Flynn’s	Taxonomy,	parallel	processors,	instruction-level	parallelism,	multiprocessors,	interconnection
networks,	 shared	 memory	 systems,	 cache	 coherence,	 memory	 models,	 superscalar	 machines,	 neural
networks,	systolic	architectures,	dataflow	computers,	quantum	computing,	and	distributed	architectures
are	covered.	Our	main	objective	in	this	chapter	is	to	help	the	reader	realize	we	are	not	limited	to	the
von	Neumann	architecture,	and	to	force	the	reader	to	consider	performance	issues,	setting	the	stage	for
the	next	chapter.
•		Chapter	10	covers	concepts	and	topics	of	interest	in	embedded	systems	that	have	not	been	covered	in
previous	 chapters.	 Specifically,	 this	 chapter	 focuses	 on	 embedded	 hardware	 and	 components,
embedded	 system	 design	 topics,	 the	 basics	 of	 embedded	 software	 construction,	 and	 embedded
operating	systems	features.
•	 	Chapter	 11	 addresses	 various	 performance	 analysis	 and	 management	 issues.	 The	 necessary
mathematical	preliminaries	are	introduced,	followed	by	a	discussion	of	MIPS,	FLOPS,	benchmarking,
and	various	optimization	issues	with	which	a	computer	scientist	should	be	familiar,	including	branch
prediction,	speculative	execution,	and	loop	optimization.
•	 	Chapter	12	 focuses	 on	 network	 organization	 and	 architecture,	 including	 network	 components	 and
protocols.	The	OSI	model	and	TCP/IP	suite	are	introduced	in	the	context	of	the	Internet.	This	chapter	is
by	no	means	intended	to	be	comprehensive.	The	main	objective	is	to	put	computer	architecture	in	the
correct	context	relative	to	network	architecture.
•		Chapter	13	introduces	some	popular	I/O	architectures	suitable	for	large	and	small	systems,	including
SCSI,	ATA,	IDE,	SATA,	PCI,	USB,	and	IEEE	1394.	This	chapter	also	provides	a	brief	overview	of
storage	area	networks	and	cloud	computing.

•		Appendix	A	is	a	short	appendix	on	data	structures	that	is	provided	for	those	situations	in	which	students
may	need	a	brief	introduction	or	review	of	such	topics	as	stacks,	queues,	and	linked	lists.
The	sequencing	of	the	chapters	is	such	that	they	can	be	taught	in	the	given	numerical	order.	However,
an	instructor	can	modify	the	order	to	better	fit	a	given	curriculum	if	necessary.	Figure	P.1	 shows	 the
prerequisite	relationships	that	exist	between	various	chapters.
FIGURE	P.1	Prerequisite	Relationship	Between	Chapters
What’s	New	in	the	Fourth	Edition
In	 the	 years	 since	 the	 third	 edition	 of	 this	 book	 was	 created,	 the	 field	 of	 computer	 architecture	 has
continued	to	grow.	In	this	fourth	edition,	we	have	incorporated	many	of	these	new	changes	in	addition	to
expanding	topics	already	introduced	in	the	first	three	editions.	Our	goal	in	the	fourth	edition	was	to	update
content	and	references,	add	new	material,	expand	current	discussions	based	on	reader	comments,	and
expand	the	number	of	exercises	in	all	of	the	core	chapters.	Although	we	cannot	itemize	all	the	changes	in
this	edition,	the	list	that	follows	highlights	those	major	changes	that	may	be	of	interest	to	the	reader:
•		Chapter	1	has	been	updated	to	include	new	examples	and	illustrations,	tablet	computers,	computing	as
a	service	(Cloud	computing),	and	cognitive	computing.	The	hardware	overview	has	been	expanded	and

updated	(notably,	the	discussion	on	CRTs	has	been	removed	and	a	discussion	of	graphics	cards	has
been	added),	and	additional	motivational	sidebars	have	been	added.	The	non-von	Neumann	section	has
been	updated,	and	a	new	section	on	parallelism	has	been	included.	The	number	of	exercises	at	the	end
of	the	chapter	has	been	increased	by	26%.
•		Chapter	2	contains	a	new	section	on	excess-M	notation.	The	simple	model	has	been	modified	to	use	a
standard	format,	and	more	examples	have	been	added.	This	chapter	has	a	44%	increase	in	the	number
of	exercises.
•		Chapter	3	has	been	modified	to	use	a	prime	(′)	instead	of	an	overbar	to	indicate	the	NOT	operator.
Timing	diagrams	have	been	added	to	help	explain	the	operation	of	sequential	circuits.	The	section	on
FSMs	has	been	expanded,	and	additional	exercises	have	been	included.
•		Chapter	4	contains	an	expanded	discussion	of	memory	organization	(including	memory	interleaving)	as
well	as	additional	examples	and	exercises.	We	are	now	using	the	“0x”	notation	to	indicate	hexadecimal
numbers.	More	detail	has	been	added	to	the	discussions	on	hardwired	and	microprogrammed	control,
and	 the	 logic	 diagrams	 for	 MARIE’s	 hardwired	 control	 unit	 and	 the	 timing	 diagrams	 for	 MARIE’s
microoperations	have	all	been	updated.
•		Chapter	5	contains	expanded	coverage	of	big	and	little	endian	and	additional	examples	and	exercises,
as	well	as	a	new	section	on	ARM	processors.
•	 	Chapter	 6	 has	 updated	 figures,	 an	 expanded	 discussion	 of	 associative	 memory,	 and	 additional
examples	 and	 discussion	 to	 clarify	 cache	 memory.	 The	 examples	 have	 all	 been	 updated	 to	 reflect
hexadecimal	addresses	instead	of	decimal	addresses.	This	chapter	now	contains	20%	more	exercises
than	the	third	edition.
•		Chapter	7	has	expanded	coverage	of	solid	state	drives	and	emerging	data	storage	devices	(such	as
carbon	nanotubes	and	memristors),	as	well	as	additional	coverage	of	RAID.	There	is	a	new	section	on
MP3	compression	and	in	addition	to	a	20%	increase	in	the	number	of	exercises	at	the	end	of	this
chapter.
•		Chapter	8	has	been	updated	to	reflect	advances	in	the	field	of	system	software.
•		Chapter	9	has	an	expanded	discussion	of	both	RISC	vs.	CISC	(integrating	this	debate	into	the	mobile
arena)	and	quantum	computing,	including	a	discussion	of	the	technological	singularity.
•		Chapter	10	contains	updated	material	for	embedded	operating	systems.
•		Chapter	12	has	been	updated	to	remove	obsolete	material	and	integrate	new	material.
•		Chapter	13	has	expanded	and	updated	coverage	of	USB,	expanded	coverage	of	Cloud	storage,	and
removal	of	obsolete	material.
Intended	Audience
This	book	was	originally	written	for	an	undergraduate	class	in	computer	organization	and	architecture	for
computer	science	majors.	Although	specifically	directed	toward	computer	science	majors,	the	book	does
not	preclude	its	use	by	IS	and	IT	majors.
This	book	contains	more	than	sufficient	material	for	a	typical	one-semester	(14	weeks,	42	lecture
hours)	course;	however,	all	the	material	in	the	book	cannot	be	mastered	by	the	average	student	in	a	one-
semester	class.	If	the	instructor	plans	to	cover	all	topics	in	detail,	a	two-semester	sequence	would	be
optimal.	The	organization	is	such	that	an	instructor	can	cover	the	major	topic	areas	at	different	levels	of
depth,	depending	on	the	experience	and	needs	of	the	students.	Table	P.2	gives	the	instructor	an	idea	of	the

amount	of	time	required	to	cover	the	topics,	and	also	lists	the	corresponding	levels	of	accomplishment	for
each	chapter.
It	is	our	intention	that	this	book	serve	as	a	useful	reference	long	after	the	formal	course	is	complete.
TABLE	P.2	Suggested	Lecture	Hours
Support	Materials
A	textbook	is	a	fundamental	tool	in	learning,	but	its	effectiveness	is	greatly	enhanced	by	supplemental
materials	and	exercises,	which	emphasize	the	major	concepts,	provide	immediate	feedback	to	the	reader,
and	 motivate	 understanding	 through	 repetition.	 We	 have,	 therefore,	 created	 the	 following	 ancillary
materials	for	the	fourth	edition	of	The	Essentials	of	Computer	Organization	and	Architecture:
•		Test	bank.
•	 	Instructor’s	Manual.	 This	 manual	 contains	 answers	 to	 exercises.	 In	 addition,	 it	 provides	 hints	 on
teaching	various	concepts	and	trouble	areas	often	encountered	by	students.
•		PowerPoint	Presentations.	These	slides	contain	lecture	material	appropriate	for	a	one-semester	course
in	computer	organization	and	architecture.
•		Figures	and	Tables.	For	those	who	wish	to	prepare	their	own	lecture	materials,	we	provide	the	figures
and	tables	in	downloadable	form.
•		Memory	Tutorial	and	Simulator.	This	package	allows	students	to	apply	the	concepts	on	cache	and
virtual	memory.
•		MARIE	Simulator.	This	package	allows	students	to	assemble	and	run	MARIE	programs.
•		Datapath	Simulator.	This	package	allows	students	to	trace	the	MARIE	datapath.
•		Tutorial	Software.	Other	tutorial	software	is	provided	for	various	concepts	in	the	book.
•		Companion	Website.	All	software,	slides,	and	related	materials	can	be	downloaded	from	the	book’s
website:
go.jblearning.com/ecoa4e

The	 exercises,	 sample	 exam	 problems,	 and	 solutions	 have	 been	 tested	 in	 numerous	 classes.	 The
Instructor’s	Manual,	which	includes	suggestions	for	teaching	the	various	chapters	in	addition	to	answers
for	the	book’s	exercises,	suggested	programming	assignments,	and	sample	example	questions,	is	available
to	instructors	who	adopt	the	book.	(Please	contact	your	Jones	&	Bartlett	Learning	representative	at	1-800-
832-0034	for	access	to	this	area	of	the	website.)
The	Instructional	Model:	MARIE
In	a	computer	organization	and	architecture	book,	the	choice	of	architectural	model	affects	the	instructor
as	well	as	the	students.	If	the	model	is	too	complicated,	both	the	instructor	and	the	students	tend	to	get
bogged	 down	 in	 details	 that	 really	 have	 no	 bearing	 on	 the	 concepts	 being	 presented	 in	 class.	 Real
architectures,	 although	 interesting,	 often	 have	 far	 too	 many	 peculiarities	 to	 make	 them	 usable	 in	 an
introductory	class.	To	make	things	even	more	complicated,	real	architectures	change	from	day	to	day.	In
addition,	it	is	difficult	to	find	a	book	incorporating	a	model	that	matches	the	local	computing	platform	in	a
given	department,	noting	that	the	platform,	too,	may	change	from	year	to	year.
To	alleviate	these	problems,	we	have	designed	our	own	simple	architecture,	MARIE,	specifically	for
pedagogical	use.	MARIE	(Machine	Architecture	that	is	Really	Intuitive	and	Easy)	allows	students	to	learn
the	essential	concepts	of	computer	organization	and	architecture,	including	assembly	language,	without
getting	 caught	 up	 in	 the	 unnecessary	 and	 confusing	 details	 that	 exist	 in	 real	 architectures.	 Despite	 its
simplicity,	it	simulates	a	functional	system.	The	MARIE	machine	simulator,	MarieSim,	has	a	user-friendly
GUI	that	allows	students	to	(1)	create	and	edit	source	code,	(2)	assemble	source	code	into	machine	object
code,	(3)	run	machine	code,	and	(4)	debug	programs.
Specifically,	MarieSim	has	the	following	features:
•		Support	for	the	MARIE	assembly	language	introduced	in	Chapter	4
•		An	integrated	text	editor	for	program	creation	and	modification
•		Hexadecimal	machine	language	object	code
•		An	integrated	debugger	with	single	step	mode,	break	points,	pause,	resume,	and	register	and	memory
tracing
•		A	graphical	memory	monitor	displaying	the	4096	addresses	in	MARIE’s	memory
•		A	graphical	display	of	MARIE’s	registers
•		Highlighted	instructions	during	program	execution
•		User-controlled	execution	speed
•		Status	messages
•		User-viewable	symbol	tables
•		An	interactive	assembler	that	lets	the	user	correct	any	errors	and	reassemble	automatically,	without
changing	environments
•		Online	help
•		Optional	core	dumps,	allowing	the	user	to	specify	the	memory	range
•		Frames	with	sizes	that	can	be	modified	by	the	user
•		A	small	learning	curve,	allowing	students	to	learn	the	system	quickly
MarieSim	was	written	in	the	Java	language	so	that	the	system	would	be	portable	to	any	platform	for

which	a	Java	Virtual	Machine	(JVM)	is	available.	Students	of	Java	may	wish	to	look	at	the	simulator’s
source	code,	and	perhaps	even	offer	improvements	or	enhancements	to	its	simple	functions.
Figure	P.2,	the	MarieSim	Graphical	Environment,	shows	the	graphical	environment	of	the	MARIE
machine	simulator.	The	screen	consists	of	four	parts:	the	menu	bar,	the	central	monitor	area,	the	memory
monitor,	and	the	message	area.
Menu	options	allow	the	user	to	control	the	actions	and	behavior	of	the	MARIE	simulator	system.
These	options	include	loading,	starting,	stopping,	setting	breakpoints,	and	pausing	programs	that	have
been	written	in	MARIE	assembly	language.
The	MARIE	simulator	illustrates	the	process	of	assembly,	loading,	and	execution,	all	in	one	simple
environment.	Users	can	see	assembly	language	statements	directly	from	their	programs,	along	with	the
corresponding	machine	code	(hexadecimal)	equivalents.	The	addresses	of	these	instructions	are	indicated
as	well,	and	users	can	view	any	portion	of	memory	at	any	time.	Highlighting	is	used	to	indicate	the	initial
loading	address	of	a	program	in	addition	to	the	currently	executing	instruction	while	a	program	runs.	The
graphical	display	of	the	registers	and	memory	allows	the	student	to	see	how	the	instructions	cause	the
values	in	the	registers	and	memory	to	change.
FIGURE	P.2	The	MarieSim	Graphical	Environment
If	You	Find	an	Error
We	have	attempted	to	make	this	book	as	technically	accurate	as	possible,	but	even	though	the	manuscript
has	been	through	numerous	proofreadings,	errors	have	a	way	of	escaping	detection.	We	would	greatly
appreciate	hearing	from	readers	who	find	any	errors	that	need	correcting.	Your	comments	and	suggestions
are	always	welcome;	please	send	on	email	to	ECOA@jblearning.com.
Credits	and	Acknowledgments
Few	books	are	entirely	the	result	of	one	or	two	people’s	unaided	efforts,	and	this	one	is	no	exception.	We

realize	that	writing	a	textbook	is	a	formidable	task	and	only	possible	with	a	combined	effort,	and	we	find
it	 impossible	 to	 adequately	 thank	 those	 who	 have	 made	 this	 book	 possible.	 If,	 in	 the	 following
acknowledgments,	we	inadvertently	omit	anyone,	we	humbly	apologize.
A	number	of	people	have	contributed	to	the	fourth	edition	of	this	book.	We	would	first	like	to	thank	all
of	the	reviewers	for	their	careful	evaluations	of	previous	editions	and	their	thoughtful	written	comments.
In	addition,	we	are	grateful	for	the	many	readers	who	have	emailed	useful	ideas	and	helpful	suggestions.
Although	we	cannot	mention	all	of	these	people	here,	we	especially	thank	John	MacCormick	(Dickinson
College)	 and	 Jacqueline	 Jones	 (Brooklyn	 College)	 for	 their	 meticulous	 reviews	 and	 their	 numerous
comments	and	suggestions.	We	extend	a	special	thanks	to	Karishma	Rao	and	Sean	Willeford	for	their	time
and	effort	in	producing	a	quality	memory	software	module.
We	would	also	like	to	thank	the	individuals	at	Jones	&	Bartlett	Learning	who	worked	closely	with	us
to	make	this	fourth	edition	possible.	We	are	very	grateful	to	Tiffany	Silter,	Laura	Pagluica,	and	Amy	Rose
for	their	professionalism,	commitment,	and	hard	work	on	the	fourth	edition.
I,	Linda	Null,	would	personally	like	to	thank	my	husband,	Tim	Wahls,	for	his	continued	patience	while
living	life	as	a	“book	widower”	for	a	fourth	time,	for	listening	and	commenting	with	frankness	about	the
book’s	contents	and	modifications,	for	doing	such	an	extraordinary	job	with	all	of	the	cooking,	and	for
putting	up	with	the	almost	daily	compromises	necessitated	by	my	writing	this	book—including	missing
our	annual	fly-fishing	vacation	and	forcing	our	horses	into	prolonged	pasture	ornament	status.	I	consider
myself	amazingly	lucky	to	be	married	to	such	a	wonderful	man.	I	extend	my	heartfelt	thanks	to	my	mentor,
Merry	 McDonald,	 who	 taught	 me	 the	 value	 and	 joys	 of	 learning	 and	 teaching,	 and	 doing	 both	 with
integrity.	Lastly,	I	would	like	to	express	my	deepest	gratitude	to	Julia	Lobur,	as	without	her,	this	book	and
its	accompanying	software	would	not	be	a	reality.	It	has	been	both	a	joy	and	an	honor	working	with	her.
I,	Julia	Lobur,	am	deeply	indebted	to	my	lawful	spouse,	Marla	Cattermole,	who	married	me	despite
the	 demands	 that	 this	 book	 has	 placed	 on	 both	 of	 us.	 She	 has	 made	 this	 work	 possible	 through	 her
forbearance	and	fidelity.	She	has	nurtured	my	body	through	her	culinary	delights	and	my	spirit	through	her
wisdom.	She	has	taken	up	my	slack	in	many	ways	while	working	hard	at	her	own	career.	I	would	also	like
to	convey	my	profound	gratitude	to	Linda	Null:	first,	for	her	unsurpassed	devotion	to	the	field	of	computer
science	education	and	dedication	to	her	students	and,	second,	for	giving	me	the	opportunity	to	share	with
her	the	ineffable	experience	of	textbook	authorship.

“Computing	is	not	about	computers	anymore.	It	is	about	living....	We	have	seen	computers
move	out	of	giant	air-conditioned	rooms	into	closets,	then	onto	desktops,	and	now	into	our
laps	and	pockets.	But	this	is	not	the	end....	Like	a	force	of	nature,	the	digital	age	cannot	be
denied	or	stopped....	The	information	superhighway	may	be	mostly	hype	today,	but	it	is	an
understatement	about	tomorrow.	It	will	exist	beyond	people’s	wildest	predictions....	We	are
not	waiting	on	any	invention.	It	is	here.	It	is	now.	It	is	almost	genetic	in	its	nature,	in	that
each	generation	will	become	more	digital	than	the	preceding	one.”
—Nicholas	Negroponte,	professor	of	media	technology	at	MIT
CHAPTER	1

Introduction
1.1			OVERVIEW
Dr.	Negroponte	is	among	many	who	see	the	computer	revolution	as	if	it	were	a	force	of	nature.	This	force
has	the	potential	to	carry	humanity	to	its	digital	destiny,	allowing	us	to	conquer	problems	that	have	eluded
us	for	centuries,	as	well	as	all	of	the	problems	that	emerge	as	we	solve	the	original	problems.	Computers
have	freed	us	from	the	tedium	of	routine	tasks,	liberating	our	collective	creative	potential	so	that	we	can,
of	course,	build	bigger	and	better	computers.
As	we	observe	the	profound	scientific	and	social	changes	that	computers	have	brought	us,	it	is	easy	to
start	feeling	overwhelmed	by	the	complexity	of	it	all.	This	complexity,	however,	emanates	from	concepts
that	are	fundamentally	very	simple.	These	simple	ideas	are	the	ones	that	have	brought	us	to	where	we	are
today	and	are	the	foundation	for	the	computers	of	the	future.	To	what	extent	they	will	survive	in	the	future
is	anybody’s	guess.	But	today,	they	are	the	foundation	for	all	of	computer	science	as	we	know	it.
Computer	scientists	are	usually	more	concerned	with	writing	complex	program	algorithms	than	with
designing	computer	hardware.	Of	course,	if	we	want	our	algorithms	to	be	useful,	a	computer	eventually
has	to	run	them.	Some	algorithms	are	so	complicated	that	they	would	take	too	long	to	run	on	today’s
systems.	These	kinds	of	algorithms	are	considered	computationally	infeasible.	Certainly,	at	the	current
rate	of	innovation,	some	things	that	are	infeasible	today	could	be	feasible	tomorrow,	but	it	seems	that	no
matter	 how	 big	 or	 fast	 computers	 become,	 someone	 will	 think	 up	 a	 problem	 that	 will	 exceed	 the
reasonable	limits	of	the	machine.
To	understand	why	an	algorithm	is	infeasible,	or	to	understand	why	the	implementation	of	a	feasible
algorithm	is	running	too	slowly,	you	must	be	able	to	see	the	program	from	the	computer’s	point	of	view.
You	must	understand	what	makes	a	computer	system	tick	before	you	can	attempt	to	optimize	the	programs
that	it	runs.	Attempting	to	optimize	a	computer	system	without	first	understanding	it	is	like	attempting	to
tune	your	car	by	pouring	an	elixir	into	the	gas	tank:	You’ll	be	lucky	if	it	runs	at	all	when	you’re	finished.
Program	optimization	and	system	tuning	are	perhaps	the	most	important	motivations	for	learning	how
computers	work.	There	are,	however,	many	other	reasons.	For	example,	if	you	want	to	write	compilers,
you	 must	 understand	 the	 hardware	 environment	 within	 which	 the	 compiler	 will	 function.	 The	 best
compilers	leverage	particular	hardware	features	(such	as	pipelining)	for	greater	speed	and	efficiency.
If	you	ever	need	to	model	large,	complex,	real-world	systems,	you	will	need	to	know	how	floating-
point	arithmetic	should	work	as	well	as	how	it	really	works	in	practice.	If	you	wish	to	design	peripheral
equipment	 or	 the	 software	 that	 drives	 peripheral	 equipment,	 you	 must	 know	 every	 detail	 of	 how	 a
particular	computer	deals	with	its	input/output	(I/O).	If	your	work	involves	embedded	systems,	you	need
to	know	that	these	systems	are	usually	resource-constrained.	Your	understanding	of	time,	space,	and	price
trade-offs,	as	well	as	I/O	architectures,	will	be	essential	to	your	career.
All	 computer	 professionals	 should	 be	 familiar	 with	 the	 concepts	 of	 benchmarking	 and	 be	 able	 to
interpret	 and	 present	 the	 results	 of	 benchmarking	 systems.	 People	 who	 perform	 research	 involving
hardware	 systems,	 networks,	 or	 algorithms	 find	 benchmarking	 techniques	 crucial	 to	 their	 day-to-day
work.	Technical	managers	in	charge	of	buying	hardware	also	use	benchmarks	to	help	them	buy	the	best
system	for	a	given	amount	of	money,	keeping	in	mind	the	ways	in	which	performance	benchmarks	can	be

manipulated	to	imply	results	favorable	to	particular	systems.
The	preceding	examples	illustrate	the	idea	that	a	fundamental	relationship	exists	between	computer
hardware	and	many	aspects	of	programming	and	software	components	in	computer	systems.	Therefore,
regardless	 of	 our	 areas	 of	 expertise,	 as	 computer	 scientists,	 it	 is	 imperative	 that	 we	 understand	 how
hardware	interacts	with	software.	We	must	become	familiar	with	how	various	circuits	and	components	fit
together	to	create	working	computer	systems.	We	do	this	through	the	study	of	computer	organization.
Computer	 organization	 addresses	 issues	 such	 as	 control	 signals	 (how	 the	 computer	 is	 controlled),
signaling	methods,	and	memory	types.	It	encompasses	all	physical	aspects	of	computer	systems.	It	helps
us	to	answer	the	question:	How	does	a	computer	work?
The	study	of	computer	architecture,	on	the	other	hand,	focuses	on	the	structure	and	behavior	of	the
computer	system	and	refers	to	the	logical	and	abstract	aspects	of	system	implementation	as	seen	by	the
programmer.	 Computer	 architecture	 includes	 many	 elements	 such	 as	 instruction	 sets	 and	 formats,
operation	codes,	data	types,	the	number	and	types	of	registers,	addressing	modes,	main	memory	access
methods,	and	various	I/O	mechanisms.	The	architecture	of	a	system	directly	affects	the	logical	execution
of	 programs.	 Studying	 computer	 architecture	 helps	 us	 to	 answer	 the	 question:	 How	 do	 I	 design	 a
computer?
The	computer	architecture	for	a	given	machine	is	the	combination	of	its	hardware	components	plus	its
instruction	set	architecture	(ISA).	The	ISA	is	the	agreed-upon	interface	between	all	the	software	that
runs	on	the	machine	and	the	hardware	that	executes	it.	The	ISA	allows	you	to	talk	to	the	machine.
The	distinction	between	computer	organization	and	computer	architecture	is	not	clear-cut.	People	in
the	 fields	 of	 computer	 science	 and	 computer	 engineering	 hold	 differing	 opinions	 as	 to	 exactly	 which
concepts	pertain	to	computer	organization	and	which	pertain	to	computer	architecture.	In	fact,	neither
computer	 organization	 nor	 computer	 architecture	 can	 stand	 alone.	 They	 are	 interrelated	 and
interdependent.	 We	 can	 truly	 understand	 each	 of	 them	 only	 after	 we	 comprehend	 both	 of	 them.	 Our
comprehension	of	computer	organization	and	architecture	ultimately	leads	to	a	deeper	understanding	of
computers	and	computation—the	heart	and	soul	of	computer	science.
1.2			THE	MAIN	COMPONENTS	OF	A	COMPUTER
Although	it	is	difficult	to	distinguish	between	the	ideas	belonging	to	computer	organization	and	those
ideas	belonging	to	computer	architecture,	it	is	impossible	to	say	where	hardware	issues	end	and	software
issues	begin.	Computer	scientists	design	algorithms	that	usually	are	implemented	as	programs	written	in
some	computer	language,	such	as	Java	or	C++.	But	what	makes	the	algorithm	run?	Another	algorithm,	of
course!	And	another	algorithm	runs	that	algorithm,	and	so	on	until	you	get	down	to	the	machine	level,
which	can	be	thought	of	as	an	algorithm	implemented	as	an	electronic	device.	Thus,	modern	computers
are	actually	implementations	of	algorithms	that	execute	other	algorithms.	This	chain	of	nested	algorithms
leads	us	to	the	following	principle:
Principle	of	Equivalence	of	Hardware	and	Software:	Any	task	done	by	software	can	also	be	done
using	hardware,	and	any	operation	performed	directly	by	hardware	can	be	done	using	software.
1
A	 special-purpose	 computer	 can	 be	 designed	 to	 perform	 any	 task,	 such	 as	 word	 processing,	 budget
analysis,	or	playing	a	friendly	game	of	Tetris.	Accordingly,	programs	can	be	written	to	carry	out	the
functions	of	special-purpose	computers,	such	as	the	embedded	systems	situated	in	your	car	or	microwave.
There	are	times	when	a	simple	embedded	system	gives	us	much	better	performance	than	a	complicated

computer	 program,	 and	 there	 are	 times	 when	 a	 program	 is	 the	 preferred	 approach.	 The	 Principle	 of
Equivalence	 of	 Hardware	 and	 Software	 tells	 us	 that	 we	 have	 a	 choice.	 Our	 knowledge	 of	 computer
organization	and	architecture	will	help	us	to	make	the	best	choice.
We	begin	our	discussion	of	computer	hardware	by	looking	at	the	components	necessary	to	build	a
computing	system.	At	the	most	basic	level,	a	computer	is	a	device	consisting	of	three	pieces:
1.		A	processor	to	interpret	and	execute	programs
2.		A	memory	to	store	both	data	and	programs
3.		A	mechanism	for	transferring	data	to	and	from	the	outside	world
We	 discuss	 these	 three	 components	 in	 detail	 as	 they	 relate	 to	 computer	 hardware	 in	 the	 following
chapters.
Once	you	understand	computers	in	terms	of	their	component	parts,	you	should	be	able	to	understand
what	a	system	is	doing	at	all	times	and	how	you	could	change	its	behavior	if	so	desired.	You	might	even
feel	like	you	have	a	few	things	in	common	with	it.	This	idea	is	not	as	far-fetched	as	it	appears.	Consider
how	a	student	sitting	in	class	exhibits	the	three	components	of	a	computer:	The	student’s	brain	is	the
processor,	the	notes	being	taken	represent	the	memory,	and	the	pencil	or	pen	used	to	take	notes	is	the	I/O
mechanism.	But	keep	in	mind	that	your	abilities	far	surpass	those	of	any	computer	in	the	world	today,	or
any	that	can	be	built	in	the	foreseeable	future.
1.3			AN	EXAMPLE	SYSTEM:	WADING	THROUGH	THE
JARGON
This	text	will	introduce	you	to	some	of	the	vocabulary	that	is	specific	to	computers.	This	jargon	can	be
confusing,	imprecise,	and	intimidating.	We	believe	that	with	a	little	explanation,	we	can	clear	the	fog.
For	the	sake	of	discussion,	we	have	provided	a	facsimile	computer	advertisement	(see	Figure	1.1).
The	ad	is	typical	of	many	in	that	it	bombards	the	reader	with	phrases	such	as	“32GB	DDR3	SDRAM,”
“PCIe	sound	card,”	and	“128KB	L1	cache.”	Without	having	a	handle	on	such	terminology,	you	would	be
hard-pressed	to	know	whether	the	stated	system	is	a	wise	buy,	or	even	whether	the	system	is	able	to	serve
your	needs.	As	we	progress	through	this	text,	you	will	learn	the	concepts	behind	these	terms.

FIGURE	1.1	A	Typical	Computer	Advertisement
Before	we	explain	the	ad,	however,	we	need	to	discuss	something	even	more	basic:	the	measurement
terminology	you	will	encounter	throughout	your	study	of	computers.
It	seems	that	every	field	has	its	own	way	of	measuring	things.	The	computer	field	is	no	exception.	For
computer	people	to	tell	each	other	how	big	something	is,	or	how	fast	something	is,	they	must	use	the	same
units	of	measure.	The	common	prefixes	used	with	computers	are	given	in	Table	1.1.	Back	in	the	1960s,
someone	decided	that	because	the	powers	of	2	were	close	to	the	powers	of	10,	the	same	prefix	names
could	be	used	for	both.	For	example,	2
10
	is	close	to	10
3
,	so	“kilo”	is	used	to	refer	to	them	both.	The	result
has	been	mass	confusion:	Does	a	given	prefix	refer	to	a	power	of	10	or	a	power	of	2?	Does	a	kilo	mean
10
3
	of	something	or	2
10
	of	something?	Although	there	is	no	definitive	answer	to	this	question,	there	are
accepted	“standards	of	usage.”	Power-of-10	prefixes	are	ordinarily	used	for	power,	electrical	voltage,
frequency	(such	as	computer	clock	speeds),	and	multiples	of	bits	(such	as	data	speeds	in	number	of	bits
per	second).	If	your	antiquated	modem	transmits	at	28.8kb/s,	then	it	transmits	28,800	bits	per	second	(or
28.8	×	10
3
).	Note	the	use	of	the	lowercase	“k”	to	mean	10
3
	and	the	lowercase	“b”	to	refer	to	bits.	An
uppercase	“K”	is	used	to	refer	to	the	power-of-2	prefix,	or	1024.	If	a	file	is	2KB	in	size,	then	it	is	2	×	2
10
or	2048	bytes.	Note	the	uppercase	“B”	to	refer	to	byte.	If	a	disk	holds	1MB,	then	it	holds	2
20
	bytes	(or	one
megabyte)	of	information.
Not	knowing	whether	specific	prefixes	refer	to	powers	of	2	or	powers	of	10	can	be	very	confusing.
For	this	reason,	the	International	Electrotechnical	Commission,	with	help	from	the	National	Institute	of
Standards	and	Technology,	has	approved	standard	names	and	symbols	for	binary	prefixes	to	differentiate
them	from	decimal	prefixes.	Each	prefix	is	derived	from	the	symbols	given	in	Table	1.1	by	adding	an	“i.”
For	example,	2
10
	has	been	renamed	“kibi”	(for	kilobinary)	and	is	represented	by	the	symbol	Ki.	Similarly,
2
20
	 is	 mebi,	 or	 Mi,	 followed	 by	 gibi	 (Gi),	 tebi	 (Ti),	 pebi	 (Pi),	 exbi	 (Ei),	 and	 so	 on.	 Thus,	 the	 term
mebibyte,	which	means	2
20
	bytes,	replaces	what	we	traditionally	call	a	megabyte.

TABLE	1.1	Common	Prefixes	Associated	with	Computer	Organization	and	Architecture
There	has	been	limited	adoption	of	these	new	prefixes.	This	is	unfortunate	because,	as	a	computer
user,	it	is	important	to	understand	the	true	meaning	of	these	prefixes.	A	kilobyte	(1KB)	of	memory	is
typically	1024	bytes	of	memory	rather	than	1000	bytes	of	memory.	However,	a	1GB	disk	drive	might
actually	be	1	billion	bytes	instead	of	2
30
	(which	means	you	are	getting	less	storage	than	you	think).	All
3.5′′	floppy	disks	are	described	as	storing	1.44MB	of	data	when	in	fact	they	store	1440KB	(or	1440	×	2
10
=	1474560	bytes).	You	should	always	read	the	manufacturer’s	fine	print	just	to	make	sure	you	know
exactly	what	1K,	1KB,	or	1G	represents.	See	the	sidebar	“When	a	Gigabyte	Isn’t	Quite	...”	for	a	good
example	of	why	this	is	so	important.
Who	Uses	Zettabytes	and	Yottabytes	Anyway?
The	 National	 Security	 Agency	 (NSA),	 an	 intelligence-gathering	 organization	 in	 the	 United	 States,
announced	that	its	new	Intelligence	Community	Comprehensive	National	Cybersecurity	Initiative	Data
Center,	in	Bluffdale,	Utah,	was	set	to	open	in	October	2013.	Approximately	100,000	square	feet	of	the
structure	is	utilized	for	the	data	center,	Whereas	the	remaining	900,000+	square	feet	houses	technical
support	and	administration.	The	new	data	center	will	help	the	NSA	monitor	the	vast	volume	of	data
traffic	on	the	Internet.
It	is	estimated	that	the	NSA	collects	roughly	2	million	gigabytes	of	data	every	hour,	24	hours	a	day,
seven	days	a	week.	This	data	includes	foreign	and	domestic	emails,	cell	phone	calls,	Internet	searches,
various	purchases,	and	other	forms	of	digital	data.	The	computer	responsible	for	analyzing	this	data	for
the	new	data	center	is	the	Titan	supercomputer,	a	water-cooled	machine	capable	of	operating	at	100
petaflops	 (or	 100,000	 trillion	 calculations	 each	 second).	 The	 PRISM	 (Planning	 Tool	 for	 Resource
Integration,	Synchronization,	and	Management)	surveillance	program	will	gather,	process,	and	track	all
collected	data.
Although	we	tend	to	think	in	terms	of	gigabytes	and	terabytes	when	buying	storage	for	our	personal
computers	and	other	devices,	the	NSA’s	data	center	storage	capacity	will	be	measured	in	zettabytes
(with	many	hypothesizing	that	storage	will	be	in	thousands	of	zettabytes,	or	yottabytes).	To	put	this	in
perspective,	in	a	2003	study	done	at	the	University	of	California	(UC)	Berkeley,	it	was	estimated	that
the	amount	of	new	data	created	in	2002	was	roughly	5EB.	An	earlier	study	by	UC	Berkeley	estimated
that	 by	 the	 end	 of	 1999,	 the	 sum	 of	 all	 information,	 including	 audio,	 video,	 and	 text,	 created	 by
humankind	was	approximately	12EB	of	data.	In	2006,	the	combined	storage	space	of	every	computer

hard	drive	in	the	world	was	estimated	at	160EB;	in	2009,	the	Internet	as	a	whole	was	estimated	to
contain	 roughly	 500	 total	 exabytes,	 or	 a	 half	 zettabyte,	 of	 data.	 Cisco,	 a	 U.S.	 computer	 network
hardware	manufacturer,	has	estimated	that	by	2016,	the	total	volume	of	data	on	the	global	internet	will
be	1.3ZB,	and	Seagate	Technology,	an	American	manufacturer	of	hard	drives,	has	estimated	that	the
total	storage	capacity	demand	will	reach	7ZB	in	2020.
The	NSA	is	not	the	only	organization	dealing	with	information	that	must	be	measured	in	numbers	of
bytes	 beyond	 the	 typical	 “giga”	 and	 “tera.”	 It	 is	 estimated	 that	 Facebook	 collects	 500TB	 of	 new
material	per	day;	YouTube	observes	roughly	1TB	of	new	video	information	every	four	minutes;	the
CERN	Large	Hadron	Collider	generates	1PB	of	data	per	second;	and	the	sensors	on	a	single,	new
Boeing	jet	engine	produce	20TB	of	data	every	hour.	Although	not	all	of	the	aforementioned	examples
require	permanent	storage	of	the	data	they	create/handle,	these	examples	nonetheless	provide	evidence
of	the	remarkable	quantity	of	data	we	deal	with	every	day.	This	tremendous	volume	of	information	is
what	prompted	the	IBM	Corporation,	in	2011,	to	develop	and	announce	its	new	120-PB	hard	drive,	a
storage	cluster	consisting	of	200,000	conventional	hard	drives	harnessed	to	work	together	as	a	single
unit.	If	you	plugged	your	MP3	player	into	this	drive,	you	would	have	roughly	two	billion	hours	of
music!
In	 this	 era	 of	 smartphones,	 tablets,	 Cloud	 computing,	 and	 other	 electronic	 devices,	 we	 will
continue	to	hear	people	talking	about	petabytes,	exabytes,	and	zettabytes	(and,	in	the	case	of	the	NSA,
even	 yottabytes).	 However,	 if	 we	 outgrow	 yottabytes,	 what	 then?	 In	 an	 effort	 to	 keep	 up	 with	 the
astronomical	growth	of	information	and	to	refer	to	even	bigger	volumes	of	data,	the	next	generation	of
prefixes	will	most	likely	include	the	terms	brontobyte	for	10
27
	and	gegobyte	for	10
30
	(although	some
argue	for	geobyte	and	geopbyte	as	the	prefixes	for	the	latter).	Although	these	are	not	yet	universally
accepted	international	prefix	units,	if	history	is	any	indication,	we	will	need	them	sooner	rather	than
later.
When	a	Gigabyte	Isn’t	Quite	...
Purchasing	 a	 new	 array	 of	 disk	 drives	 should	 be	 a	 relatively	 straightforward	 process	 once	 you
determine	your	technical	requirements	(e.g.,	disk	transfer	rate,	interface	type,	etc.).	From	here,	you
should	be	able	to	make	your	decision	based	on	a	simple	price/capacity	ratio,	such	as	dollars	per
gigabyte,	and	then	you’ll	be	done.	Well,	not	so	fast.
The	first	boulder	in	the	path	of	a	straightforward	analysis	is	that	you	must	make	sure	that	the	drives
you	are	comparing	all	express	their	capacities	either	in	formatted	or	unformatted	bytes.	As	much	as
16%	of	drive	space	is	consumed	during	the	formatting	process.	(Some	vendors	give	this	number	as
“usable	capacity.”)	Naturally,	the	price–capacity	ratio	looks	much	better	when	unformatted	bytes	are
used,	although	you	are	most	interested	in	knowing	the	amount	of	usable	space	a	disk	provides.
Your	next	obstacle	is	to	make	sure	that	the	same	radix	is	used	when	comparing	disk	sizes.	It	is
increasingly	common	for	disk	capacities	to	be	given	in	base	10	rather	than	base	2.	Thus,	a	“1GB”	disk
drive	has	a	capacity	of	10
9
	=	1,000,000,000	bytes,	rather	than	2
30
	=	1,073,741,824	bytes—a	reduction
of	about	7%.	This	can	make	a	huge	difference	when	purchasing	multigigabyte	enterprise-class	storage
systems.
As	 a	 concrete	 example,	 suppose	 you	 are	 considering	 purchasing	 a	 disk	 array	 from	 one	 of	 two
leading	 manufacturers.	 Manufacturer	x	 advertises	 an	 array	 of	 12	 250GB	disks	 for	 $20,000.

Manufacturer	y	is	offering	an	array	of	12	212.5GB	disks	for	$21,000.	All	other	things	being	equal,	the
cost	ratio	overwhelmingly	favors	Manufacturer	x:
Manufacturer	x:	$20,000	÷	(12	×	250GB)		$6.67	per	GB
Manufacturer	y:	$21,000	÷	(12	×	212.5GB)		$8.24	per	GB
Being	a	little	suspicious,	you	make	a	few	telephone	calls	and	learn	that	Manufacturer	x	is	citing
capacities	using	unformatted	base	10	gigabytes	and	Manufacturer	y	is	using	formatted	base	2	gigabytes.
These	facts	cast	the	problem	in	an	entirely	different	light:	To	start	with,	Manufacturer	x’s	disks	aren’t
really	 250GB	 in	 the	 way	 that	 we	 usually	 think	 of	 gigabytes.	 Instead,	 they	 are	 about	 232.8	 base	 2
gigabytes.	After	formatting,	the	number	reduces	even	more	to	about	197.9GB.	So	the	real	cost	ratios
are,	in	fact:
Manufacturer	x:	$20,000	÷	(12	×	197.9GB)		$8.42	per	GB
Manufacturer	y:	$21,000	÷	(12	×	212.5GB)		$8.24	per	GB
Indeed,	some	vendors	are	scrupulously	honest	in	disclosing	the	capabilities	of	their	equipment.
Unfortunately,	 others	 reveal	 the	 facts	 only	 under	 direct	 questioning.	 Your	 job	 as	 an	 educated
professional	is	to	ask	the	right	questions.
When	we	want	to	talk	about	how	fast	something	is,	we	speak	in	terms	of	fractions	of	a	second—
usually	thousandths,	millionths,	billionths,	or	trillionths.	Prefixes	for	these	metrics	are	given	in	the	right-
hand	side	of	Table	1.1.	Generally,	negative	powers	refer	to	powers	of	10,	not	powers	of	2.	For	this
reason,	the	new	binary	prefix	standards	do	not	include	any	new	names	for	the	negative	powers.	Notice
that	the	fractional	prefixes	have	exponents	that	are	the	reciprocal	of	the	prefixes	on	the	left	side	of	the
table.	Therefore,	if	someone	says	to	you	that	an	operation	requires	a	microsecond	to	complete,	you	should
also	understand	that	a	million	of	those	operations	could	take	place	in	one	second.	When	you	need	to	talk
about	how	many	of	these	things	happen	in	a	second,	you	would	use	the	prefix	mega-.	When	you	need	to
talk	about	how	fast	the	operations	are	performed,	you	would	use	the	prefix	micro-.
Now	to	explain	the	ad.	The	microprocessor	in	the	ad	is	an	Intel	i7	Quad	Core	processor	(which	means
it	is	essentially	four	processors)	and	belongs	to	a	category	of	processors	known	as	multicore	processors
(Section	 1.10	 contains	 more	 information	 on	 multicore	 processors).	 This	 particular	 processor	 runs	 at
3.9GHz.	Every	computer	system	contains	a	clock	that	keeps	the	system	synchronized.	The	clock	sends
electrical	pulses	simultaneously	to	all	main	components,	ensuring	that	data	and	instructions	will	be	where
they’re	supposed	to	be,	when	they’re	supposed	to	be	there.	The	number	of	pulsations	emitted	each	second
by	the	clock	is	its	frequency.	Clock	frequencies	are	measured	in	cycles	per	second,	or	hertz.	If	computer
system	clocks	generate	millions	of	pulses	per	second,	we	say	that	they	operate	in	the	megahertz	(MHz)
range.	Most	computers	today	operate	in	the	gigahertz	(GHz)	range,	generating	billions	of	pulses	per
second.	And	because	nothing	much	gets	done	in	a	computer	system	without	microprocessor	involvement,
the	frequency	rating	of	the	microprocessor	is	crucial	to	overall	system	speed.	The	microprocessor	of	the
system	in	our	advertisement	operates	at	3.9	billion	cycles	per	second,	so	the	seller	says	that	it	runs	at
3.9GHz.
The	 fact	 that	 this	 microprocessor	 runs	 at	 3.9GHz,	 however,	 doesn’t	 necessarily	 mean	 that	 it	 can
execute	 3.9	 billion	 instructions	 every	 second	 or,	 equivalently,	 that	 every	 instruction	 requires	 0.039
nanoseconds	to	execute.	Later	in	this	text,	you	will	see	that	each	computer	instruction	requires	a	fixed
number	 of	 cycles	 to	 execute.	 Some	 instructions	 require	 one	 clock	 cycle;	 however,	 most	 instructions

require	more	than	one.	The	number	of	instructions	per	second	that	a	microprocessor	can	actually	execute
is	proportionate	to	its	clock	speed.	The	number	of	clock	cycles	required	to	carry	out	a	particular	machine
instruction	is	a	function	of	both	the	machine’s	organization	and	its	architecture.
The	next	thing	we	see	in	the	ad	is	“1600MHz	32GB	DDR3	SDRAM.”	The	1600MHz	refers	to	the
speed	of	the	system	bus,	which	is	a	group	of	wires	that	moves	data	and	instructions	to	various	places
within	the	computer.	Like	the	microprocessor,	the	speed	of	the	bus	is	also	measured	in	MHz	or	GHz.
Many	computers	have	a	special	local	bus	for	data	that	supports	very	fast	transfer	speeds	(such	as	those
required	 by	 video).	 This	 local	 bus	 is	 a	 high-speed	 pathway	 that	 connects	 memory	 directly	 to	 the
processor.	Bus	speed	ultimately	sets	the	upper	limit	on	the	system’s	information-carrying	capability.
The	system	in	our	advertisement	also	boasts	a	memory	capacity	of	32	gigabytes	(GB),	or	about	32
billion	characters.	Memory	capacity	not	only	determines	the	size	of	the	programs	you	can	run,	but	also
how	many	programs	you	can	run	at	the	same	time	without	bogging	down	the	system.	Your	application	or
operating	system	manufacturer	will	usually	recommend	how	much	memory	you’ll	need	to	run	its	products.
(Sometimes	these	recommendations	can	be	hilariously	conservative,	so	be	careful	whom	you	believe!)
In	addition	to	memory	size,	our	advertised	system	provides	us	with	a	memory	type,	SDRAM,	short	for
synchronous	 dynamic	 random	 access	 memory.	 SDRAM	 is	 much	 faster	 than	 conventional
(nonsynchronous)	memory	because	it	can	synchronize	itself	with	a	microprocessor’s	bus.	The	system	in
our	 ad	 has	DDR3	SDRAM,	 or	double	 data	 rate	 type	 three	 SDRAM	 (for	 more	 information	 on	 the
different	types	of	memory,	see	Chapter	6).
A	Look	Inside	a	Computer
Have	you	ever	wondered	what	the	inside	of	a	computer	really	looks	like?	The	example	computer
described	in	this	section	gives	a	good	overview	of	the	components	of	a	modern	PC.	However,	opening
a	computer	and	attempting	to	find	and	identify	the	various	pieces	can	be	frustrating,	even	if	you	are
familiar	with	the	components	and	their	functions.

Photo	courtesy	of	Moxfyre	at	en.wikipedia	(from
http://commons.wikimedia.org/wiki/File:Acer_E360_Socket_939_motherboard_by_Foxconn.svg).
If	you	remove	the	cover	on	your	computer,	you	will	no	doubt	first	notice	a	big	metal	box	with	a	fan
attached.	This	is	the	power	supply.	You	will	also	see	various	drives,	including	a	hard	drive	and	a	DVD
drive	(or	perhaps	an	older	floppy	or	CD	drive).	There	are	many	integrated	circuits—small,	black
rectangular	boxes	with	legs	attached.	You	will	also	notice	electrical	pathways,	or	buses,	in	the	system.
There	are	printed	circuit	boards	(expansion	cards)	that	plug	into	sockets	on	the	motherboard,	the	large
board	at	the	bottom	of	a	standard	desktop	PC	or	on	the	side	of	a	PC	configured	as	a	tower	or	mini-
tower.	The	motherboard	is	the	printed	circuit	board	that	connects	all	the	components	in	the	computer,
including	the	CPU,	and	RAM	and	ROM,	as	well	as	an	assortment	of	other	essential	components.	The
components	on	the	motherboard	tend	to	be	the	most	difficult	to	identify.	Above	you	see	an	Acer	E360
motherboard	with	the	more	important	components	labeled.
The	Southbridge,	an	integrated	circuit	that	controls	the	hard	disk	and	I/O	(including	sound	and
video	cards),	is	a	hub	that	connects	slower	I/O	devices	to	the	system	bus.	These	devices	connect	via
the	I/O	ports	at	the	bottom	of	the	board.	The	PCI	slots	allow	for	expansion	boards	belonging	to	various
PCI	devices.	This	motherboard	also	has	PS/2	and	Firewire	connectors.	It	has	serial	and	parallel	ports,
in	addition	to	four	USB	ports.	This	motherboard	has	two	IDE	connector	slots,	four	SATA	connector
slots,	and	one	floppy	disk	controller.	The	super	I/O	chip	is	a	type	of	I/O	controller	that	controls	the
floppy	disk,	both	the	parallel	and	serial	ports,	and	the	keyboard	and	mouse.	The	motherboard	also	has
an	integrated	audio	chip,	as	well	as	an	integrated	Ethernet	chip	and	an	integrated	graphics	processor.
There	are	four	RAM	memory	banks.	There	is	no	processor	currently	plugged	into	this	motherboard,	but
we	see	the	socket	where	the	CPU	is	to	be	placed.	All	computers	have	an	internal	battery,	as	seen	in	the
top	middle	of	the	picture.	The	power	supply	plugs	into	the	power	connector.	The	BIOS	flash	chip
contains	the	instructions	in	ROM	that	your	computer	uses	when	it	is	first	powered	up.
A	note	of	caution	regarding	looking	inside	the	box:	There	are	many	safety	issues,	for	both	you	and
your	computer,	involved	with	removing	the	cover.	There	are	many	things	you	can	do	to	minimize	the

risks.	First	and	foremost,	make	sure	the	computer	is	turned	off.	Leaving	it	plugged	in	is	often	preferred,
as	this	offers	a	path	for	static	electricity.	Before	opening	your	computer	and	touching	anything	inside,
you	should	make	sure	you	are	properly	grounded	so	static	electricity	will	not	damage	any	components.
Many	of	the	edges,	both	on	the	cover	and	on	the	circuit	boards,	can	be	sharp,	so	take	care	when
handling	the	various	pieces.	Trying	to	jam	misaligned	cards	into	sockets	can	damage	both	the	card	and
the	motherboard,	so	be	careful	if	you	decide	to	add	a	new	card	or	remove	and	reinstall	an	existing	one.
The	next	 line	 in	 the	 ad,	 “128KB	 L1	 cache,	2MB	 L2	 cache”	 also	 describes	 a	 type	 of	 memory.	 In
Chapter	6,	you	will	learn	that	no	matter	how	fast	a	bus	is,	it	still	takes	“a	while”	to	get	data	from	memory
to	the	processor.	To	provide	even	faster	access	to	data,	many	systems	contain	a	special	memory	called
cache.	The	 system	 in	 our	 advertisement	 has	 two	 kinds	 of	cache.	 Level	 1	 cache	 (L1)	 is	 a	 small,	 fast
memory	cache	that	is	built	into	the	microprocessor	chip	and	helps	speed	up	access	to	frequently	used
data.	 Level	 2	 cache	 (L2)	 is	 a	 collection	 of	 fast,	 built-in	 memory	 chips	 situated	 between	 the
microprocessor	and	main	memory.	Notice	that	the	cache	in	our	system	has	a	capacity	of	kilobytes	(KB),
which	is	much	smaller	than	main	memory.	In	Chapter	6,	you	will	learn	how	cache	works,	and	that	a	bigger
cache	isn’t	always	better.
On	the	other	hand,	everyone	agrees	that	the	more	fixed	disk	capacity	you	have,	the	better	off	you	are.
The	advertised	system	has	a	1TB	hard	drive,	an	average	size	by	today’s	standards.	The	storage	capacity
of	a	fixed	(or	hard)	disk	is	not	the	only	thing	to	consider,	however.	A	large	disk	isn’t	very	helpful	if	it	is
too	slow	for	its	host	system.	The	computer	in	our	ad	has	a	hard	drive	that	rotates	at	7200	revolutions	per
minute	(RPM).	To	the	knowledgeable	reader,	this	indicates	(but	does	not	state	outright)	that	this	is	a	fairly
fast	drive.	Usually,	disk	speeds	are	stated	in	terms	of	the	number	of	milliseconds	required	(on	average)	to
access	data	on	the	disk,	in	addition	to	how	fast	the	disk	rotates.
Rotational	speed	is	only	one	of	the	determining	factors	in	the	overall	performance	of	a	disk.	The
manner	 in	 which	 it	 connects	 to—or	interfaces	 with—the	 rest	 of	 the	 system	 is	 also	 important.	 The
advertised	system	uses	a	SATA	(serial	advanced	technology	attachment	or	serial	ATA)	disk	interface.
This	is	an	evolutionary	storage	interface	that	has	replaced	IDE,	or	integrated	drive	electronics.	Another
common	interface	is	EIDE,	enhanced	integrated	drive	electronics,	a	cost-effective	hardware	interface
alternative	for	mass	storage	devices.	EIDE	contains	special	circuits	that	allow	it	to	enhance	a	computer’s
connectivity,	speed,	and	memory	capability.	Most	ATA,	IDE,	and	EIDE	systems	share	the	main	system	bus
with	the	processor	and	memory,	so	the	movement	of	data	to	and	from	the	disk	is	also	dependent	on	the
speed	of	the	system	bus.
Whereas	the	system	bus	is	responsible	for	all	data	movement	internal	to	the	computer,	ports	allow
movement	of	data	to	and	from	devices	external	to	the	computer.	Our	ad	speaks	of	two	different	ports	with
the	line,	“10	USB	ports,	1	serial	port.”	Serial	ports	transfer	data	by	sending	a	series	of	electrical	pulses
across	one	or	two	data	lines.	Another	type	of	port	some	computers	have	is	a	parallel	port.	Parallel	ports
use	at	least	eight	data	lines,	which	are	energized	simultaneously	to	transmit	data.	Many	new	computers	no
longer	come	with	serial	or	parallel	ports,	but	instead	have	only	USB	ports.	USB	(universal	serial	bus)	is
a	 popular	 external	 bus	 that	 supports	Plug-and-Play	 installation	 (the	 ability	 to	 configure	 devices
automatically)	as	well	as	hot	plugging	(the	ability	to	add	and	remove	devices	while	the	computer	is
running).
Expansion	slots	are	openings	on	the	motherboard	where	various	boards	can	be	plugged	in	to	add	new
capabilities	to	a	computer.	These	slots	can	be	used	for	such	things	as	additional	memory,	video	cards,
sound	cards,	network	cards,	and	modems.	Some	systems	augment	their	main	bus	with	dedicated	I/O	buses

using	these	expansion	slots.	Peripheral	Component	Interconnect	(PCI)	is	one	such	I/O	bus	standard	that
supports	the	connection	of	multiple	peripheral	devices.	PCI,	developed	by	the	Intel	Corporation,	operates
at	high	speeds	and	also	supports	Plug-and-Play.
PCI	is	an	older	standard	(it	has	been	around	since	1993)	and	was	superseded	by	PCI-x	in	2004.	PCI-x
basically	doubled	the	bandwidth	of	regular	PCI.	Both	PCI	and	PCI-x	are	parallel	in	operation.	In	2004,
PCI	 express	 (PCIe)	 replaced	 PCI-x.	 PCIe	 operates	 in	 serial	 and	 is	 currently	 the	 standard	 in	 today’s
computers.	In	the	ad,	we	see	the	computer	has	1	PCI	slot,	1	PCI	x	16	slot,	and	2	PCI	x	1	slots.	This
computer	 also	 has	 Bluetooth	 (a	 wireless	 technology	 allowing	 the	 transfer	 of	 information	 over	 short
distances)	and	an	HDMI	port	(High-Definition	Multimedia	Interface,	used	to	transmit	audio	and	video).
PCIe	has	not	only	superseded	PCI	and	PCI-x,	but	in	the	graphics	world,	it	has	also	progressively
replaced	the	AGP	(accelerated	graphics	port)	graphics	interface	designed	by	Intel	specifically	for	3D
graphics.	The	computer	in	our	ad	has	a	PCIe	video	card	with	1GB	of	memory.	The	memory	is	used	by	a
special	graphics	processing	unit	on	the	card.	This	processor	is	responsible	for	performing	the	necessary
calculations	to	render	the	graphics	so	the	main	processor	of	the	computer	is	not	required	to	do	so.	This
computer	also	has	a	PCIe	sound	card;	a	sound	card	contains	components	needed	by	the	system’s	stereo
speakers	and	microphone.
In	addition	to	telling	us	about	the	ports	and	expansion	slots	in	the	advertised	system,	the	ad	supplies	us
with	information	on	an	LCD	(liquid	crystal	display)	monitor,	or	“flat	panel”	display.	Monitors	have	little
to	do	with	the	speed	or	efficiency	of	a	computer	system,	but	they	have	great	bearing	on	the	comfort	of	the
user.	This	LCD	monitor	has	the	following	specifications:	24",	1920	×	1200	WUXGA,	300	cd/m
2
,	active
matrix,	1000:1	(static),	8ms,	24-bit	color	(16.7	million	colors),	VGA/DVI	input,	and	2USB	ports.	LCDs
use	a	liquid	crystal	material	sandwiched	between	two	pieces	of	polarized	glass.	Electric	currents	cause
the	crystals	to	move	around,	allowing	differing	levels	of	backlighting	to	pass	through,	creating	the	text,
colors,	 and	 pictures	 that	 appear	 on	 the	 screen.	 This	 is	 done	 by	 turning	 on/off	 different	pixels,	 small
“picture	elements”	or	dots	on	the	screen.	Monitors	typically	have	millions	of	pixels,	arranged	in	rows	and
columns.	This	monitor	has	1920	×	1200	(more	than	a	million)	pixels.
Most	 LCDs	 manufactured	 today	 utilize	 active	 matrix	 technology,	 Whereas	 passive	 technology	 is
reserved	for	smaller	devices	such	as	calculators	and	clocks.	Active	matrix	technology	uses	one	transistor
per	pixel;	passive	matrix	technology	uses	transistors	that	activate	entire	rows	and	columns.	Although
passive	technology	is	less	costly,	active	technology	renders	a	better	image	because	it	drives	each	pixel
independently.
The	LCD	monitor	in	the	ad	is	24",	measured	diagonally.	This	measurement	affects	the	aspect	ratio	of
the	monitor—the	ratio	of	horizontal	pixels	to	vertical	pixels	that	the	monitor	can	display.	Traditionally,
this	ratio	was	4:3,	but	newer	widescreen	monitors	use	ratios	of	16:10	or	16:9.	Ultra-wide	monitors	use	a
higher	ratio,	around	3:1	or	2:1.
When	discussing	resolution	and	LCDs,	it	is	important	to	note	that	LCDs	have	a	native	resolution;	this
means	 LCDs	 are	 designed	 for	 a	 specific	 resolution	 (generally	 given	 in	 horizontal	 pixels	 by	 vertical
pixels).	 Although	 you	 can	 change	 the	 resolution,	 the	 image	 quality	 typically	 suffers.	 Resolutions	 and
aspect	ratios	are	often	paired.	When	listing	resolutions	for	LCDs,	manufacturers	often	use	the	following
abbreviations:	 XGA	 (extended	 graphics	 array);	 XGA+	 (extended	 graphics	 array	 plus);	 SXGA	 (super
XGA);	 UXGA	 (ultra	 XGA);	 W	 prefix	 (wide);	 and	 WVA	 (wide	 viewing	 angle).	 The	 viewing	 angle
specifies	an	angle,	in	degrees,	that	indicates	at	which	angle	a	user	can	still	see	the	image	on	the	screen;
common	angles	range	from	120	to	170	degrees.	Some	examples	of	standard	4:3	native	resolutions	include
XGA	(1024	×	768),	SXGA	(1280	×	1024),	SXGA+	(1400	×	1050),	and	UXGA	(1600	×	1200).	Common
16:9	and	16:10	resolutions	include	WXGA	(1280	×	800),	WXGA+	(1440	×	900),	WSXGA+	(1680	×

1050),	and	WUXGA	(1920	×	1200).
LCD	monitor	specifications	often	list	a	response	time,	which	indicates	the	rate	at	which	the	pixels
can	change	colors.	If	this	rate	is	too	slow,	ghosting	and	blurring	can	occur.	The	LCD	monitor	in	the	ad	has
a	response	time	of	8ms.	Originally,	response	rates	measured	the	time	to	go	from	black	to	white	and	back
to	black.	Many	manufacturers	now	list	the	response	time	for	gray-to-gray	transitions	(which	is	generally
faster).	Because	they	typically	do	not	specify	which	transition	has	been	measured,	it	is	very	difficult	to
compare	monitors.	One	manufacturer	may	specify	a	response	time	of	2ms	for	a	monitor	(and	it	measures
gray-to-gray),	 while	 another	 manufacturer	 may	 specify	 a	 response	 rate	 of	 5ms	 for	 its	 monitor	 (and	 it
measures	black-to-white-to-black).	In	reality,	the	monitor	with	the	response	rate	of	5ms	may	actually	be
faster	overall.
Continuing	with	the	ad,	we	see	that	the	LCD	monitor	has	a	specification	of	300	cd/m
2
,	which	is	the
monitor’s	 luminance.	Luminance	 (or	 image	 brightness)	 is	 a	 measure	 of	 the	 amount	 of	 light	 an	 LCD
monitor	emits.	This	measure	is	typically	given	in	candelas	per	square	meter	(cd/m
2
).	When	purchasing	a
monitor,	the	 brightness	 level	 should	 be	 at	 least	 250	 (the	 higher	 the	 better);	 the	 average	 for	 computer
monitors	is	from	200	to	300	cd/m
2
.	Luminance	affects	how	easy	a	monitor	is	to	read,	particularly	in	low
light	situations.
Whereas	luminance	measures	the	brightness,	the	contrast	ratio	measures	the	difference	in	intensity
between	bright	whites	and	dark	blacks.	Contrast	ratios	can	be	static	(the	ratio	of	the	brightest	point	on	the
monitor	to	the	darkest	point	on	the	monitor	that	can	be	produced	at	a	given	instant	in	time)	or	dynamic	(the
ratio	of	the	darkest	point	in	one	image	to	the	lightest	point	in	another	image	produced	at	a	separate	point	in
time).	 Static	 specifications	 are	 typically	 preferred.	 A	 low	 static	 ratio	 (such	 as	 300:1)	 makes	 it	 more
difficult	to	discern	shades;	a	good	static	ratio	is	500:1	(with	ranges	from	400:1	to	3000:1).	The	monitor	in
the	ad	has	a	static	contrast	ratio	of	1000:1.	LCD	monitors	can	have	dynamic	ratios	of	12,000,000:1	and
higher,	but	a	higher	dynamic	number	does	not	necessarily	mean	the	monitor	is	better	than	a	monitor	with	a
much	lower	static	ratio.
The	next	specification	given	for	the	LCD	monitor	in	the	ad	is	its	color	depth.	This	number	reflects	the
number	of	colors	that	can	be	displayed	on	the	screen	at	one	time.	Common	depths	are	8-bit,	16-bit,	24-bit,
and	32-bit.	The	LCD	monitor	in	our	ad	can	display	2
24
,	or	roughly	16.7	million	colors.
LCD	 monitors	 also	 have	 many	 optional	 features.	 Some	 have	 integrated	 USB	 ports	 (as	 in	 this	 ad)
and/or	speakers.	Many	are	HDCP	(high	bandwidth	digital	content	protection)	compliant	(which	means
you	can	watch	HDCP-encrypted	materials,	such	as	Blu-ray	discs).	LCD	monitors	may	also	come	with
both	VGA	(video	graphics	array)	and	DVI	(digital	video	interface)	connections	(as	seen	in	the	ad).	VGA
sends	analog	signals	to	the	monitor	from	the	computer,	which	requires	digital-to-analog	conversion;	DVI
is	already	digital	in	format	and	requires	no	conversion,	resulting	in	a	cleaner	signal	and	crisper	image.
Although	 an	 LCD	 monitor	 typically	 provides	 better	 images	 using	 a	 DVI	 connection,	 having	 both
connectors	allows	one	to	use	an	LCD	with	existing	system	components.
Now	that	we	have	discussed	how	an	LCD	monitor	works	and	we	understand	the	concept	of	a	pixel,
let’s	go	back	and	discuss	graphics	cards	(also	called	video	cards)	in	more	detail.	With	millions	of	pixels
on	the	screen,	it	is	quite	challenging	to	determine	which	ones	should	be	off	and	which	ones	should	be	on
(and	in	what	color).	The	job	of	the	graphics	card	is	to	input	the	binary	data	from	your	computer	and
“translate”	 it	 into	 signals	 to	 control	 all	 pixels	 on	 the	 monitor;	 the	 graphics	 card	 therefore	 acts	 as	 a
“middleman”	between	the	computer’s	processor	and	monitor.	As	mentioned	previously,	some	computers
have	integrated	graphics,	which	means	the	computer’s	processor	is	responsible	for	doing	this	translation,
causing	a	large	workload	on	this	processor;	therefore,	many	computers	have	slots	for	graphics	cards,
allowing	the	processor	on	the	graphics	card	(called	a	graphics	processing	unit,	or	GPU)	to	perform	this

translation	instead.
The	GPU	is	no	ordinary	processor;	it	is	designed	to	most	efficiently	perform	the	complex	calculations
required	 for	 image	 rendering	 and	 contains	 special	 programs	 allowing	 it	 to	 perform	 this	 task	 more
effectively.	Graphics	cards	typically	contain	their	own	dedicated	RAM	used	to	hold	temporary	results	and
information,	including	the	location	and	color	for	each	pixel	on	the	screen.	A	frame	buffer	(part	of	this
RAM)	is	used	to	store	rendered	images	until	these	images	are	intended	to	be	displayed.	The	memory	on	a
graphics	card	connects	to	a	digital-to-analog	converter	(DAC),	a	device	that	converts	a	binary	image	to
analog	signals	that	a	monitor	can	understand	and	sends	them	via	a	cable	to	the	monitor.	Most	graphics
cards	today	have	two	types	of	monitor	connections:	DVI	for	LCD	screens	and	VGA	for	the	older	CRT
(cathode	ray	tube)	screens.
Most	graphics	cards	are	plugged	into	slots	in	computer	motherboards,	so	are	thus	powered	by	the
computers	themselves.	However,	some	are	very	powerful	and	actually	require	a	connection	directly	to	a
computer’s	power	supply.	These	high-end	graphics	cards	are	typically	found	in	computers	that	deal	with
image-intensive	applications,	such	as	video	editing	and	high-end	gaming.
Continuing	with	the	ad,	we	see	that	the	advertised	system	has	a	16x	DVD	+/–	RW	drive.	This	means
we	can	read	and	write	to	DVDs	and	CDs.	“16x”	is	a	measure	of	the	drive	speed	and	measures	how
quickly	the	drive	can	read	and	write.	DVDs	and	CDs	are	discussed	in	more	detail	in	Chapter	7.
Computers	are	more	useful	if	they	can	communicate	with	the	outside	world.	One	way	to	communicate
is	to	employ	an	Internet	service	provider	and	a	modem.	There	is	no	mention	of	a	modem	for	the	computer
in	 our	 ad,	 as	 many	 desktop	 owners	 use	 external	 modems	 provided	 by	 their	 Internet	 service	 provider
(phone	modem,	cable	modem,	satellite	modem,	etc).	However,	both	USB	and	PCI	modems	are	available
that	allow	you	to	connect	your	computer	to	the	Internet	using	the	phone	line;	many	of	these	also	allow	you
to	use	your	computer	as	a	fax	machine.	I/O	and	I/O	buses	in	general	are	discussed	in	Chapter	7.
A	computer	can	also	connect	directly	to	a	network.	Networking	allows	computers	to	share	files	and
peripheral	devices.	Computers	can	connect	to	a	network	via	either	a	wired	or	a	wireless	technology.
Wired	computers	use	Ethernet	technology,	an	international	standard	networking	technology	for	wired
networks,	and	there	are	two	options	for	the	connection.	The	first	is	to	use	a	network	interface	card
(NIC),	which	connects	to	the	motherboard	via	a	PCI	slot.	NICs	typically	support	10/100	Ethernet	(both
Ethernet	at	a	speed	of	10Mbps	and	fast	Ethernet	at	a	speed	of	100Mbps)	or	10/100/1000	(which	adds
Ethernet	at	1,000Mbps).	Another	option	for	wired	network	capability	is	integrated	Ethernet,	which	means
that	the	motherboard	itself	contains	all	necessary	components	to	support	10/100	Ethernet;	thus	no	PCI	slot
is	required.	Wireless	networking	has	the	same	two	options.	Wireless	NICs	are	available	from	a	multitude
of	vendors	and	are	available	for	both	desktops	and	laptops.	For	installation	in	desktop	machines,	you
need	 an	 internal	 card	 that	 will	 most	 likely	 have	 a	 small	 antenna.	 Laptops	 usually	 use	 an	 expansion
(PCMCIA)	slot	for	the	wireless	network	card,	and	vendors	have	started	to	integrate	the	antenna	into	the
back	of	the	case	behind	the	screen.	Integrated	wireless	(such	as	that	found	in	the	Intel	Centrino	mobile
technology)	eliminates	the	hassle	of	cables	and	cards.	The	system	in	our	ad	employs	integrated	Ethernet.
Note	 that	 many	 new	 computers	 may	 have	 integrated	 graphics	 and/or	 integrated	 sound	 in	 addition	 to
integrated	Ethernet.
Although	we	cannot	delve	into	all	of	the	brand-specific	components	available,	after	completing	this
text,	you	should	understand	the	concept	of	how	most	computer	systems	operate.	This	understanding	is
important	for	casual	users	as	well	as	experienced	programmers.	As	a	user,	you	need	to	be	aware	of	the
strengths	and	limitations	of	your	computer	system	so	you	can	make	informed	decisions	about	applications
and	thus	use	your	system	more	effectively.	As	a	programmer,	you	need	to	understand	exactly	how	your
system	hardware	functions	so	you	can	write	effective	and	efficient	programs.	For	example,	something	as

simple	 as	 the	 algorithm	 your	 hardware	 uses	 to	 map	 main	 memory	 to	 cache	 and	 the	 method	 used	 for
memory	 interleaving	 can	 have	 a	 tremendous	 effect	 on	 your	 decision	 to	 access	 array	 elements	 in	 row
versus	column-major	order.
Throughout	 this	 text,	 we	 investigate	 both	 large	 and	 small	 computers.	 Large	 computers	 include
mainframes,	enterprise-class	servers,	and	supercomputers.	Small	computers	include	personal	systems,
workstations,	 and	 handheld	 devices.	 We	 will	 show	 that	 regardless	 of	 whether	 they	 carry	 out	 routine
chores	or	perform	sophisticated	scientific	tasks,	the	components	of	these	systems	are	very	similar.	We
also	visit	some	architectures	that	lie	outside	what	is	now	the	mainstream	of	computing.	We	hope	that	the
knowledge	you	gain	from	this	text	will	ultimately	serve	as	a	springboard	for	your	continuing	studies	the
vast	and	exciting	fields	of	computer	organization	and	architecture.
Tablet	Computers
Ken	 Olsen,	 the	 founder	 of	 Digital	 Equipment	 Corporation,	 has	 been	 unfairly	 ridiculed	 for	 saying
“There	is	no	reason	for	any	individual	to	have	a	computer	in	his	home.”	He	made	this	statement	in
1977	 when	 the	 word,	computer,	 evoked	 a	 vision	 of	 the	 type	 of	 machine	 made	 by	 his	 company:
refrigerator-sized	behemoths	that	cost	a	fortune	and	required	highly	skilled	personnel	to	operate.	One
might	safely	say	that	no	one—except	perhaps	a	computer	engineer—ever	had	such	a	machine	in	his	or
her	home.
As	already	discussed,	the	“personal	computing”	wave	that	began	in	the	1980s	erupted	in	the	1990s
with	the	establishment	of	the	World	Wide	Web.	By	2010,	decennial	census	data	reported	that	68%	of
U.S.	households	claimed	to	have	a	personal	computer.	There	is,	however,	some	evidence	that	this	trend
has	peaked	and	is	now	in	decline,	owing	principally	to	the	widespread	use	of	smartphones	and	tablet
computers.	According	to	some	estimates,	as	many	as	65%	of	Internet	users	in	the	United	States	connect
exclusively	via	mobile	platforms.	The	key	to	this	trend	is	certainly	the	enchanting	usability	of	these
devices.
We	hardly	need	the	power	of	a	desktop	computer	to	surf	the	Web,	read	email,	or	listen	to	music.
Much	more	economical	and	lightweight,	tablet	computers	give	us	exactly	what	we	need	in	an	easy-to-
use	 package.	 With	 its	 booklike	 form,	 one	 is	 tempted	 to	 claim	 that	 a	 tablet	 constitutes	 the	 perfect
“portable	computer.”
The	 figure	 on	 the	 next	 page	 shows	 a	 disassembled	 Pandigital	 Novel	 tablet	 computer.	 We	 have
labeled	several	items	common	to	all	tablets.	The	mini	USB	port	provides	access	to	internal	storage
and	the	removable	SD	card.	Nearly	all	tablets	provide	Wi-Fi	connection	to	the	Internet,	with	some	also
supporting	2G,	3G,	and	4G	cellular	protocols.	Battery	life	can	be	as	much	as	14	hours	for	the	most
efficient	high-end	tablet	computers.	Unlike	the	Pandigital,	most	tablets	include	at	least	one	camera	for
still	photography	and	live	video.

A	Disassembled	Tablet	Computer
Courtesy	of	Julia	Lobur.
A	touchscreen	dominates	the	real	estate	of	all	portable	devices.	For	consumer	tablets	and	phones,
touchscreens	come	in	two	general	types:	resistive	and	capacitive.	Resistive	touchscreens	respond	to
the	pressure	of	a	finger	or	a	stylus.	Capacitive	touchscreens	react	to	the	electrical	properties	of	the
human	 skin.	 Resistive	 screens	 are	 less	 sensitive	 than	 capacitive	 screens,	 but	 they	 provide	 higher
resolution.	 Unlike	 resistive	 screens,	 capacitive	 screens	 support	 multitouch,	 which	 is	 the	 ability	 to
detect	the	simultaneous	press	of	two	or	more	fingers.
Military	and	medical	computer	touchscreens	are	necessarily	more	durable	than	those	intended	for
the	consumer	market.	Two	different	technologies,	surface	acoustic	wave	touch	sense	and	infrared
touch	sense,	 respectively,	 send	 ultrasonic	 and	 infrared	 waves	 across	 the	 surface	 of	 a	 ruggedized
touchscreen.	The	matrix	of	waves	is	broken	when	a	finger	comes	in	contact	with	the	surface	of	the
screen.
Because	of	its	high	efficiency,	cell	phone	CPU	technology	has	been	adapted	for	use	in	the	tablet
platform.	The	mobile	computing	space	has	been	dominated	by	ARM	chips,	although	Intel	and	AMD
have	been	gaining	market	share.	Operating	systems	for	these	devices	include	variants	of	Android	by
Google	 and	 iOS	 by	 Apple.	 Microsoft’s	 Surface	 tablets	 running	 Windows	 8	 provide	 access	 to	 the
Microsoft	Office	suite	of	products.
As	tablet	computers	continue	to	replace	desktop	systems,	they	will	also	find	uses	in	places	where
traditional	computers—even	laptops—are	impractical.	Thousands	of	free	and	inexpensive	applications
are	 available	 for	 all	 platforms,	 thereby	 increasing	 demand	 even	 further.	 Educational	 applications
abound.	With	a	size,	shape,	and	weight	similar	to	a	paperback	book,	tablet	computers	are	replacing
paper	 textbooks	 in	 some	 U.S.	 school	 districts.	 Thus,	 the	 elusive	 dream	 of	 “a	 computer	 for	 every
student”	is	finally	coming	true—thanks	to	the	tablet.	By	1985,	people	were	already	laughing	at	Olsen’s
“home	computer”	assertion.	Would	perhaps	these	same	people	have	scoffed	if	instead	he	would	have
predicted	a	computer	in	every	backpack?
1.4			STANDARDS	ORGANIZATIONS
Suppose	you	decide	you’d	like	to	have	one	of	those	nifty	new	LCD	widescreen	monitors.	You	figure	you

can	shop	around	a	bit	to	find	the	best	price.	You	make	a	few	phone	calls,	surf	the	Web,	and	drive	around
town	until	you	find	the	one	that	gives	you	the	most	for	your	money.	From	your	experience,	you	know	you
can	 buy	 your	 monitor	 anywhere	 and	 it	 will	 probably	 work	 fine	 on	 your	 system.	 You	 can	 make	 this
assumption	 because	 computer	 equipment	 manufacturers	 have	 agreed	 to	 comply	 with	 connectivity	 and
operational	specifications	established	by	a	number	of	government	and	industry	organizations.
Some	of	these	standards-setting	organizations	are	ad	hoc	trade	associations	or	consortia	made	up	of
industry	leaders.	Manufacturers	know	that	by	establishing	common	guidelines	for	a	particular	type	of
equipment,	they	can	market	their	products	to	a	wider	audience	than	if	they	came	up	with	separate—and
perhaps	incompatible—specifications.
Some	standards	organizations	have	formal	charters	and	are	recognized	internationally	as	the	definitive
authority	 in	 certain	 areas	 of	 electronics	 and	 computers.	 As	 you	 continue	 your	 studies	 in	 computer
organization	and	architecture,	you	will	encounter	specifications	formulated	by	these	groups,	so	you	should
know	something	about	them.
The	Institute	of	Electrical	and	Electronics	Engineers	(IEEE)	is	an	organization	dedicated	to	the
advancement	of	the	professions	of	electronic	and	computer	engineering.	The	IEEE	actively	promotes	the
interests	of	the	worldwide	engineering	community	by	publishing	an	array	of	technical	literature.	The	IEEE
also	 sets	 standards	 for	 various	 computer	 components,	 signaling	 protocols,	 and	 data	 representation,	 to
name	 only	 a	 few	 areas	 of	 its	 involvement.	 The	 IEEE	 has	 a	 democratic,	 albeit	 convoluted,	 procedure
established	for	the	creation	of	new	standards.	Its	final	documents	are	well	respected	and	usually	endure
for	several	years	before	requiring	revision.
The	International	Telecommunications	Union	(ITU)	is	based	in	Geneva,	Switzerland.	The	ITU	was
formerly	 known	 as	 the	Comité	 Consultatif	 International	 Télégraphique	 et	 Téléphonique,	 or	 the
International	 Consultative	 Committee	 on	 Telephony	 and	 Telegraphy.	 As	 its	 name	 implies,	 the	 ITU
concerns	itself	with	the	interoperability	of	telecommunications	systems,	including	telephone,	telegraph,
and	data	communication	systems.	The	telecommunications	arm	of	the	ITU,	the	ITU-T,	has	established	a
number	of	standards	that	you	will	encounter	in	the	literature.	You	will	see	these	standards	prefixed	by
ITU-T	or	the	group’s	former	initials,	CCITT.
Many	countries,	including	the	European	Community,	have	commissioned	umbrella	organizations	to
represent	their	interests	in	various	international	groups.	The	group	representing	the	United	States	is	the
American	National	Standards	Institute	(ANSI).	 Great	 Britain	 has	 its	British	Standards	Institution
(BSI)	in	addition	to	having	a	voice	on	the	CEN	(Comité	Européen	de	Normalisation),	the	European
committee	for	standardization.
The	International	Organization	for	Standardization	(ISO)	is	the	entity	that	coordinates	worldwide
standards	development,	including	the	activities	of	ANSI	with	BSI,	among	others.	ISO	is	not	an	acronym,
but	derives	from	the	Greek	word,	isos,	meaning	“equal.”	The	ISO	consists	of	more	than	2800	technical
committees,	each	of	which	is	charged	with	some	global	standardization	issue.	Its	interests	range	from	the
behavior	 of	 photographic	 film	 to	 the	 pitch	 of	 screw	 threads	 to	 the	 complex	 world	 of	 computer
engineering.	The	proliferation	of	global	trade	has	been	facilitated	by	the	ISO.	Today,	the	ISO	touches
virtually	every	aspect	of	our	lives.
Throughout	 this	 text,	 we	 mention	 official	 standards	 designations	 where	 appropriate.	 Definitive
information	concerning	many	of	these	standards	can	be	found	in	excruciating	detail	on	the	website	of	the
organization	responsible	for	establishing	the	standard	cited.	As	an	added	bonus,	many	standards	contain
“normative”	and	informative	references,	which	provide	background	information	in	areas	related	to	the
standard.

1.5			HISTORICAL	DEVELOPMENT
During	 their	 60-year	 life	 span,	 computers	 have	 become	 the	 perfect	 example	 of	 modern	 convenience.
Living	memory	is	strained	to	recall	the	days	of	steno	pools,	carbon	paper,	and	mimeograph	machines.	It
sometimes	seems	that	these	magical	computing	machines	were	developed	instantaneously	in	the	form	that
we	 now	 know	 them.	 But	 the	 developmental	 path	 of	 computers	 is	 paved	 with	 accidental	 discovery,
commercial	coercion,	and	whimsical	fancy.	And	occasionally	computers	have	even	improved	through	the
application	of	 solid	engineering	 practices!	 Despite	all	 the	twists,	 turns,	 and	technological	 dead	 ends,
computers	have	evolved	at	a	pace	that	defies	comprehension.	We	can	fully	appreciate	where	we	are	today
only	when	we	have	seen	where	we’ve	come	from.
In	the	sections	that	follow,	we	divide	the	evolution	of	computers	into	generations,	each	generation
being	defined	by	the	technology	used	to	build	the	machine.	We	have	provided	approximate	dates	for	each
generation	 for	 reference	 purposes	 only.	 You	 will	 find	 little	 agreement	 among	 experts	 as	 to	 the	 exact
starting	and	ending	times	of	each	technological	epoch.
Every	invention	reflects	the	time	in	which	it	was	made,	so	one	might	wonder	whether	it	would	have
been	called	a	computer	if	it	had	been	invented	in	the	late	1990s.	How	much	computation	do	we	actually
see	pouring	from	the	mysterious	boxes	perched	on	or	beside	our	desks?	Until	recently,	computers	served
us	only	by	performing	mind-bending	mathematical	manipulations.	No	longer	limited	to	white-jacketed
scientists,	today’s	computers	help	us	to	write	documents,	keep	in	touch	with	loved	ones	across	the	globe,
and	 do	 our	 shopping	 chores.	 Modern	 business	 computers	 spend	 only	 a	 minuscule	 part	 of	 their	 time
performing	accounting	calculations.	Their	main	purpose	is	to	provide	users	with	a	bounty	of	strategic
information	 for	 competitive	 advantage.	 Has	 the	 word	computer	 now	 become	 a	 misnomer?	 An
anachronism?	What,	then,	should	we	call	them,	if	not	computers?
We	cannot	present	the	complete	history	of	computing	in	a	few	pages.	Entire	texts	have	been	written	on
this	subject	and	even	they	leave	their	readers	wanting	more	detail.	If	we	have	piqued	your	interest,	we
refer	you	to	some	of	the	books	cited	in	the	list	of	references	at	the	end	of	this	chapter.
1.5.1		Generation	Zero:	Mechanical	Calculating	Machines	(1642–
1945)
Prior	to	the	1500s,	a	typical	European	businessperson	used	an	abacus	for	calculations	and	recorded	the
result	of	his	ciphering	in	Roman	numerals.	After	the	decimal	numbering	system	finally	replaced	Roman
numerals,	 a	 number	 of	 people	 invented	 devices	 to	 make	 decimal	 calculations	 even	 faster	 and	 more
accurate.	Wilhelm	Schickard	(1592–1635)	has	been	credited	with	the	invention	of	the	first	mechanical
calculator,	the	Calculating	Clock	(exact	date	unknown).	This	device	was	able	to	add	and	subtract	numbers
containing	as	many	as	six	digits.	In	1642,	Blaise	Pascal	(1623–1662)	developed	a	mechanical	calculator
called	the	Pascaline	to	help	his	father	with	his	tax	work.	The	Pascaline	could	do	addition	with	carry	and
subtraction.	It	was	probably	the	first	mechanical	adding	device	actually	used	for	a	practical	purpose.	In
fact,	the	Pascaline	was	so	well	conceived	that	its	basic	design	was	still	being	used	at	the	beginning	of	the
twentieth	century,	as	evidenced	by	the	Lightning	Portable	Adder	in	1908	and	the	Addometer	in	1920.
Gottfried	Wilhelm	von	Leibniz	(1646–1716),	a	noted	mathematician,	invented	a	calculator	known	as	the
Stepped	 Reckoner	 that	 could	 add,	 subtract,	 multiply,	 and	 divide.	 None	 of	 these	 devices	 could	 be
programmed	or	had	memory.	They	required	manual	intervention	throughout	each	step	of	their	calculations.
Although	machines	like	the	Pascaline	were	used	into	the	twentieth	century,	new	calculator	designs
began	 to	 emerge	 in	 the	 nineteenth	 century.	 One	 of	 the	 most	 ambitious	 of	 these	 new	 designs	 was	 the

Difference	Engine	by	Charles	Babbage	(1791–1871).	Some	people	refer	to	Babbage	as	“the	father	of
computing.”	By	all	accounts,	he	was	an	eccentric	genius	who	brought	us,	among	other	things,	the	skeleton
key	and	the	“cow	catcher,”	a	device	intended	to	push	cows	and	other	movable	obstructions	out	of	the	way
of	locomotives.
Babbage	built	his	Difference	Engine	in	1822.	The	Difference	Engine	got	its	name	because	it	used	a
calculating	technique	called	the	method	of	differences.	 The	 machine	 was	 designed	 to	 mechanize	 the
solution	of	polynomial	functions	and	was	actually	a	calculator,	not	a	computer.	Babbage	also	designed	a
general-purpose	machine	in	1833	called	the	Analytical	Engine.	Although	Babbage	died	before	he	could
build	it,	the	Analytical	Engine	was	designed	to	be	more	versatile	than	his	earlier	Difference	Engine.	The
Analytical	Engine	would	have	been	capable	of	performing	any	mathematical	operation.	The	Analytical
Engine	included	many	of	the	components	associated	with	modern	computers:	an	arithmetic	processing	unit
to	perform	calculations	(Babbage	referred	to	this	as	the	mill),	a	memory	(the	store),	and	input	and	output
devices.	 Babbage	 also	 included	 a	 conditional	 branching	 operation	 where	 the	 next	 instruction	 to	 be
performed	 was	 determined	 by	 the	 result	 of	 the	 previous	 operation.	 Ada,	 Countess	 of	 Lovelace	 and
daughter	of	poet	Lord	Byron,	suggested	that	Babbage	write	a	plan	for	how	the	machine	would	calculate
numbers.	This	is	regarded	as	the	first	computer	program,	and	Ada	is	considered	to	be	the	first	computer
programmer.	It	is	also	rumored	that	she	suggested	the	use	of	the	binary	number	system	rather	than	the
decimal	number	system	to	store	data.
A	perennial	problem	facing	machine	designers	has	been	how	to	get	data	into	the	machine.	Babbage
designed	the	Analytical	Engine	to	use	a	type	of	punched	card	for	input	and	programming.	Using	cards	to
control	the	behavior	of	a	machine	did	not	originate	with	Babbage,	but	with	one	of	his	friends,	Joseph-
Marie	 Jacquard	 (1752–1834).	 In	 1801,	 Jacquard	 invented	 a	 programmable	 weaving	 loom	 that	 could
produce	intricate	patterns	in	cloth.	Jacquard	gave	Babbage	a	tapestry	that	had	been	woven	on	this	loom
using	 more	 than	 10,000	 punched	 cards.	 To	 Babbage,	 it	 seemed	 only	 natural	 that	 if	 a	 loom	 could	 be
controlled	by	cards,	then	his	Analytical	Engine	could	be	as	well.	Ada	expressed	her	delight	with	this
idea,	writing,	“[T]he	Analytical	Engine	weaves	algebraical	patterns	just	as	the	Jacquard	loom	weaves
flowers	and	leaves.”
The	punched	card	proved	to	be	the	most	enduring	means	of	providing	input	to	a	computer	system.
Keyed	data	input	had	to	wait	until	fundamental	changes	were	made	in	how	calculating	machines	were
constructed.	In	the	latter	half	of	the	nineteenth	century,	most	machines	used	wheeled	mechanisms,	which
were	difficult	to	integrate	with	early	keyboards	because	they	were	levered	devices.	But	levered	devices
could	easily	punch	cards	and	wheeled	devices	could	easily	read	them.	So	a	number	of	devices	were
invented	 to	 encode	 and	 then	 “tabulate”	 card-punched	 data.	 The	 most	 important	 of	 the	 late-nineteenth-
century	tabulating	machines	was	the	one	invented	by	Herman	Hollerith	(1860–1929).	Hollerith’s	machine
was	used	for	encoding	and	compiling	1890	census	data.	This	census	was	completed	in	record	time,	thus
boosting	Hollerith’s	finances	and	the	reputation	of	his	invention.	Hollerith	later	founded	the	company	that
would	become	IBM.	His	80-column	punched	card,	the	Hollerith	card,	was	a	staple	of	automated	data
processing	for	more	than	50	years.
A	Pre-Modern	“Computer”	Hoax
The	 latter	 half	 of	 the	 sixteenth	 century	 saw	 the	 beginnings	 of	 the	 first	 Industrial	 Revolution.	 The
spinning	jenny	allowed	one	textile	worker	to	do	the	work	of	twenty,	and	steam	engines	had	power

equivalent	to	hundreds	of	horses.	Thus	began	our	enduring	fascination	with	all	things	mechanical.	With
the	right	skills	applied	to	the	problems	at	hand,	there	seemed	no	limits	to	what	humankind	could	do
with	its	machines!
Elaborate	 clocks	 began	 appearing	 at	 the	 beginning	 of	 the	 1700s.	 Complex	 and	 ornate	 models
graced	cathedrals	and	town	halls.	These	clockworks	eventually	morphed	into	mechanical	robots	called
automata.	Typical	models	played	musical	instruments	such	as	flutes	and	keyboard	instruments.	In	the
mid-1700s,	the	most	sublime	of	these	devices	entertained	royal	families	across	Europe.	Some	relied
on	trickery	to	entertain	their	audiences.	It	soon	became	something	of	a	sport	to	unravel	the	chicanery.
Empress	Marie-Therese	of	the	Austria-Hungarian	Empire	relied	on	a	wealthy	courtier	and	tinkerer,
Wolfgang	von	Kempelen,	to	debunk	the	spectacles	on	her	behalf.	One	day,	following	a	particularly
impressive	display,	Marie-Therese	challenged	von	Kempelen	to	build	an	automaton	to	surpass	all	that
had	ever	been	brought	to	her	court.
von	Kempelen	took	the	challenge,	and	after	several	months’	work,	he	delivered	a	turban-wearing,
pipe-smoking,	chess-playing	automaton.	For	all	appearances,	“The	Turk”	was	a	formidable	opponent
for	even	the	best	players	of	the	day.	As	an	added	touch,	the	machine	contained	a	set	of	baffles	enabling
it	to	rasp	“Échec!”	as	needed.	So	impressive	was	this	machine	that	for	84	years	it	drew	crowds	across
Europe	and	the	United	States.
Of	course,	as	with	all	similar	automata,	von	Kempelen’s	Turk	relied	on	trickery	to	perform	its
prodigious	feat.	Despite	some	astute	debunkers	correctly	deducing	how	it	was	done,	the	secret	of	the
Turk	was	never	divulged:	A	human	chess	player	was	cleverly	concealed	inside	its	cabinet.	The	Turk
thus	pulled	off	one	of	the	first	and	most	impressive	“computer”	hoaxes	in	the	history	of	technology.	It
would	take	another	200	years	before	a	real	machine	could	match	the	Turk—without	the	trickery.

The	mechanical	Turk
Reprinted	from	Robert	Willis,	An	attempt	to	Analyse	the	Automaton	Chess	Player	of	Mr.	de
Kempelen.	JK	Booth,	London.	1824.
1.5.2		The	First	Generation:	Vacuum	Tube	Computers	(1945–1953)
Although	Babbage	is	often	called	the	“father	of	computing,”	his	machines	were	mechanical,	not	electrical
or	 electronic.	 In	 the	 1930s,	 Konrad	 Zuse	 (1910–1995)	 picked	 up	 where	 Babbage	 left	 off,	 adding
electrical	 technology	 and	 other	 improvements	 to	 Babbage’s	 design.	 Zuse’s	 computer,	 the	 Z1,	 used
electromechanical	relays	instead	of	Babbage’s	hand-cranked	gears.	The	Z1	was	programmable	and	had	a
memory,	an	arithmetic	unit,	and	a	control	unit.	Because	money	and	resources	were	scarce	in	wartime
Germany,	Zuse	used	discarded	movie	film	instead	of	punched	cards	for	input.	Although	his	machine	was
designed	to	use	vacuum	tubes,	Zuse,	who	was	building	his	machine	on	his	own,	could	not	afford	the	tubes.
Thus,	the	Z1	correctly	belongs	in	the	first	generation,	although	it	had	no	tubes.
Zuse	built	the	Z1	in	his	parents’	Berlin	living	room	while	Germany	was	at	war	with	most	of	Europe.
Fortunately,	he	couldn’t	convince	the	Nazis	to	buy	his	machine.	They	did	not	realize	the	tactical	advantage
such	a	device	would	give	them.	Allied	bombs	destroyed	all	three	of	Zuse’s	first	systems,	the	Z1,	Z2,	and
Z3.	 Zuse’s	 impressive	 machines	 could	 not	 be	 refined	 until	 after	 the	 war	 and	 ended	 up	 being	 another

“evolutionary	dead	end”	in	the	history	of	computers.
Digital	computers,	as	we	know	them	today,	are	the	outcome	of	work	done	by	a	number	of	people	in
the	1930s	and	1940s.	Pascal’s	basic	mechanical	calculator	was	designed	and	modified	simultaneously	by
many	people;	the	same	can	be	said	of	the	modern	electronic	computer.	Notwithstanding	the	continual
arguments	 about	 who	 was	 first	 with	 what,	 three	 people	 clearly	 stand	 out	 as	 the	 inventors	 of	 modern
computers:	John	Atanasoff,	John	Mauchly,	and	J.	Presper	Eckert.
John	Atanasoff	(1904–1995)	has	been	credited	with	the	construction	of	the	first	completely	electronic
computer.	The	Atanasoff	Berry	Computer	(ABC)	was	a	binary	machine	built	from	vacuum	tubes.	Because
this	system	was	built	specifically	to	solve	systems	of	linear	equations,	we	cannot	call	it	a	general-purpose
computer.	There	were,	however,	some	features	that	the	ABC	had	in	common	with	the	general-purpose
ENIAC	(Electronic	Numerical	Integrator	and	Computer),	which	was	invented	a	few	years	later.	These
common	features	caused	considerable	controversy	as	to	who	should	be	given	the	credit	(and	patent	rights)
for	the	invention	of	the	electronic	digital	computer.	(The	interested	reader	can	find	more	details	on	a
rather	lengthy	lawsuit	involving	Atanasoff	and	the	ABC	in	Mollenhoff	[1988].)
John	Mauchly	(1907–1980)	and	J.	Presper	Eckert	(1929–1995)	were	the	two	principal	inventors	of
the	 ENIAC,	 introduced	 to	 the	 public	 in	 1946.	 The	 ENIAC	 is	 recognized	 as	 the	 first	 all-electronic,
general-purpose	digital	computer.	This	machine	used	17,468	vacuum	tubes,	occupied	1800	square	feet	of
floor	space,	weighed	30	tons,	and	consumed	174	kilowatts	of	power.	The	ENIAC	had	a	memory	capacity
of	about	1000	information	bits	(about	20	10-digit	decimal	numbers)	and	used	punched	cards	to	store	data.
John	Mauchly’s	vision	for	an	electronic	calculating	machine	was	born	from	his	lifelong	interest	in
predicting	the	weather	mathematically.	While	a	professor	of	physics	at	Ursinus	College	near	Philadelphia,
Mauchly	 engaged	 dozens	 of	 adding	 machines	 and	 student	 operators	 to	 crunch	 mounds	 of	 data	 that	 he
believed	would	reveal	mathematical	relationships	behind	weather	patterns.	He	felt	that	if	he	could	have
only	 a	 little	 more	 computational	 power,	 he	 could	 reach	 the	 goal	 that	 seemed	 just	 beyond	 his	 grasp.
Pursuant	 to	 the	 Allied	 war	 effort,	 and	 with	 ulterior	 motives	 to	 learn	 about	 electronic	 computation,
Mauchly	 volunteered	 for	 a	 crash	 course	 in	 electrical	 engineering	 at	 the	 University	 of	 Pennsylvania’s
Moore	School	of	Engineering.	Upon	completion	of	this	program,	Mauchly	accepted	a	teaching	position	at
the	Moore	School,	where	he	taught	a	brilliant	young	student,	J.	Presper	Eckert.	Mauchly	and	Eckert	found
a	mutual	interest	in	building	an	electronic	calculating	device.	In	order	to	secure	the	funding	they	needed	to
build	their	machine,	they	wrote	a	formal	proposal	for	review	by	the	school.	They	portrayed	their	machine
as	conservatively	as	they	could,	billing	it	as	an	“automatic	calculator.”	Although	they	probably	knew	that
computers	would	be	able	to	function	most	efficiently	using	the	binary	numbering	system,	Mauchly	and
Eckert	designed	their	system	to	use	base	10	numbers,	in	keeping	with	the	appearance	of	building	a	huge
electronic	adding	machine.	The	university	rejected	Mauchly	and	Eckert’s	proposal.	Fortunately,	the	U.S.
Army	was	more	interested.

U.S.	Army,	1946.
During	 World	 War	 II,	 the	 army	 had	 an	 insatiable	 need	 for	 calculating	 the	 trajectories	 of	 its	 new

ballistic	armaments.	Thousands	of	human	“computers”	were	engaged	around	the	clock	cranking	through
the	arithmetic	required	for	these	firing	tables.	Realizing	that	an	electronic	device	could	shorten	ballistic
table	calculation	from	days	to	minutes,	the	army	funded	the	ENIAC.	And	the	ENIAC	did	indeed	shorten
the	time	to	calculate	a	table	from	20	hours	to	30	seconds.	Unfortunately,	the	machine	wasn’t	ready	before
the	end	of	the	war.	But	the	ENIAC	had	shown	that	vacuum	tube	computers	were	fast	and	feasible.	During
the	next	decade,	vacuum	tube	systems	continued	to	improve	and	were	commercially	successful.
What	Is	a	Vacuum	Tube?
The	wired	world	that	we	know	today	was	born	from	the	invention	of	a	single	electronic	device	called
a	vacuum	tube	by	Americans	and—more	accurately—a	valve	by	the	British.	Vacuum	tubes	should	be
called	valves	because	they	control	the	flow	of	electrons	in	electrical	systems	in	much	the	same	way	as
valves	control	the	flow	of	water	in	a	plumbing	system.	In	fact,	some	mid-twentieth-century	breeds	of
these	electron	tubes	contain	no	vacuum	at	all,	but	are	filled	with	conductive	gases,	such	as	mercury
vapor,	which	can	provide	desirable	electrical	behavior.
The	electrical	phenomenon	that	makes	tubes	work	was	discovered	by	Thomas	A.	Edison	in	1883
while	 he	 was	 trying	 to	 find	 ways	 to	 keep	 the	 filaments	 of	 his	 light	 bulbs	 from	 burning	 away	 (or
oxidizing)	a	few	minutes	after	electrical	current	was	applied.	Edison	reasoned	correctly	that	one	way
to	prevent	filament	oxidation	would	be	to	place	the	filament	in	a	vacuum.	Edison	didn’t	immediately
understand	that	air	not	only	supports	combustion,	but	also	is	a	good	insulator.	When	he	energized	the
electrodes	holding	a	new	tungsten	filament,	the	filament	soon	became	hot	and	burned	out	as	the	others
had	before	it.	This	time,	however,	Edison	noticed	that	electricity	continued	to	flow	from	the	warmed
negative	terminal	to	the	cool	positive	terminal	within	the	light	bulb.	In	1911,	Owen	Willans	Richardson
analyzed	this	behavior.	He	concluded	that	when	a	negatively	charged	filament	was	heated,	electrons
“boiled	 off”	 as	 water	 molecules	 can	 be	 boiled	 to	 create	 steam.	 He	 aptly	 named	 this	 phenomenon
thermionic	emission.
Thermionic	emission,	as	Edison	had	documented	it,	was	thought	by	many	to	be	only	an	electrical
curiosity.	But	in	1905,	a	British	former	assistant	to	Edison,	John	A.	Fleming,	saw	Edison’s	discovery

as	much	more	than	a	novelty.	He	knew	that	thermionic	emission	supported	the	flow	of	electrons	in	only
one	direction:	from	the	negatively	charged	cathode	to	the	positively	charged	anode,	also	called	a
plate.	 He	 realized	 that	 this	 behavior	 could	rectify	 alternating	 current.	 That	 is,	 it	 could	 change
alternating	 current	 into	 the	 direct	 current	 that	 was	 essential	 for	 the	 proper	 operation	 of	 telegraph
equipment.	Fleming	used	his	ideas	to	invent	an	electronic	valve	later	called	a	diode	tube	or	rectifier.
The	 diode	 was	 well	 suited	 for	 changing	 alternating	 current	 into	 direct	 current,	 but	 the	 greatest
power	of	the	electron	tube	was	yet	to	be	discovered.	In	1907,	an	American	named	Lee	DeForest	added
a	third	element,	called	a	control	grid.	The	control	grid,	when	carrying	a	negative	charge,	can	reduce	or
prevent	electron	flow	from	the	cathode	to	the	anode	of	a	diode.
When	DeForest	patented	his	device,	he	called	it	an	audion	tube.	It	was	later	known	as	a	triode.
The	schematic	symbol	for	the	triode	is	shown	at	the	left.
A	triode	can	act	as	either	a	switch	or	an	amplifier.	Small	changes	in	the	charge	of	the	control	grid
can	cause	much	larger	changes	in	the	flow	of	electrons	between	the	cathode	and	the	anode.	Therefore,
a	weak	signal	applied	to	the	grid	results	in	a	much	stronger	signal	at	the	plate	output.	A	sufficiently
large	negative	charge	applied	to	the	grid	stops	all	electrons	from	leaving	the	cathode.
Additional	control	grids	were	eventually	added	to	the	triode	to	allow	more	exact	control	of	the
electron	flow.	Tubes	with	two	grids	(four	elements)	are	called	tetrodes;	tubes	with	three	grids	are
called	pentodes.	Triodes	and	pentodes	were	the	tubes	most	commonly	used	in	communications	and
computer	 applications.	 Often,	 two	 or	 three	 triodes	 or	 pentodes	 would	 be	 combined	 within	 one
envelope	so	they	could	share	a	single	heater,	thereby	reducing	the	power	consumption	of	a	particular
device.	These	latter-day	devices	were	called	“miniature”	tubes	because	many	were	about	2	inches
(5cm)	high	and	0.5	inch	(1.5cm)	in	diameter.	Equivalent	full-sized	diodes,	triodes,	and	pentodes	were
a	little	smaller	than	a	household	light	bulb.

Vacuum	 tubes	 were	 not	 well	 suited	 for	 building	 computers.	 Even	 the	 simplest	 vacuum	 tube
computer	system	required	thousands	of	tubes.	Enormous	amounts	of	electrical	power	were	required	to
heat	the	cathodes	of	these	devices.	To	prevent	a	meltdown,	this	heat	had	to	be	removed	from	the	system
as	 quickly	 as	 possible.	 Power	 consumption	 and	 heat	 dissipation	 could	 be	 reduced	 by	 running	 the
cathode	 heaters	 at	 lower	 voltages,	 but	 this	 reduced	 the	 already	 slow	 switching	 speed	 of	 the	 tube.
Despite	 their	 limitations	 and	 power	 consumption,	 vacuum	 tube	 computer	 systems,	 both	 analog	 and
digital,	 served	 their	 purpose	 for	 many	 years	 and	 are	 the	 architectural	 foundation	 for	 all	 modern
computer	systems.
Although	decades	have	passed	since	the	last	vacuum	tube	computer	was	manufactured,	vacuum
tubes	are	still	used	in	audio	amplifiers.	These	“high-end”	amplifiers	are	favored	by	musicians	who
believe	that	tubes	provide	a	resonant	and	pleasing	sound	unattainable	by	solid-state	devices.
1.5.3		The	Second	Generation:	Transistorized	Computers	(1954–1965)
The	 vacuum	 tube	 technology	 of	 the	 first	 generation	 was	 not	 very	 dependable.	 In	 fact,	 some	 ENIAC
detractors	believed	that	the	system	would	never	run	because	the	tubes	would	burn	out	faster	than	they
could	be	replaced.	Although	system	reliability	wasn’t	as	bad	as	the	doomsayers	predicted,	vacuum	tube
systems	often	experienced	more	downtime	than	uptime.
In	 1948,	 three	 researchers	 with	 Bell	 Laboratories—John	 Bardeen,	 Walter	 Brattain,	 and	 William
Shockley—invented	 the	 transistor.	 This	 new	 technology	 not	 only	 revolutionized	 devices	 such	 as
televisions	and	radios,	but	also	pushed	the	computer	industry	into	a	new	generation.	Because	transistors
consume	less	power	than	vacuum	tubes,	are	smaller,	and	work	more	reliably,	the	circuitry	in	computers
consequently	became	smaller	and	more	reliable.	Despite	using	transistors,	computers	of	this	generation
were	still	bulky	and	quite	costly.	Typically	only	universities,	governments,	and	large	businesses	could
justify	the	expense.	Nevertheless,	a	plethora	of	computer	makers	emerged	in	this	generation;	IBM,	Digital
Equipment	Corporation	(DEC),	and	Univac	(now	Unisys)	dominated	the	industry.	IBM	marketed	the	7094
for	scientific	applications	and	the	1401	for	business	applications.	DEC	was	busy	manufacturing	the	PDP-
1.	 A	 company	 founded	 (but	 soon	 sold)	 by	 Mauchly	 and	 Eckert	 built	 the	 Univac	 systems.	 The	 most
successful	Unisys	systems	of	this	generation	belonged	to	its	1100	series.	Another	company,	Control	Data
Corporation	 (CDC),	 under	 the	 supervision	 of	 Seymour	 Cray,	 built	 the	 CDC	 6600,	 the	 world’s	 first

supercomputer.	The	$10	million	CDC	6600	could	perform	10	million	instructions	per	second,	used	60-bit
words,	and	had	an	astounding	128	kilowords	of	main	memory.
What	Is	a	Transistor?
The	transistor,	short	for	transfer	resistor,	is	the	solid-state	version	of	the	triode.	There	is	no	such
thing	as	a	solid-state	version	of	the	tetrode	or	pentode.	Electrons	are	better	behaved	in	a	solid	medium
than	in	the	open	void	of	a	vacuum	tube,	so	there	is	no	need	for	the	extra	controlling	grids.	Either
germanium	or	silicon	can	be	the	basic	“solid”	used	in	these	solid-state	devices.	In	their	pure	form,
neither	of	these	elements	is	a	good	conductor	of	electricity.	But	when	they	are	combined	with	trace
amounts	 of	 elements	 that	 are	 their	 neighbors	 in	 the	 Periodic	 Chart	 of	 the	 Elements,	 they	 conduct
electricity	in	an	effective	and	easily	controlled	manner.
Boron,	aluminum,	and	gallium	can	be	found	to	the	left	of	silicon	and	germanium	on	the	Periodic
Chart.	Because	they	lie	to	the	left	of	silicon	and	germanium,	they	have	one	less	electron	in	their	outer
electron	shell,	or	valence.	So	if	you	add	a	small	amount	of	aluminum	to	silicon,	the	silicon	ends	up
with	a	slight	imbalance	in	its	outer	electron	shell,	and	therefore	attracts	electrons	from	any	pole	that
has	a	negative	potential	(an	excess	of	electrons).	When	modified	(or	doped)	in	this	way,	silicon	or
germanium	becomes	a	P-type	material.
Similarly,	if	we	add	a	little	boron,	arsenic,	or	gallium	to	silicon,	we’ll	have	extra	electrons	in
valences	of	the	silicon	crystals.	This	gives	us	an	N-type	material.	A	small	amount	of	current	will	flow
through	the	N-type	material	if	we	provide	the	loosely	bound	electrons	in	the	N-type	material	with	a
place	to	go.	In	other	words,	if	we	apply	a	positive	potential	to	N-type	material,	electrons	will	flow
from	the	negative	pole	to	the	positive	pole.	If	the	poles	are	reversed,	that	is,	if	we	apply	a	negative

potential	to	the	N-type	material	and	a	positive	potential	to	the	P-type	material,	no	current	will	flow.
This	means	we	can	make	a	solid-state	diode	from	a	simple	junction	of	N-	and	P-type	materials.
The	solid-state	triode,	the	transistor,	consists	of	three	layers	of	semiconductor	material.	Either	a
slice	of	P-type	material	is	sandwiched	between	two	N-type	materials,	or	a	slice	of	N-type	material	is
sandwiched	between	two	P-type	materials.	The	former	is	called	an	NPN	transistor,	the	latter	a	PNP
transistor.	 The	 inner	 layer	 of	 the	 transistor	 is	 called	 the	 base;	 the	 other	 two	 layers	 are	 called	 the
collector	and	the	emitter.
The	figure	at	the	left	shows	how	current	flows	through	NPN	and	PNP	transistors.	The	base	in	a
transistor	works	just	like	the	control	grid	in	a	triode	tube:	Small	changes	in	the	current	at	the	base	of	a
transistor	result	in	a	large	electron	flow	from	the	emitter	to	the	collector.
A	discrete-component	transistor	is	shown	in	“TO-50”	packaging	in	the	figure	at	the	top	of	this
sidebar.	There	are	only	three	wires	(leads)	that	connect	the	base,	emitter,	and	collector	of	the	transistor
to	the	rest	of	the	circuit.	Transistors	are	not	only	smaller	than	vacuum	tubes,	but	they	also	run	cooler
and	are	much	more	reliable.	Vacuum	tube	filaments,	like	light	bulb	filaments,	run	hot	and	eventually
burn	out.	Computers	using	transistorized	components	will	naturally	be	smaller	and	run	cooler	than	their
vacuum	 tube	 predecessors.	 The	 ultimate	 miniaturization,	 however,	 is	 not	 realized	 by	 replacing
individual	triodes	with	discrete	transistors,	but	in	shrinking	entire	circuits	onto	one	piece	of	silicon.
Integrated	 circuits,	 or	chips,	 contain	 hundreds	 to	 billions	 of	 microscopic	 transistors.	 Several
different	techniques	are	used	to	manufacture	integrated	circuits.	One	of	the	simplest	methods	involves
creating	a	circuit	using	computer-aided	design	software	that	can	print	large	maps	of	each	of	the	several
silicon	layers	forming	the	chip.	Each	map	is	used	like	a	photographic	negative	where	light-induced
changes	in	a	photoresistive	substance	on	the	chip’s	surface	produce	the	delicate	patterns	of	the	circuit
when	the	silicon	chip	is	immersed	in	a	chemical	that	washes	away	the	exposed	areas	of	the	silicon.
This	technique	is	called	photomicrolithography.	After	the	etching	is	completed,	a	layer	of	N-type	or
P-type	 material	 is	 deposited	 on	 the	 bumpy	 surface	 of	 the	 chip.	 This	 layer	 is	 then	 treated	 with	 a
photoresistive	 substance,	 exposed	 to	 light,	 and	 etched	 as	 was	 the	 layer	 before	 it.	 This	 process
continues	 until	 all	 the	 layers	 have	 been	 etched.	 The	 resulting	 peaks	 and	 valleys	 of	 P-	 and	 N-type
material	form	microscopic	electronic	components,	including	transistors,	that	behave	just	like	larger

versions	fashioned	from	discrete	components,	except	that	they	run	a	lot	faster	and	consume	a	small
fraction	of	the	power.
1.5.4		The	Third	Generation:	Integrated	Circuit	Computers	(1965–
1980)
The	real	explosion	in	computer	use	came	with	the	integrated	circuit	generation.	Jack	Kilby	invented	the
integrated	circuit	(IC),	or	microchip,	made	of	germanium.	Six	months	later,	Robert	Noyce	(who	had	also
been	working	on	integrated	circuit	design)	created	a	similar	device	using	silicon	instead	of	germanium.
This	 is	 the	 silicon	 chip	 upon	 which	 the	 computer	 industry	 was	 built.	 Early	 ICs	 allowed	 dozens	 of
transistors	to	exist	on	a	single	silicon	chip	that	was	smaller	than	a	single	“discrete	component”	transistor.
Computers	 became	 faster,	 smaller,	 and	 cheaper,	 bringing	 huge	 gains	 in	 processing	 power.	 The	 IBM
System/360	family	of	computers	was	among	the	first	commercially	available	systems	to	be	built	entirely
of	solid-state	components.	The	360	product	line	was	also	IBM’s	first	offering	in	which	all	the	machines	in
the	 family	 were	 compatible,	 meaning	 they	 all	 used	 the	 same	 assembly	 language.	 Users	 of	 smaller
machines	could	upgrade	to	larger	systems	without	rewriting	all	their	software.	This	was	a	revolutionary
new	concept	at	the	time.
The	IC	generation	also	saw	the	introduction	of	time-sharing	and	multiprogramming	(the	ability	for
more	 than	 one	 person	 to	 use	 the	 computer	 at	 a	 time).	 Multiprogramming,	 in	 turn,	 necessitated	 the
introduction	of	new	operating	systems	for	these	computers.	Time-sharing	minicomputers	such	as	DEC’s
PDP-8	 and	 PDP-11	 made	 computing	 affordable	 to	 smaller	 businesses	 and	 more	 universities.	IC
technology	also	allowed	for	the	development	of	more	powerful	supercomputers.	Seymour	Cray	took	what
he	 had	 learned	 while	 building	 the	 CDC	 6600	 and	 started	 his	 own	 company,	 the	 Cray	 Research
Corporation.	This	company	produced	a	number	of	supercomputers,	starting	with	the	$8.8	million	Cray-1,
in	1976.	The	Cray-1,	in	stark	contrast	to	the	CDC	6600,	could	execute	more	than	160	million	instructions
per	second	and	could	support	8MB	of	memory.	See	Figure	1.2	for	a	size	comparison	of	vacuum	tubes,
transistors,	and	integrated	circuits.
1.5.5		The	Fourth	Generation:	VLSI	Computers	(1980–????)
In	the	third	generation	of	electronic	evolution,	multiple	transistors	were	integrated	onto	one	chip.	As
manufacturing	techniques	and	chip	technologies	advanced,	increasing	numbers	of	transistors	were	packed
onto	one	chip.	There	are	now	various	levels	of	integration:	SSI	(small-scale	integration),	in	which	there
are	10	to	100	components	per	chip;	MSI	(medium-scale	integration),	in	which	there	are	100	to	1000
components	per	chip;	LSI	(large-scale	integration),	in	which	there	are	1000	to	10,000	components	per
chip;	and	finally,	VLSI	(very-large-scale	integration),	in	which	there	are	more	than	10,000	components
per	chip.	This	last	level,	VLSI,	marks	the	beginning	of	the	fourth	generation	of	computers.	The	complexity
of	integraged	circuits	continues	to	grow,	with	more	transistors	being	added	all	the	time.	The	term	ULSI
(ultra-large-scale	integration)	has	been	suggested	for	integrated	circuits	containing	more	than	1	million
transistors.	In	2005,	billions	of	transistors	were	put	on	a	single	chip.	Other	useful	terminology	includes:
(1)	WSI	(wafer-scale	integration,	building	superchip	ICs	from	an	entire	silicon	wafer;	(2)	3D-IC	(three-
dimensional	integrated	circuit);	and	(3)	SOC	(system-on-a-chip),	an	IC	that	includes	all	the	necessary
components	for	the	entire	computer.

FIGURE	1.2	Comparison	of	Computer	Components	Clockwise,	starting	from	the	top:
1)	Vacuum	tube
2)	Transistor
3)	Chip	containing	3200	2-input	NAND	gates
4)	Integrated	circuit	package	(the	small	silver	square	in	the	lower	left-hand	corner	is	an	integrated	circuit)
Courtesy	of	Linda	Null.
To	 give	 some	 perspective	 to	 these	 numbers,	 consider	 the	 ENIAC-on-a-chip	 project.	 In	 1997,	 to
commemorate	 the	 fiftieth	 anniversary	 of	 its	 first	 public	 demonstration,	 a	 group	 of	 students	 at	 the
University	of	Pennsylvania	constructed	a	single-chip	equivalent	of	the	ENIAC.	The	1800-square-foot,	30-
ton	beast	that	devoured	174	kilowatts	of	power	the	minute	it	was	turned	on	had	been	reproduced	on	a	chip
the	size	of	a	thumbnail.	This	chip	contained	approximately	174,569	transistors—an	order	of	magnitude
fewer	than	the	number	of	components	typically	placed	on	the	same	amount	of	silicon	in	the	late	1990s.
VLSI	allowed	Intel,	in	1971,	to	create	the	world’s	first	microprocessor,	the	4004,	which	was	a	fully
functional,	4-bit	system	that	ran	at	108KHz.	Intel	also	introduced	the	random	access	memory	(RAM)	chip,
accommodating	four	kilobits	of	memory	on	a	single	chip.	This	allowed	computers	of	the	fourth	generation
to	become	smaller	and	faster	than	their	solid-state	predecessors.
VLSI	technology,	and	its	incredible	shrinking	circuits,	spawned	the	development	of	microcomputers.
These	systems	were	small	enough	and	inexpensive	enough	to	make	computers	available	and	affordable	to
the	general	public.	The	premiere	microcomputer	was	the	Altair	8800,	released	in	1975	by	the	Micro
Instrumentation	and	Telemetry	(MITS)	corporation.	The	Altair	8800	was	soon	followed	by	the	Apple	I
and	Apple	II,	and	Commodore’s	PET	and	Vic	20.	Finally,	in	1981,	IBM	introduced	its	PC	(Personal
Computer).
The	Personal	Computer	was	IBM’s	third	attempt	at	producing	an	“entry-level”	computer	system.	Its

Datamaster	and	its	5100	Series	desktop	computers	flopped	miserably	in	the	marketplace.	Despite	these
early	failures,	IBM’s	John	Opel	convinced	his	management	to	try	again.	He	suggested	forming	a	fairly
autonomous	“independent	business	unit”	in	Boca	Raton,	Florida,	far	from	IBM’s	headquarters	in	Armonk,
New	York.	Opel	picked	Don	Estridge,	an	energetic	and	capable	engineer,	to	champion	the	development	of
the	 new	 system,	 code-named	 the	 Acorn.	 In	 light	 of	 IBM’s	 past	 failures	 in	 the	 small-systems	 area,
corporate	management	held	tight	rein	on	the	Acorn’s	timeline	and	finances.	Opel	could	get	his	project	off
the	ground	only	after	promising	to	deliver	it	within	a	year,	a	seemingly	impossible	feat.
Estridge	 knew	 that	 the	 only	 way	 he	 could	 deliver	 the	 PC	 within	 the	 wildly	 optimistic	 12-month
schedule	would	be	to	break	with	IBM	convention	and	use	as	many	“off-the-shelf”	parts	as	possible.	Thus,
from	the	outset,	the	IBM	PC	was	conceived	with	an	“open”	architecture.	Although	some	people	at	IBM
may	have	later	regretted	the	decision	to	keep	the	architecture	of	the	PC	as	nonproprietary	as	possible,	it
was	this	very	openness	that	allowed	IBM	to	set	the	standard	for	the	industry.	While	IBM’s	competitors
were	busy	suing	companies	for	copying	their	system	designs,	PC	clones	proliferated.	Before	long,	the
price	of	“IBM-compatible”	microcomputers	came	within	reach	for	just	about	every	small	business.	Also,
thanks	to	the	clone	makers,	large	numbers	of	these	systems	soon	began	finding	true	“personal	use”	in
people’s	homes.
IBM	eventually	lost	its	microcomputer	market	dominance,	but	the	genie	was	out	of	the	bottle.	For
better	or	worse,	the	IBM	architecture	continues	to	be	the	de	facto	standard	for	microcomputing,	with	each
year	 heralding	 bigger	 and	 faster	 systems.	 Today,	 the	 average	 desktop	 computer	 has	 many	 times	 the
computational	power	of	the	mainframes	of	the	1960s.
Since	the	1960s,	mainframe	computers	have	seen	stunning	improvements	in	price–performance	ratios
owing	to	VLSI	technology.	Although	the	IBM	System/360	was	an	entirely	solid-state	system,	it	was	still	a
water-cooled,	power-gobbling	behemoth.	It	could	perform	only	about	50,000	instructions	per	second	and
supported	only	16MB	of	memory	(while	usually	having	kilobytes	of	physical	memory	installed).	These
systems	were	so	costly	that	only	the	largest	businesses	and	universities	could	afford	to	own	or	lease	one.
Today’s	mainframes—now	called	“enterprise	servers”—are	still	priced	in	the	millions	of	dollars,	but
their	processing	capabilities	have	grown	several	thousand	times	over,	passing	the	billion-instructions-
per-second	mark	in	the	late	1990s.	These	systems,	often	used	as	Web	servers,	routinely	support	hundreds
of	thousands	of	transactions	per	minute!
The	 processing	 power	 brought	 by	 VLSI	 to	 supercomputers	 defies	 comprehension.	 The	 first
supercomputer,	the	CDC	6600,	could	perform	10	million	instructions	per	second,	and	had	128KB	of	main
memory.	By	contrast,	supercomputers	of	today	contain	thousands	of	processors,	can	address	terabytes	of
memory,	and	will	soon	be	able	to	perform	a	quadrillion	instructions	per	second.
What	technology	will	mark	the	beginning	of	the	fifth	generation?	Some	say	the	fifth	generation	will
mark	the	acceptance	of	parallel	processing	and	the	use	of	networks	and	single-user	workstations.	Many
people	believe	we	have	already	crossed	into	this	generation.	Some	believe	it	will	be	quantum	computing.
Some	people	characterize	the	fifth	generation	as	being	the	generation	of	neural	network,	DNA,	or	optical
computing	 systems.	 It’s	 possible	 that	 we	 won’t	 be	 able	 to	 define	 the	 fifth	 generation	 until	 we	 have
advanced	into	the	sixth	or	seventh	generations,	and	whatever	those	eras	will	bring.
The	Integrated	Circuit	and	Its	Production
Integrated	circuits	are	found	all	around	us,	from	computers	to	cars	to	refrigerators	to	cell	phones.	The

most	advanced	circuits	contain	hundreds	of	millions	(and	even	billions)	of	components	in	an	area
about	the	size	of	your	thumbnail.	The	transistors	in	these	advanced	circuits	can	be	as	small	as	45nm,	or
0.000045	millimeters,	in	size.	Thousands	of	these	transistors	would	fit	in	a	circle	the	diameter	of	a
human	hair.
How	 are	 these	 circuits	 made?	 They	 are	 manufactured	 in	 semiconductor	 fabrication	 facilities.
Because	the	components	are	so	small,	all	precautions	must	be	taken	to	ensure	a	sterile,	particle-free
environment,	so	manufacturing	is	done	in	a	“clean	room.”	There	can	be	no	dust,	no	skin	cells,	no	smoke
—not	even	bacteria.	Workers	must	wear	clean	room	suits,	often	called	“bunny	suits,”	to	ensure	that
even	the	tiniest	particle	does	not	escape	into	the	air.
The	 process	 begins	 with	 the	 chip	 design,	 which	 eventually	 results	 in	 a	 mask,	 the	 template	 or
blueprint	that	contains	the	circuit	patterns.	A	silicon	wafer	is	then	covered	by	an	insulating	layer	of
oxide,	followed	by	a	layer	of	photosensitive	film	called	photo-resist.	This	photo-resist	has	regions	that
break	down	under	UV	light	and	other	regions	that	do	not.	A	UV	light	is	then	shone	through	the	mask	(a
process	called	photolithography).	Bare	oxide	is	left	on	portions	where	the	photo-resist	breaks	down
under	the	UV	light.	Chemical	“etching”	is	then	used	to	dissolve	the	revealed	oxide	layer	and	also	to
remove	the	remaining	photo-resist	not	affected	by	the	UV	light.	The	“doping”	process	embeds	certain
impurities	 into	 the	 silicon	 that	 alters	 the	 electrical	 properties	 of	 the	 unprotected	 areas,	 basically
creating	 the	 transistors.	 The	 chip	 is	 then	 covered	 with	 another	 layer	 of	 both	 the	 insulating	 oxide
material	 and	 the	 photo-resist,	 and	 the	 entire	 process	 is	 repeated	 hundreds	 of	 times,	 each	 iteration
creating	a	new	layer	of	the	chip.	Different	masks	are	used	with	a	similar	process	to	create	the	wires
that	connect	the	components	on	the	chip.	The	circuit	is	finally	encased	in	a	protective	plastic	cover,
tested,	and	shipped	out.
As	 components	 become	 smaller	 and	 smaller,	 the	 equipment	 used	 to	 make	 them	 must	 be	 of
continually	higher	quality.	This	has	resulted	in	a	dramatic	increase	in	the	cost	of	manufacturing	ICs
over	the	years.	In	the	early	1980s,	the	cost	to	build	a	semiconductor	factory	was	roughly	$10	million.
By	the	late	1980s,	that	cost	had	risen	to	approximately	$200	million,	and	by	the	late	1990s,	an	IC
fabrication	factory	cost	more	or	less	around	$1	billion.	In	2005,	Intel	spent	approximately	$2	billion
for	a	single	fabrication	facility	and,	in	2007,	invested	roughly	$7	billion	to	retool	three	plants	in	order
to	 allow	 them	 to	 produce	 a	 smaller	 processor.	 In	 2009,	 AMD	 begin	 building	 a	 $4.2	 billion	 chip
manufacturing	facility	in	upstate	New	York.
The	manufacturing	facility	is	not	the	only	high-dollar	item	when	it	comes	to	making	ICs.	The	cost	to
design	a	chip	and	create	the	mask	can	run	anywhere	from	$1	million	to	$3	million—more	for	smaller
chips	and	less	for	larger	ones.	Considering	the	costs	of	both	the	chip	design	and	the	fabrication	facility,
it	 truly	 is	 amazing	 that	 we	 can	 walk	 into	 our	 local	 computer	 store	 and	 buy	 a	 new	 Intel	 i3
microprocessor	chip	for	around	$100.
1.5.6		Moore’s	Law
So	where	does	it	end?	How	small	can	we	make	transistors?	How	densely	can	we	pack	chips?	No	one	can
say	for	sure.	Every	year,	scientists	continue	to	thwart	prognosticators’	attempts	to	define	the	limits	of
integration.	In	fact,	more	than	one	skeptic	raised	an	eyebrow	when,	in	1965,	Intel	founder	Gordon	Moore
stated,	“The	density	of	transistors	in	an	integrated	circuit	will	double	every	year.”	The	current	version	of
this	 prediction	 is	 usually	 conveyed	 as	 “the	 density	 of	 silicon	 chips	 doubles	 every	 18	 months.”	 This
assertion	has	become	known	as	Moore’s	Law.	Moore	intended	this	postulate	to	hold	for	only	10	years.
However,	advances	in	chip	manufacturing	processes	have	allowed	this	assertion	to	hold	for	almost	40

years	(and	many	believe	it	will	continue	to	hold	well	into	the	2010s).
Yet,	using	current	technology,	Moore’s	Law	cannot	hold	forever.	There	are	physical	and	financial
limitations	that	must	ultimately	come	into	play.	At	the	current	rate	of	miniaturization,	it	would	take	about
500	years	to	put	the	entire	solar	system	on	a	chip!	Clearly,	the	limit	lies	somewhere	between	here	and
there.	Cost	may	be	the	ultimate	constraint.	Rock’s	Law,	proposed	by	early	Intel	capitalist	Arthur	Rock,	is
a	corollary	to	Moore’s	Law:	“The	cost	of	capital	equipment	to	build	semiconductors	will	double	every
four	years.”	Rock’s	Law	arises	from	the	observations	of	a	financier	who	saw	the	price	tag	of	new	chip
facilities	escalate	from	about	$12,000	in	1968	to	$12	million	in	the	mid-1990s.	In	2005,	the	cost	of
building	a	new	chip	plant	was	nearing	$3	billion.	At	this	rate,	by	the	year	2035,	not	only	will	the	size	of	a
memory	element	be	smaller	than	an	atom,	but	it	would	also	require	the	entire	wealth	of	the	world	to	build
a	single	chip!	So	even	if	we	continue	to	make	chips	smaller	and	faster,	the	ultimate	question	may	be
whether	we	can	afford	to	build	them.
Certainly,	if	Moore’s	Law	is	to	hold,	Rock’s	Law	must	fall.	It	is	evident	that	for	these	two	things	to
happen,	computers	must	shift	to	a	radically	different	technology.	Research	into	new	computing	paradigms
has	 been	 proceeding	 in	 earnest	 during	 the	 last	 half	 decade.	 Laboratory	 prototypes	 fashioned	 around
organic	computing,	superconducting,	molecular	physics,	and	quantum	computing	have	been	demonstrated.
Quantum	computers,	which	leverage	the	vagaries	of	quantum	mechanics	to	solve	computational	problems,
are	 particularly	 exciting.	 Not	 only	 would	 quantum	 systems	 compute	 exponentially	 faster	 than	 any
previously	used	method,	but	they	would	also	revolutionize	the	way	in	which	we	define	computational
problems.	Problems	that	today	are	considered	ludicrously	infeasible	could	be	well	within	the	grasp	of	the
next	generation’s	schoolchildren.	These	school-children	may,	in	fact,	chuckle	at	our	“primitive”	systems
in	the	same	way	that	we	are	tempted	to	chuckle	at	the	ENIAC.
1.6			THE	COMPUTER	LEVEL	HIERARCHY
If	a	machine	is	to	be	capable	of	solving	a	wide	range	of	problems,	it	must	be	able	to	execute	programs
written	in	different	languages,	from	Fortran	and	C	to	Lisp	and	Prolog.	As	we	shall	see	in	Chapter	3,	the
only	 physical	 components	 we	 have	 to	 work	 with	 are	 wires	 and	 gates.	 A	 formidable	 open	 space—a
semantic	gap—exists	between	these	physical	components	and	a	high-level	language	such	as	C++.	For	a
system	to	be	practical,	the	semantic	gap	must	be	invisible	to	most	of	the	users	of	the	system.
Programming	experience	teaches	us	that	when	a	problem	is	large,	we	should	break	it	down	and	use	a
“divide	and	conquer”	approach.	In	programming,	we	divide	a	problem	into	modules	and	then	design	each
module	separately.	Each	module	performs	a	specific	task,	and	modules	need	only	know	how	to	interface
with	other	modules	to	make	use	of	them.
Computer	 system	 organization	 can	 be	 approached	 in	 a	 similar	 manner.	 Through	 the	 principle	 of
abstraction,	we	can	imagine	the	machine	to	be	built	from	a	hierarchy	of	levels,	in	which	each	level	has	a
specific	function	and	exists	as	a	distinct	hypothetical	machine.	We	call	the	hypothetical	computer	at	each
level	a	virtual	machine.	 Each	 level’s	 virtual	 machine	 executes	 its	 own	 particular	 set	 of	 instructions,
calling	 upon	 machines	 at	 lower	 levels	 to	 carry	 out	 the	 tasks	 when	 necessary.	 By	 studying	 computer
organization,	you	will	see	the	rationale	behind	the	hierarchy’s	partitioning,	as	well	as	how	these	layers
are	 implemented	 and	 interface	 with	 each	 other.	Figure	 1.3	 shows	 the	 commonly	 accepted	 layers
representing	the	abstract	virtual	machines.
Level	6,	the	User	Level,	is	composed	of	applications	and	is	the	level	with	which	everyone	is	most
familiar.	At	this	level,	we	run	programs	such	as	word	processors,	graphics	packages,	or	games.	The

lower	levels	are	nearly	invisible	from	the	User	Level.
Level	5,	the	High-Level	Language	Level,	consists	of	languages	such	as	C,	C++,	Fortran,	Lisp,	Pascal,
and	Prolog.	These	languages	must	be	translated	(using	either	a	compiler	or	an	interpreter)	to	a	language
the	 machine	 can	 understand.	 Compiled	 languages	 are	 translated	 into	 assembly	 language	 and	 then
assembled	into	machine	code.	(They	are	translated	to	the	next	lower	level.)	The	user	at	this	level	sees
very	little	of	the	lower	levels.	Even	though	a	programmer	must	know	about	data	types	and	the	instructions
available	for	those	types,	he	or	she	need	not	know	about	how	those	types	are	actually	implemented.
FIGURE	1.3	The	Abstract	Levels	of	Modern	Computing	Systems
Level	4,	the	Assembly	Language	Level,	encompasses	some	type	of	assembly	language.	As	previously
mentioned,	 compiled	 higher-level	 languages	 are	 first	 translated	 to	 assembly,	 which	 is	 then	 directly
translated	 to	 machine	 language.	 This	 is	 a	 one-to-one	 translation,	 meaning	 that	 one	 assembly	 language
instruction	is	translated	to	exactly	one	machine	language	instruction.	By	having	separate	levels,	we	reduce
the	semantic	gap	between	a	high-level	language,	such	as	C++,	and	the	actual	machine	language	(which
consists	of	0s	and	1s).
Level	 3,	 the	 System	 Software	 Level,	 deals	 with	 operating	 system	 instructions.	 This	 level	 is
responsible	 for	 multiprogramming,	 protecting	 memory,	 synchronizing	 processes,	 and	 various	 other
important	functions.	Often,	instructions	translated	from	assembly	language	to	machine	language	are	passed
through	this	level	unmodified.

Level	2,	the	Instruction	Set	Architecture	(ISA),	or	Machine	Level,	consists	of	the	machine	language
recognized	by	the	particular	architecture	of	the	computer	system.	Programs	written	in	a	computer’s	true
machine	 language	 on	 a	 hardwired	 computer	 (see	 below)	 can	 be	 executed	 directly	 by	 the	 electronic
circuits	without	any	interpreters,	translators,	or	compilers.	We	will	study	ISAs	in	depth	in	Chapters	4	and
5.
Level	1,	the	Control	Level,	is	where	a	control	unit	makes	sure	that	instructions	are	decoded	and
executed	properly	and	that	data	is	moved	where	and	when	it	should	be.	The	control	unit	interprets	the
machine	instructions	passed	to	it,	one	at	a	time,	from	the	level	above,	causing	the	required	actions	to	take
place.
Control	 units	 can	 be	 designed	 in	 one	 of	 two	 ways:	 They	 can	 be	hardwired	 or	 they	 can	 be
microprogrammed.	 In	 hardwired	 control	 units,	 control	 signals	 emanate	 from	 blocks	 of	 digital	 logic
components.	These	signals	direct	all	the	data	and	instruction	traffic	to	appropriate	parts	of	the	system.
Hardwired	control	units	are	typically	very	fast	because	they	are	actually	physical	components.	However,
once	implemented,	they	are	very	difficult	to	modify	for	the	same	reason.
The	other	option	for	control	is	to	implement	instructions	using	a	microprogram.	A	microprogram	is	a
program	 written	 in	 a	 low-level	 language	 that	 is	 implemented	 directly	 by	 the	 hardware.	 Machine
instructions	produced	in	Level	2	are	fed	into	this	microprogram,	which	then	interprets	the	instructions	by
activating	 hardware	 suited	 to	 execute	 the	 original	 instruction.	 One	 machine-level	 instruction	 is	 often
translated	into	several	microcode	instructions.	This	is	not	the	one-to-one	correlation	that	exists	between
assembly	 language	 and	 machine	 language.	 Microprograms	 are	 popular	 because	 they	 can	 be	 modified
relatively	 easily.	 The	 disadvantage	 of	 microprogramming	 is,	 of	 course,	 that	 the	 additional	 layer	 of
translation	typically	results	in	slower	instruction	execution.
Level	0,	the	Digital	Logic	Level,	is	where	we	find	the	physical	components	of	the	computer	system:
the	gates	and	wires.	These	are	the	fundamental	building	blocks,	the	implementations	of	the	mathematical
logic,	that	are	common	to	all	computer	systems.	Chapter	3	presents	the	Digital	Logic	Level	in	detail.
1.7			CLOUD	COMPUTING:	COMPUTING	AS	A	SERVICE
We	must	never	forget	that	the	ultimate	aim	of	every	computer	system	is	to	deliver	functionality	to	its	users.
Computer	users	typically	do	not	care	about	terabytes	of	storage	and	gigahertz	of	processor	speed.	In	fact,
many	 companies	 and	 government	 agencies	 have	 “gotten	 out	 of	 the	 technology	 business”	 entirely	 by
outsourcing	their	data	centers	to	third-party	specialists.	These	outsourcing	agreements	tend	to	be	highly
complex	and	prescribe	every	aspect	of	the	hardware	configuration.	Along	with	the	detailed	hardware
specifications,	service-level	 agreements	 (SLAs)	 provide	 penalties	 if	 certain	 parameters	 of	 system
performance	and	availability	are	not	met.	Both	contracting	parties	employ	individuals	whose	main	job	is
to	 monitor	 the	 contract,	 calculate	 bills,	 and	 determine	 SLA	 penalties	 when	 needed.	 Thus,	 with	 the
additional	administrative	overhead,	data	center	outsourcing	is	neither	a	cheap	nor	an	easy	solution	for
companies	that	want	to	avoid	the	problems	of	technology	management.

FIGURE	1.4	Levels	of	Computing	as	a	Service
A	 somewhat	 easier	 approach	 may	 be	 found	 in	 the	 emerging	 field	 of	 Cloud	 computing.	Cloud
computing	is	the	general	term	for	any	type	of	virtual	computing	platform	provided	over	the	Internet.	A
Cloud	computing	platform	is	defined	in	terms	of	the	services	that	it	provides	rather	than	its	physical
configuration.	Its	name	derives	from	the	cloud	icon	that	symbolizes	the	Internet	on	schematic	diagrams.
But	the	metaphor	carries	well	into	the	actual	Cloud	infrastructure,	because	the	computer	is	more	abstract
than	real.	The	“computer”	and	“storage”	appear	to	the	user	as	a	single	entity	in	the	Cloud	but	usually	span
several	 physical	 servers.	 The	 storage	 is	 usually	 located	 on	 an	 array	 of	 disks	 that	 are	 not	 directly
connected	to	any	particular	server.	System	software	is	designed	to	give	this	configuration	the	illusion	of
being	a	single	system;	thus,	we	say	that	it	presents	a	virtual	machine	to	the	user.
Cloud	computing	services	can	be	defined	and	delivered	in	a	number	of	ways	based	on	levels	of	the
computer	hierarchy	shown	again	in	Figure	1.4.	At	the	top	of	the	hierarchy,	where	we	have	executable
programs,	 a	 Cloud	 provider	 might	 offer	 an	 entire	 application	 over	 the	 Internet,	 with	 no	 components
installed	locally.	This	is	called	Software	as	a	Service,	or	SaaS.	The	consumer	of	this	service	does	not
maintain	 the	 application	 or	 need	 to	 be	 at	 all	 concerned	 with	 the	 infrastructure	 in	 any	 way.	 SaaS
applications	tend	to	focus	on	narrow,	non-business-critical	applications.	Well-known	examples	include
Gmail,	 Dropbox,	 GoToMeeting,	 and	 Netflix.	 Specialized	 products	 are	 available	 for	 tax	 return
preparation,	payroll,	fleet	management,	and	case	management,	to	name	only	a	few.	Salesforce.com	is	a

pioneering,	full-featured	SaaS	offering	designed	for	customer	relationship	management.	Fee-based	SaaS
is	typically	billed	monthly	according	to	the	number	of	users,	sometimes	with	per-transaction	fees	added
on	as	well.
A	great	disadvantage	of	SaaS	is	that	the	consumer	has	little	control	over	the	behavior	of	the	product.
This	may	be	problematic	if	a	company	has	to	make	radical	changes	to	its	processes	or	policies	in	order	to
use	a	SaaS	product.	Companies	that	desire	to	have	more	control	over	their	applications,	or	that	need
applications	 for	 which	 SaaS	 is	 unavailable,	 might	 instead	 opt	 to	 deploy	 their	 own	 applications	 on	 a
Cloud-hosted	 environment	 called	Platform	as	a	Service,	 or	PaaS.	 PaaS	 provides	 server	 hardware,
operating	systems,	database	services,	security	components,	and	backup	and	recovery	services.	The	PaaS
provider	manages	performance	and	availability	of	the	environment,	whereas	the	customer	manages	the
applications	hosted	in	the	PaaS	Cloud.	The	customer	is	typically	billed	monthly	per	megabytes	of	storage,
processor	utilization,	and	megabytes	of	data	transferred.	Well-known	PaaS	providers	include	Google	App
Engine	 and	 Microsoft	 Windows	 Azure	 Cloud	 Services	 [as	 well	 as	Force.com	 (PaaS	 provided	 by
Salesforce.com)].
PaaS	is	not	a	good	fit	in	situations	where	rapid	configuration	changes	are	required.	This	would	be	the
case	if	a	company’s	main	business	is	software	development.	The	formality	of	change	processes	necessary
to	a	well-run	PaaS	operation	impedes	rapid	software	deployment	[by	forcing	a	company	to	play	by	the
service	provider’s	rules].	Indeed,	in	any	company	where	staff	is	capable	of	managing	operating	system
and	database	software,	the	Infrastructure	as	a	Service	(IaaS)	Cloud	model	might	be	the	best	option.
IaaS,	 [the	 most	 basic	 of	 the	 models,]	provides	 only	 server	 hardware,	 secure	 network	 access	 to	 the
servers,	and	backup	and	recovery	services.	The	customer	is	responsible	for	all	system	software	including
the	 operating	 system	 and	 databases.	 IaaS	 is	 typically	 billed	 by	 the	 number	 of	 virtual	 machines	 used,
megabytes	of	storage,	and	megabytes	of	data	transferred,	but	at	a	lower	rate	than	PaaS.	The	biggest	names
in	IaaS	include	Amazon	EC2,	Google	Compute	Engine,	Microsoft	Azure	Services	Platform,	Rackspace,
and	HP	Cloud.
Not	only	do	PaaS	and	IaaS	liberate	the	customer	from	the	difficulties	of	data	center	management,	they
also	provide	elasticity:	the	ability	to	add	and	remove	resources	based	on	demand.	A	customer	pays	for
only	as	much	infrastructure	as	is	needed.	So	if	a	business	has	a	peak	season,	extra	capacity	needs	to	be
allocated	only	for	the	duration	of	the	peak	period.	This	flexibility	can	save	a	company	a	great	deal	of
money	when	it	has	large	variations	in	computing	demands.
Cloud	storage	 is	 a	 limited	 type	 of	 IaaS.	 The	 general	 public	 can	 obtain	 small	 amounts	 of	 Cloud
storage	inexpensively	through	services	such	as	Dropbox,	Google	Drive,	and	Amazon.com’s	Cloud	Drive
—to	name	only	a	few	among	a	crowded	field.	Google,	Amazon,	HP,	IBM,	and	Microsoft	are	among
several	 vendors	 that	 provide	 Cloud	 storage	 for	 the	 enterprise.	 As	 with	 Cloud	 computing	 in	 general,
enterprise-grade	Cloud	storage	also	requires	careful	management	of	performance	and	availability.
The	question	that	all	potential	Cloud	computing	customers	must	ask	themselves	is	whether	it	is	less
expensive	to	maintain	their	own	data	center	or	to	buy	Cloud	services—including	the	allowances	for	peak
periods.	 Moreover,	 as	 with	 traditional	 outsourcing,	 vendor-provided	 Cloud	 computing	 still	 involves
considerable	contract	negotiation	and	management	on	the	part	of	both	parties.	SLA	management	remains
an	 important	 activity	 in	 the	 relationship	 between	 the	 service	 provider	 and	 the	 service	 consumer.
Moreover,	once	an	enterprise	moves	its	assets	to	the	Cloud,	it	might	be	difficult	to	transition	back	to	a
company-owned	data	center,	should	the	need	arise.	Thus,	any	notion	of	moving	assets	to	the	Cloud	must
be	carefully	considered,	and	the	risks	clearly	understood.
The	 Cloud	 also	 presents	 a	 number	 of	 challenges	 to	 computer	 scientists.	 First	 and	 foremost	 is	 the
technical	configuration	of	the	data	center.	The	infrastructure	must	provide	for	uninterrupted	service,	even

during	 maintenance	 activities.	 It	 must	 permit	 expedient	 allocation	 of	 capacity	 to	 where	 it	 is	 needed
without	degrading	or	interrupting	services.	Performance	of	the	infrastructure	must	be	carefully	monitored
and	 interventions	 taken	 whenever	 performance	 falls	 below	 certain	 defined	 thresholds;	 otherwise,
monetary	SLA	penalties	may	be	incurred.
On	the	consumer	side	of	the	Cloud,	software	architects	and	programmers	must	be	mindful	of	resource
consumption,	 because	 the	 Cloud	 model	 charges	 fees	 in	 proportion	 to	 the	 resources	 consumed.	 These
resources	 include	 communications	 bandwidth,	 processor	 cycles,	 and	 storage.	 Thus,	 to	 save	 money,
application	programs	should	be	designed	to	reduce	trips	over	the	network,	economize	machine	cycles,
and	minimize	bytes	of	storage.	Meticulous	testing	is	crucial	prior	to	deploying	a	program	in	the	Cloud:	An
errant	module	that	consumes	resources,	say,	in	an	infinite	loop,	could	result	in	a	“surprising”	Cloud	bill	at
the	end	of	the	month.
With	 the	 cost	 and	 complexity	 of	 data	 centers	 continuing	 to	 rise—with	 no	 end	 in	 sight—Cloud
computing	is	almost	certain	to	become	the	platform	of	choice	for	medium-	to	small-sized	businesses.	But
the	 Cloud	 is	 not	 worry-free.	 A	 company	 might	 end	 up	 trading	 its	 technical	 challenges	 for	 even	 more
vexing	supplier	management	challenges.
1.8			THE	VON	NEUMANN	MODEL
In	the	earliest	electronic	computing	machines,	programming	was	synonymous	with	connecting	wires	to
plugs.	No	layered	architecture	existed,	so	programming	a	computer	was	as	much	of	a	feat	of	electrical
engineering	as	it	was	an	exercise	in	algorithm	design.	Before	their	work	on	the	ENIAC	was	complete,
John	 W.	 Mauchly	 and	 J.	 Presper	 Eckert	 conceived	 of	 an	 easier	 way	 to	 change	 the	 behavior	 of	 their
calculating	 machine.	 They	 reckoned	 that	 memory	 devices,	 in	 the	 form	 of	 mercury	 delay	 lines,	 could
provide	a	way	to	store	program	instructions.	This	would	forever	end	the	tedium	of	rewiring	the	system
each	time	it	had	a	new	problem	to	solve,	or	an	old	one	to	debug.	Mauchly	and	Eckert	documented	their
idea,	proposing	it	as	the	foundation	for	their	next	computer,	the	EDVAC.	Unfortunately,	while	they	were
involved	in	the	top	secret	ENIAC	project	during	World	War	II,	Mauchly	and	Eckert	could	not	immediately
publish	their	insight.
No	 such	 proscriptions,	 however,	 applied	 to	 a	 number	 of	 people	 working	 at	 the	 periphery	 of	 the
ENIAC	project.	One	of	these	people	was	a	famous	Hungarian	mathematician	named	John	von	Neumann
(pronounced	von	noy-man).	After	reading	Mauchly	and	Eckert’s	proposal	for	the	EDVAC,	von	Neumann
published	and	publicized	the	idea.	So	effective	was	he	in	the	delivery	of	this	concept	that	history	has
credited	him	with	its	invention.	All	stored-program	computers	have	come	to	be	known	as	von	Neumann
systems	using	the	von	Neumann	architecture.	Although	we	are	compelled	by	tradition	to	say	that	stored-
program	computers	use	the	von	Neumann	architecture,	we	shall	not	do	so	without	paying	proper	tribute	to
its	true	inventors:	John	W.	Mauchly	and	J.	Presper	Eckert.
Today’s	 version	 of	 the	 stored-program	 machine	 architecture	 satisfies	 at	 least	 the	 following
characteristics:
•	 	 	 Consists	 of	 three	 hardware	 systems:	 A	central	 processing	 unit	 (CPU)	 with	 a	 control	 unit,	 an
arithmetic	logic	unit	(ALU),	registers	(small	storage	areas),	and	a	program	counter;	a	main	memory
system,	which	holds	programs	that	control	the	computer’s	operation;	and	an	I/O	system.
•			Capacity	to	carry	out	sequential	instruction	processing.
•			Contains	a	single	path,	either	physically	or	logically,	between	the	main	memory	system	and	the	control

unit	 of	 the	 CPU,	 forcing	 alternation	 of	 instruction	 and	 execution	 cycles.	 This	 single	 path	 is	 often
referred	to	as	the	von	Neumann	bottleneck.
Figure	1.5	shows	how	these	features	work	together	in	modern	computer	systems.	Notice	that	the	system
shown	in	the	figure	passes	all	of	its	I/O	through	the	arithmetic	logic	unit	(actually,	it	passes	through	the
accumulator,	which	is	part	of	the	ALU).	This	architecture	runs	programs	in	what	is	known	as	the	von
Neumann	execution	cycle	 (also	 called	 the	fetch-decode-execute	cycle),	 which	 describes	 how	 the
machine	works.	One	iteration	of	the	cycle	is	as	follows:
1.		The	control	unit	fetches	the	next	program	instruction	from	the	memory,	using	the	program	counter	to
determine	where	the	instruction	is	located.
2.		The	instruction	is	decoded	into	a	language	the	ALU	can	understand.
3.		Any	data	operands	required	to	execute	the	instruction	are	fetched	from	memory	and	placed	in	registers
in	the	CPU.
4.		The	ALU	executes	the	instruction	and	places	the	results	in	registers	or	memory.
The	ideas	present	in	the	von	Neumann	architecture	have	been	extended	so	that	programs	and	data
stored	in	a	slow-to-access	storage	medium,	such	as	a	hard	disk,	can	be	copied	to	a	fast-access,	volatile
storage	medium	such	as	RAM	prior	to	execution.	This	architecture	has	also	been	streamlined	into	what	is
currently	called	the	system	bus	model,	which	is	shown	in	Figure	1.6.	The	data	bus	moves	data	from	main
memory	to	the	CPU	registers	(and	vice	versa).	The	address	bus	holds	the	address	of	the	data	that	the	data
bus	is	currently	accessing.	The	control	bus	carries	the	necessary	control	signals	that	specify	how	the
information	transfer	is	to	take	place.

FIGURE	1.5	The	von	Neumann	Architecture
FIGURE	1.6	The	Modified	von	Neumann	Architecture,	Adding	a	System	Bus
Other	enhancements	to	the	von	Neumann	architecture	include	using	index	registers	for	addressing,
adding	 floating-point	 data,	 using	 interrupts	 and	 asynchronous	 I/O,	 adding	 virtual	 memory,	 and	 adding
general	registers.	You	will	learn	a	great	deal	about	these	enhancements	in	the	chapters	that	follow.
Quantum	Leap	for	Computers:	How	Small	Can	We	Go?
VLSI	technology	has	allowed	us	to	put	billions	of	transistors	on	a	single	chip,	but	there	is	a	limit	to
how	small	we	can	go	with	current	transistor	technology.	Researchers	at	the	University	of	New	South
Wales’	 Centre	 for	 Quantum	 Computer	 Technology	 and	 the	 University	 of	 Wisconsin–Madison	 have
taken	“small”	to	an	entirely	new	level.	In	May	2010,	they	announced	the	7-atom	transistor,	a	working
transistor	embedded	in	silicon	that	is	only	7	atoms	in	size.	Transistors	1	atom	in	size	that	allowed	the
flows	of	electrons	were	reported	as	early	as	2002,	but	this	transistor	is	different	in	that	it	provides	all
the	functionality	of	a	transistor	as	we	know	it	today.
The	7-atom	transistor	was	created	by	hand,	using	a	scanning	tunneling	microscope.	It’s	a	long	way
from	being	mass	produced,	but	the	researchers	hope	to	make	it	commercially	available	by	2015.	The
transistor’s	 tiny	 size	 means	 smaller	 but	 more	 powerful	 computers.	 Experts	 estimate	 it	 may	 shrink
microchips	by	a	factor	of	100,	while	enabling	an	exponential	speedup	in	processing.	This	means	our
computers	could	become	one	hundred	times	smaller,	but	at	the	same	time,	also	one	hundred	times
faster.
In	addition	to	replacing	traditional	transistors,	this	discovery	may	be	fundamental	in	the	efforts	to
build	a	quantum	computer	in	silicon.	Quantum	computing	is	expected	to	be	the	next	significant	leap	in
computer	technology.	Small	quantum	computers	now	exist	that	perform	calculations	millions	of	times
faster	than	conventional	computers,	but	these	computers	are	too	small	to	be	of	much	use.	A	large-scale,
working	quantum	computer	would	enable	us	to	perform	calculations	and	solve	problems	that	would
take	a	conventional	computer	more	than	13	billion	years.	That	could	change	the	way	we	view	the
world.	For	one	thing,	every	encryption	algorithm	employed	today	would	be	useless	against	that	kind	of
computing	 power.	 On	 the	 other	 hand,	 ultra-secure	 communications	 would	 be	 possible	 using	 new

quantum	technologies.
Quantum	computers	have	significant	potential.	Current	applications,	including	special	effects	for
movies,	cryptography,	searching	large	data	files,	factoring	large	numbers,	simulating	various	systems
(such	as	nuclear	explosions	and	weather	patterns),	military	and	intelligence	gathering,	and	intensive,
time-consuming	computations	(such	as	those	found	in	astronomy,	physics,	and	chemistry),	would	all
see	tremendous	performance	increases	if	quantum	computing	were	used.	New	applications	we	have	not
yet	discovered	are	likely	to	evolve	as	well.
In	addition	to	its	potential	to	change	computing	as	we	know	it	today,	this	new	7-atom	transistor	is
significant	for	another	reason.	Recall	Moore’s	Law;	this	law	is	not	so	much	a	law	of	nature,	but	rather
an	expectation	of	innovation	and	a	significant	driving	force	in	chip	design.	Moore’s	Law	has	held	since
1965,	but	in	order	to	do	so,	chip	manufacturers	have	jumped	from	one	technology	to	another.	Gordon
Moore	himself	has	predicted	that,	if	restricted	to	CMOS	silicon,	his	law	will	fail	sometime	around
2020.	The	discovery	of	this	7-atom	transistor	gives	new	life	to	Moore’s	Law—and	we	suspect	that
Gordon	 Moore	 is	 breathing	 a	 sigh	 of	 relief	 over	 its	 discovery.	 However,	 noted	 physicist	 Stephen
Hawking	has	explained	that	chip	manufacturers	are	limited	in	their	quest	to	“enforce”	Moore’s	Law	by
two	fundamental	constraints:	the	speed	of	light	and	the	atomic	nature	of	matter,	implying	that	Moore’s
Law	will	eventually	fail,	regardless	of	the	technology	being	used.
1.9			NON–VON	NEUMANN	MODELS
Until	 recently,	 almost	 all	 general-purpose	 computers	 followed	 the	 von	 Neumann	 design.	 That	 is,	 the
architecture	consisted	of	a	CPU,	memory,	and	I/O	devices,	and	they	had	single	storage	for	instructions	and
data,	as	well	as	a	single	bus	used	for	fetching	instructions	and	transferring	data.	von	Neumann	computers
execute	 instructions	 sequentially	 and	 are	 therefore	 extremely	 well	 suited	 to	 sequential	 processing.
However,	 the	 von	 Neumann	 bottleneck	 continues	 to	 baffle	 engineers	 looking	 for	 ways	 to	 build	 fast
systems	that	are	inexpensive	and	compatible	with	the	vast	body	of	commercially	available	software.
Engineers	who	are	not	constrained	by	the	need	to	maintain	compatibility	with	von	Neumann	systems
are	free	to	use	many	different	models	of	computing.	Non–von	Neumann	architectures	are	those	in	which
the	model	of	computation	varies	from	the	characteristics	listed	for	the	von	Neumann	architecture.	For
example,	an	architecture	that	does	not	store	programs	and	data	in	memory	or	does	not	process	a	program
sequentially	would	be	considered	a	non–von	Neumann	machine.	Also,	a	computer	that	has	two	buses,	one
for	 data	 and	 a	 separate	 one	 for	 instructions,	 would	 be	 considered	 a	 non–von	 Neumann	 machine.
Computers	designed	using	the	Harvard	architecture	have	two	buses,	thus	allowing	data	and	instructions
to	be	transferred	simultaneously,	but	also	have	separate	storage	for	data	and	instructions.	Many	modern
general-purpose	 computers	 use	 a	 modified	 version	 of	 the	 Harvard	 architecture	 in	 which	 they	 have
separate	 pathways	 for	 data	 and	 instructions	 but	 not	 separate	 storage.	 Pure	 Harvard	 architectures	 are
typically	used	in	microcontrollers	(an	entire	computer	system	on	a	chip),	such	as	those	found	in	embedded
systems,	as	in	appliances,	toys,	and	cars.
Many	non–von	Neumann	machines	are	designed	for	special	purposes.	The	first	recognized	non–von
Neumann	processing	chip	was	designed	strictly	for	image	processing.	Another	example	is	a	reduction
machine	 (built	 to	 perform	 combinatory	 logic	 calculations	 using	 graph	 reduction).	 Other	 non–von
Neumann	computers	include	digital	signal	processors	(DSPs)	and	media	processors,	which	can	execute
a	single	instruction	on	a	set	of	data	(instead	of	executing	a	single	instruction	on	a	single	piece	of	data).
A	number	of	different	subfields	fall	into	the	non–von	Neumann	category,	including	neural	networks

(using	 ideas	 from	 models	 of	 the	 brain	 as	 a	 computing	 paradigm)	 implemented	 in	 silicon,	cellular
automata,	cognitive	computers	(machines	that	learn	by	experience	rather	than	through	programming,
including	IBM’s	SyNAPSE	computer,	a	machine	that	models	the	human	brain),	quantum	computation	(a
combination	of	computing	and	quantum	physics),	dataflow	computation,	and	parallel	computers.	These
all	have	something	in	common—the	computation	is	distributed	among	different	processing	units	that	act	in
parallel.	They	differ	in	how	weakly	or	strongly	the	various	components	are	connected.	Of	these,	parallel
computing	is	currently	the	most	popular.
1.10			PARALLEL	PROCESSORS	AND	PARALLEL	COMPUTING
Today,	parallel	processing	solves	some	of	our	biggest	problems	in	much	the	same	way	that	settlers	of	the
Old	West	solved	their	biggest	problems	using	parallel	oxen.	If	they	were	using	an	ox	to	move	a	tree	and
the	ox	was	not	big	enough	or	strong	enough,	they	certainly	didn’t	try	to	grow	a	bigger	ox—they	used	two
oxen.	If	our	computer	isn’t	fast	enough	or	powerful	enough,	instead	of	trying	to	develop	a	faster,	more
powerful	computer,	why	not	simply	use	multiple	computers?	This	is	precisely	what	parallel	computing
does.	The	first	parallel	processing	systems	were	built	in	the	late	1960s	and	had	only	two	processors.	The
1970s	saw	the	introduction	of	supercomputers	with	as	many	as	32	processors,	and	the	1980s	brought	the
first	 systems	 with	 more	 than	 1000	 processors.	 Finally,	 in	 1999,	 IBM	 announced	 funding	 for	 the
development	of	a	supercomputer	architecture	called	the	Blue	Gene	series.	The	first	computer	in	this
series,	the	Blue	Gene/L,	is	a	massively	parallel	computer	containing	131,000	dual-core	processors,	each
with	 its	 own	 dedicated	 memory.	 In	 addition	 to	 allowing	 researchers	 to	 study	 the	 behavior	 of	 protein
folding	(by	using	large	simulations),	this	computer	has	also	allowed	researchers	to	explore	new	ideas	in
parallel	architectures	and	software	for	those	architectures.	IBM	has	continued	to	add	computers	to	this
series.	The	Blue	Gene/P	appeared	in	2007	and	has	quad-core	processors.	The	latest	computer	designed
for	this	series,	the	Blue	Gene/Q,	uses	16-core	processors,	with	1024	compute	nodes	per	rack,	scalable
up	to	512	racks.	Installations	of	the	Blue	Gene/Q	computer	include	Nostromo	(being	used	for	biomedical
data	in	Poland),	Sequoia	(being	used	at	Lawrence	Livermore	National	Laboratory	for	nuclear	simulations
and	scientific	research),	and	Mira	(used	at	Argonne	National	Laboratory).
Dual-core	 and	 quad-core	 processors	 (and	 higher,	 as	 we	 saw	 in	 Blue	 Gene/Q)	 are	 examples	 of
multicore	processors.	But	what	is	a	multicore	processor?	Essentially,	it	is	a	special	type	of	parallel
processor.	 Parallel	 processors	 are	 often	 classified	 as	 either	 “shared	 memory”	 processors	 (in	 which
processors	 all	 share	 the	 same	 global	 memory)	 or	 “distributed	 memory”	 computers	 (in	 which	 each
processor	has	its	own	private	memory).	Chapter	9	covers	parallel	processors	in	detail.	The	following
discussion	is	limited	to	shared	memory	multicore	architectures—the	type	used	in	personal	computers.
Multicore	architectures	are	parallel	processing	machines	that	allow	for	multiple	processing	units
(often	called	cores)	on	a	single	chip.	Dual	core	means	2	cores;	quad	core	machines	have	4	cores;	and	so
on.	But	what	is	a	core?	Instead	of	a	single	processing	unit	in	an	integrated	circuit	(as	found	in	typical	von
Neumann	machines),	independent	multiple	cores	are	“plugged	in”	and	run	in	parallel.	Each	processing
unit	has	its	own	ALU	and	set	of	registers,	but	all	processors	share	memory	and	some	other	resources.
“Dual	 core”	 is	 different	 from	 “dual	 processor.”	 Dual-processor	 machines,	 for	 example,	 have	 two
processors,	but	each	processor	plugs	into	the	motherboard	separately.	The	important	distinction	to	note	is
that	all	cores	in	multicore	machines	are	integrated	into	the	same	chip.	This	means	that	you	could,	for
example,	 replace	 a	 single-core	 (uniprocessor)	 chip	 in	 your	 computer	 with,	 for	 example,	 a	 dual-core
processor	chip	(provided	your	computer	had	the	appropriate	socket	for	the	new	chip).	Many	computers

today	are	advertised	as	dual	core,	quad	core,	or	higher.	Dual	core	is	generally	considered	the	standard	in
today’s	 computers.	 Although	 most	 desktop	 and	 laptop	 computers	 have	 limited	 cores	 (fewer	 than	 8),
machines	with	hundreds	of	cores	are	available	for	the	right	price,	of	course.
Just	because	your	computer	has	multiple	cores	does	not	mean	it	will	run	your	programs	more	quickly.
Application	 programs	 (including	 operating	 systems)	 must	 be	 written	 to	 take	 advantage	 of	 multiple
processing	units	(this	statement	is	true	for	parallel	processing	in	general).	Multicore	computers	are	very
useful	for	multitasking—when	users	are	doing	more	than	one	thing	at	a	time.	For	example,	you	may	be
reading	email,	listening	to	music,	browsing	the	Web,	and	burning	a	DVD	all	at	the	same	time.	These
“multiple	 tasks”	 can	 be	 assigned	 to	 different	 processors	 and	 carried	 out	 in	 parallel,	 provided	 the
operating	system	is	able	to	manipulate	many	tasks	at	once.
In	addition	to	multitasking,	multithreading	can	also	increase	the	performance	of	any	application	with
inherent	parallelism.	Programs	are	divided	into	threads,	which	can	be	thought	of	as	mini-processes.	For
example,	a	Web	browser	is	multithreaded;	one	thread	can	download	text,	while	each	image	is	controlled
and	 downloaded	 by	 a	 separate	 thread.	 If	 an	 application	 is	 multithreaded,	 separate	 threads	 can	 run	 in
parallel	 on	 different	 processing	 units.	 We	 should	 note	 that	 even	 on	 uniprocessors,	 multithreading	 can
improve	 performance,	 but	 this	 is	 a	 discussion	 best	 left	 for	 another	 time.	 For	 more	 information,	 see
Stallings	(2012).
To	 summarize,	 parallel	 processing	 refers	 to	 a	 collection	 of	 different	 architectures,	 from	 multiple
separate	computers	working	together,	to	multiple	processors	sharing	memory,	to	multiple	cores	integrated
onto	the	same	chip.	Parallel	processors	are	technically	not	classified	as	von	Neumann	machines	because
they	do	not	process	instructions	sequentially.	However,	many	argue	that	parallel	processing	computers
contain	CPUs,	use	program	counters,	and	store	both	programs	and	data	in	main	memory,	which	makes
them	more	like	an	extension	to	the	von	Neumann	architecture	rather	than	a	departure	from	it;	these	people
view	parallel	processing	computers	as	sets	of	cooperating	von	Neumann	machines.	In	this	regard,	perhaps
it	is	more	appropriate	to	say	that	parallel	processing	exhibits	“non–von	Neumannness.”	Regardless	of
how	parallel	processors	are	classified,	parallel	computing	allows	us	to	multitask	and	to	solve	larger	and
more	complex	problems,	and	is	driving	new	research	in	various	software	tools	and	programming.
Even	parallel	computing	has	its	limits,	however.	As	the	number	of	processors	increases,	so	does	the
overhead	of	managing	how	tasks	are	distributed	to	those	processors.	Some	parallel	processing	systems
require	extra	processors	just	to	manage	the	rest	of	the	processors	and	the	resources	assigned	to	them.	No
matter	how	many	processors	we	place	in	a	system,	or	how	many	resources	we	assign	to	them,	somehow,
somewhere,	a	bottleneck	is	bound	to	develop.	The	best	we	can	do,	however,	is	make	sure	the	slowest
parts	of	the	system	are	the	ones	that	are	used	the	least.	This	is	the	idea	behind	Amdahl’s	Law.	This	law
states	that	the	performance	enhancement	possible	with	a	given	improvement	is	limited	by	the	amount	that
the	improved	feature	is	used.	The	underlying	premise	is	that	every	algorithm	has	a	sequential	part	that
ultimately	limits	the	speedup	that	can	be	achieved	by	multiprocessor	implementation.
If	parallel	machines	and	other	non–von	Neumann	architectures	give	such	huge	increases	in	processing
speed	and	power,	why	isn’t	everyone	using	them	everywhere?	The	answer	lies	in	their	programmability.
Advances	in	operating	systems	that	can	utilize	multiple	cores	have	put	these	chips	in	laptops	and	desktops
that	 we	 can	 buy	 today;	 however,	 true	 multiprocessor	 programming	 is	 more	 complex	 than	 both
uniprocessor	and	multicore	programming	and	requires	people	to	think	about	problems	in	a	different	way,
using	new	algorithms	and	programming	tools.
One	of	these	programming	tools	is	a	set	of	new	programming	languages.	Most	of	our	programming
languages	 are	 von	 Neumann	 languages,	 created	 for	 the	 von	 Neumann	 architecture.	 Many	 common
languages	have	been	extended	with	special	libraries	to	accommodate	parallel	programming,	and	many

new	languages	have	been	designed	specifically	for	the	parallel	programming	environment.	We	have	very
few	 programming	 languages	 for	 the	 remaining	 (nonparallel)	 non–von	 Neumann	 platforms,	 and	 fewer
people	who	really	understand	how	to	program	in	these	environments	efficiently.	Examples	of	non–von
Neumann	languages	include	Lucid	(for	dataflow)	and	QCL	(Quantum	Computation	Language)	for	quantum
computers,	as	well	as	VHDL	or	Verilog	(languages	used	to	program	FPGAs).	However,	even	with	the
inherent	difficulties	in	programming	parallel	machines,	we	see	in	the	next	section	that	significant	progress
is	being	made.
1.11		PARALLELISM:	ENABLER	OF	MACHINE	INTELLIGENCE
—DEEP	BLUE	AND	WATSON
It	 is	 evident	 by	 our	 sidebar	 on	 the	 Mechanical	 Turk	 that	 chess	 playing	 has	 long	 been	 considered	 the
ultimate	demonstration	of	a	“thinking	machine.”	The	chess-board	is	a	battlefield	where	human	can	meet
machine	on	more-or-less	equal	terms—with	the	human	always	having	the	edge,	of	course.	Real	chess-
playing	computers	have	been	around	since	the	late	1950s.	Over	the	decades,	they	gradually	improved
their	hardware	and	software	to	eventually	become	formidable	opponents	for	reasonably	skilled	players.
The	 problem	 of	championship	 chess	 playing,	 however,	 had	 long	 been	 considered	 so	 hard	 that	 many
believed	a	machine	could	never	beat	a	human	Grandmaster.	On	May	11,	1997,	a	machine	called	Deep
Blue	did	just	that.
Deep	Blue’s	principal	designers	were	IBM	researchers	Feng-hsiung	Hsu,	Thomas	Anantharaman,	and
Murray	Campbell.	Reportedly	costing	more	than	$6	million	and	taking	six	years	to	build,	Deep	Blue	was
a	massively	parallel	system	consisting	of	30	RS/6000-based	nodes	supplemented	with	480	chips	built
especially	 to	 play	 chess.	 Deep	 Blue	 included	 a	 database	 of	 700,000	 complete	 games	 with	 separate
systems	 for	 opening	 and	 endgames.	 It	 evaluated	 200	 million	 positions	 per	 second	 on	 average.	 This
enabled	Deep	Blue	to	produce	a	12-move	look	ahead.
Having	soundly	beat	an	earlier	version	of	Deep	Blue,	world	chess	champion	Garry	Kasparov	was
overwhelmingly	favored	to	win	a	rematch	starting	May	3,	1997.	At	the	end	of	five	games,	Kasparov	and
Deep	Blue	were	tied,	2½	to	2½.	Then	Deep	Blue	quickly	seized	upon	an	error	that	Kasparov	made	early
in	the	sixth	game.	Kasparov	had	no	choice	but	to	concede,	thus	making	Deep	Blue	the	first	machine	to
ever	defeat	a	chess	Grandmaster.
With	Deep	Blue’s	stunning	win	over	Kasparov	now	in	the	history	books,	IBM	Research	manager
Charles	Lickel	began	looking	for	a	new	challenge.	In	2004,	Lickel	was	among	the	millions	mesmerized	by
Ken	 Jennings’s	 unprecedented	 74-game	 winning	 streak	 on	 the	 American	 quiz	 show,	Jeopardy!	 As	 he
watched	 Jennings	 win	 one	 match	 after	 another,	 Lickel	 dared	 to	 think	 that	 it	 was	 possible	 to	 build	 a
machine	that	could	win	at	Jeopardy!	Moreover,	he	believed	that	IBM	Research	had	the	talent	to	build
such	a	machine.	He	tapped	Dr.	David	Ferrucci	to	lead	the	effort.
IBM	scientists	were	in	no	rush	to	sign	on	to	Lickel’s	audacious	project.	They	doubted—with	good
reason—that	 such	 a	 machine	 could	 be	 built.	 After	 all,	 creating	 Deep	 Blue	 was	 hard	 enough.	 Playing
Jeopardy!	 is	 enormously	 more	 difficult	 than	 playing	 chess.	 In	 chess,	 the	 problem	 domain	 is	 clearly
defined	 with	 fixed,	unambiguous	 rules,	 and	 a	 finite	 (although	 very	 large)	 solution	 space.	Jeopardy!
questions,	on	the	other	hand,	cover	a	nearly	infinite	problem	space	compounded	by	the	vagaries	of	human
language,	odd	relations	between	concepts,	puns,	and	vast	amounts	of	unstructured	factual	information.	For
example,	a	Jeopardy!	category	could	be	titled	“Doozy	Twos”	and	relate	to	an	African	leader,	an	article	of
clothing,	an	Al	Jolson	song,	and	an	ammunition	size	(Benjamin	Tutu,	tutu	skirt,	“Toot	Toot	Tootsie,”	and

.22	caliber).	Whereas	a	human	being	has	little	trouble	seeing	the	relationship	(especially	once	the	answer
is	revealed),	computers	are	utterly	baffled.
To	make	the	game	fair,	Watson	had	to	emulate	a	human	player	as	closely	as	possible.	No	connection	to
the	Internet	or	any	other	computers	was	permitted,	and	Watson	was	required	to	physically	press	a	plunger
to	“buzz	in”	with	an	answer.	However,	Watson	wasn’t	programmed	to	process	sound	or	images,	so	visual
and	strictly	audio	clues—such	as	musical	selections—were	not	used	during	the	match.
Once	a	clue	was	read,	Watson	initiated	several	parallel	processes.	Each	process	examined	different
aspects	 of	 the	 clue,	 narrowed	 the	 solution	 space,	 and	 formulated	 a	 hypothesis	 as	 to	 the	 answer.	 The
hypothesis	included	a	probability	of	its	being	correct.	Watson	selected	the	most	likely	of	the	hypotheses,
or	selected	no	hypothesis	at	all	if	the	probability	of	correctness	didn’t	reach	a	predetermined	threshold.
Watson’s	designers	determined	that	if	Watson	were	to	attempt	just	70%	of	the	questions	and	respond
correctly	just	85%	of	the	time,	it	would	win	the	contest.	No	human	players	had	ever	done	as	well.
Using	Watson’s	algorithms,	a	typical	desktop	computer	would	need	about	two	hours	to	come	up	with	a
good	hypothesis.	Watson	had	to	do	it	in	less	than	three	seconds.	It	achieved	this	feat	through	a	massively
parallel	 architecture	 dubbed	DeepQA	 (Deep	 Question	 and	 Answer).	 The	 system	 relied	 on	 90	 IBM
POWER	750	servers.	Each	server	was	equipped	with	four	POWER7	processors,	and	each	POWER7
processor	had	eight	cores,	giving	a	total	of	2880	processor	cores.	While	playing	Jeopardy!,	each	core
had	access	to	16TB	of	main	memory	and	4TB	of	clustered	storage.
Unlike	 Deep	 Blue,	 Watson	 could	 not	 be	 programmed	 to	 solve	 problems	 through	 brute	 force:	 The
problem	 space	 was	 much	 too	 large.	 Watson’s	 designers,	 therefore,	 approached	 the	 situation	 just	 as	 a
human	being	would:	Watson	“learned”	by	consuming	terabytes	of	unstructured	data	from	thousands	of
news	 sources,	 journals,	 and	 books.	 The	 DeepQA	 algorithms	 provided	 Watson	 with	 the	 ability	 to
synthesize	information—in	a	humanlike	manner—from	this	universe	of	raw	data.	Watson	drew	inferences
and	 made	 assumptions	 using	 hard	 facts	 and	 incomplete	 information.	 Watson	 could	 see	 information	 in
context:	The	same	question,	in	a	different	context,	might	well	produce	a	different	answer.
On	the	third	day	of	its	match,	February	16,	2011,	Watson	stunned	the	world	by	soundly	beating	both
reigning	Jeopardy!	champs,	Ken	Jennings	and	Brad	Rutter.	Watson’s	winnings	were	donated	to	charity,
but	Watson’s	service	to	humanity	was	only	beginning.	Watson’s	ability	to	absorb	and	draw	inferences
from	pools	of	unstructured	data	made	it	a	perfect	candidate	for	medical	school.	Beginning	in	2011,	IBM,
WellPoint,	 and	 Memorial	 Sloan-Kettering	 Cancer	 Center	 set	 Watson	 to	 work	 absorbing	 more	 than
600,000	 pieces	 of	 medical	 evidence,	 and	 two	 million	 pages	 of	 text	 from	 42	 medical	 journals	 and
oncology	research	documents.	Watson’s	literature	assimilation	was	supplemented	with	14,700	hours	of
live	training	provided	by	WellPoint	nurses.	Watson	was	then	given	25,000	test	case	scenarios	and	1500
real-life	 cases	 from	 which	 it	 demonstrated	 that	 it	 had	 gained	 the	 ability	 to	 derive	 meaning	 from	 the
mountain	of	complex	medical	data,	some	of	which	was	in	informal	natural	language—such	as	doctors’
notes,	patient	records,	medical	annotations,	and	clinical	feedback.	Watson’s	Jeopardy!	success	has	now
been	matched	by	its	medical	school	success.	Commercial	products	based	on	Watson	technology,	including
“Interactive	Care	Insights	for	Oncology”	and	“Interactive	Care	Reviewer,”	are	now	available.	They	hold
the	promise	to	improve	the	speed	and	accuracy	of	medical	care	for	cancer	patients.
Although	 Watson’s	 applications	 and	 abilities	 have	 been	 growing,	 Watson’s	 footprint	 has	 been
shrinking.	 In	 the	 span	 of	 only	 a	 few	 years,	 system	 performance	 has	 improved	 by	 240%	 with	 a	 75%
reduction	in	physical	resources.	Watson	can	now	be	run	on	a	single	POWER	750	server,	leading	some	to
claim	that	“Watson	on	a	chip”	is	just	around	the	corner.
In	Watson,	we	have	not	merely	seen	an	amazing	Jeopardy!	player	or	crack	oncologist.	What	we	have
seen	is	the	future	of	computing.	Rather	than	people	being	trained	to	use	computers,	computers	will	train

themselves	to	interact	with	people—with	all	their	fuzzy	and	incomplete	information.	Tomorrow’s	systems
will	meet	humans	on	human	terms.	As	Dr.	Ferrucci	puts	it,	there	simply	is	no	other	future	for	computers
except	to	become	like	Watson.	It	just	has	to	be	this	way.
CHAPTER	SUMMARY
In	this	chapter,	we	have	presented	a	brief	overview	of	computer	organization	and	computer	architecture
and	 shown	 how	 they	 differ.	 We	 also	 have	 introduced	 some	 terminology	 in	 the	 context	 of	 a	 fictitious
computer	advertisement.	Much	of	this	terminology	will	be	expanded	on	in	later	chapters.
Historically,	computers	were	simply	calculating	machines.	As	computers	became	more	sophisticated,
they	became	general-purpose	machines,	which	necessitated	viewing	each	system	as	a	hierarchy	of	levels
instead	of	one	gigantic	machine.	Each	layer	in	this	hierarchy	serves	a	specific	purpose,	and	all	levels	help
minimize	the	semantic	gap	between	a	high-level	programming	language	or	application	and	the	gates	and
wires	that	make	up	the	physical	hardware.	Perhaps	the	single	most	important	development	in	computing
that	affects	us	as	programmers	is	the	introduction	of	the	stored-program	concept	of	the	von	Neumann
machine.	Although	there	are	other	architectural	models,	the	von	Neumann	architecture	is	predominant	in
today’s	general-purpose	computers.
FURTHER	READING
We	encourage	you	to	build	on	our	brief	presentation	of	the	history	of	computers.	We	think	you	will	find
this	subject	intriguing	because	it	is	as	much	about	people	as	it	is	about	machines.	You	can	read	about	the
“forgotten	father	of	the	computer,”	John	Atanasoff,	in	Mollenhoff	(1988).	This	book	documents	the	odd
relationship	between	Atanasoff	and	John	Mauchly,	and	recounts	the	open	court	battle	of	two	computer
giants,	Honeywell	and	Sperry	Rand.	This	trial	ultimately	gave	Atanasoff	his	proper	recognition.
For	a	lighter	look	at	computer	history,	try	the	book	by	Rochester	and	Gantz	(1983).	Augarten’s	(1985)
illustrated	history	of	computers	is	a	delight	to	read	and	contains	hundreds	of	hard-to-find	pictures	of	early
computers	and	computing	devices.	For	a	complete	discussion	of	the	historical	development	of	computers,
you	can	check	out	the	three-volume	dictionary	by	Cortada	(1987).	A	particularly	thoughtful	account	of	the
history	of	computing	is	presented	in	Ceruzzi	(1998).	If	you	are	interested	in	an	excellent	set	of	case
studies	about	historical	computers,	see	Blaauw	and	Brooks	(1997).
You	will	also	be	richly	rewarded	by	reading	McCartney’s	(1999)	book	about	the	ENIAC,	Chopsky
and	Leonsis’s	(1988)	chronicle	of	the	development	of	the	IBM	PC,	and	Toole’s	(1998)	biography	of	Ada,
Countess	of	Lovelace.	Polachek’s	(1997)	article	conveys	a	vivid	picture	of	the	complexity	of	calculating
ballistic	firing	tables.	After	reading	this	article,	you	will	understand	why	the	army	would	gladly	pay	for
anything	that	promised	to	make	the	process	faster	or	more	accurate.	The	Maxfield	and	Brown	book	(1997)
contains	a	fascinating	look	at	the	origins	and	history	of	computing	as	well	as	in-depth	explanations	of	how
a	computer	works.
For	 more	 information	 on	 Moore’s	 Law,	 we	 refer	 the	 reader	 to	 Schaller	 (1997).	 For	 detailed
descriptions	of	early	computers	as	well	as	profiles	and	reminiscences	of	industry	pioneers,	you	may	wish
to	consult	the	IEEE	Annals	of	the	History	of	Computing,	which	is	published	quarterly.	The	Computer
Museum	History	Center	can	be	found	online	at	www.computerhistory.org.	It	contains	various	exhibits,
research,	timelines,	and	collections.	Many	cities	now	have	computer	museums	and	allow	visitors	to	use
some	of	the	older	computers.
A	wealth	of	information	can	be	found	at	the	websites	of	the	standards-making	bodies	discussed	in	this

chapter	(as	well	as	sites	not	discussed	in	this	chapter).	The	IEEE	can	be	found	at	www.ieee.org;	ANSI	at
www.ansi.org;	the	ISO	at	www.iso.ch;	the	BSI	at	www.bsi-global.com;	and	the	ITU-T	at	www.itu.int.
The	ISO	site	offers	a	vast	amount	of	information	and	standards	reference	materials.
The	 WWW	 Computer	 Architecture	 Home	 Page	 at	www.cs.wisc.edu/~arch/www/	 contains	 a
comprehensive	 index	 to	 computer	 architecture–related	 information.	 Many	 USENET	 newsgroups	 are
devoted	to	these	topics	as	well,	including	comp.arch	and	comp.arch.storage.
The	entire	May–June	2000	issue	of	MIT’s	Technology	Review	magazine	is	devoted	to	architectures
that	may	be	the	basis	of	tomorrow’s	computers.	Reading	this	issue	will	be	time	well	spent.	In	fact,	we
could	say	the	same	of	every	issue.
For	a	truly	unique	account	of	human	computers,	we	invite	you	to	read	Grier’s	When	Computers	Were
Human.	 Among	 other	 things,	 he	 presents	 a	 stirring	 account	 of	 the	 human	 computers	 who	 drove	 the
mathematical	 tables	 project	 under	 the	 Depression-era	 Works	 Progress	 Administration	 (WPA).	 The
contributions	 made	 by	 these	 “table	 factories”	 were	 crucial	 to	 America’s	 victory	 in	 World	 War	 II.	 A
shorter	account	of	this	effort	can	also	be	found	in	Grier’s	1998	article	that	appears	in	the	IEEE	Annals	of
the	History	of	Computing.
The	entire	May–June	2012	issue	of	the	IBM	Journal	of	Research	and	Development	is	dedicated	to
the	building	of	Watson.	The	two	articles	by	Ferrucci	and	Lewis	give	great	insight	into	the	challenges	and
triumphs	 of	 this	 groundbreaking	 machine.	 The	 IBM	 whitepaper,	 “Watson—A	 System	 Designed	 for
Answers,”	provides	a	nice	summary	of	Watson’s	hardware	architecture.	Feng-hsiung	Hsu	gives	his	first-
person	account	of	the	building	of	Deep	Blue	in	Behind	Deep	Blue:	Building	the	Computer	that	Defeated
the	World	Chess	Champion.	Readers	interested	in	the	Mechanical	Turk	can	find	more	information	in	the
book	of	the	same	name	by	Tom	Standage.
REFERENCES
Augarten,	S.	Bit	by	Bit:	An	Illustrated	History	of	Computers.	London:	Unwin	Paperbacks,	1985.
Blaauw,	G.,	&	Brooks,	F.	Computer	Architecture:	Concepts	and	Evolution.	Reading,	MA:	Addison-
Wesley,	1997.
Ceruzzi,	P.	E.	A	History	of	Modern	Computing.	Cambridge,	MA:	MIT	Press,	1998.
Chopsky,	J.,	&	Leonsis,	T.	Blue	Magic:	The	People,	Power	and	Politics	Behind	the	IBM	Personal
Computer.	New	York:	Facts	on	File	Publications,	1988.
Cortada,	J.	W.	Historical	Dictionary	of	Data	Processing,	Volume	1:	Biographies;	Volume	2:
Organization;	Volume	3:	Technology.	Westport,	CT:	Greenwood	Press,	1987.
Ferrucci,	D.	A.,	“Introduction	to	‘This	is	Watson.’	”	IBM	Journal	of	Research	and	Development	56:3/4,
May–June	2012,	pp.	1:1–1:15.
Grier,	D.	A.	“The	Math	Tables	Project	of	the	Work	Projects	Administration:	The	Reluctant	Start	of	the
Computing	Era.”	IEEE	Annals	of	the	History	of	Computing	20:3,	July–Sept.	1998,	pp.	33–50.
Grier,	D.	A.	When	Computers	Were	Human.	Princeton,	NJ:	Princeton	University	Press,	2007.
Hsu,	F.-h.	Behind	Deep	Blue:	Building	the	Computer	that	Defeated	the	World	Chess	Champion.
Princeton,	NJ:	Princeton	University	Press,	2006.
IBM.	“Watson—A	System	Designed	for	Answers:	The	future	of	workload	optimized	systems	design.”
February	2011.	ftp://public.dhe.ibm.com/common/ssi/ecm/en/pow03061usen/POW-
03061USEN.PDF.	Retrieved	June	4,	2013.

Lewis,	B.	L.	“In	the	game:	The	interface	between	Watson	and	Jeopardy!”	IBM	Journal	of	Research	and
Development	56:3/4,	May–June	2012,	pp.	17:1–17:6.
Maguire,	Y.,	Boyden	III,	E.	S.,	&	Gershenfeld,	N.	“Toward	a	Table-Top	Quantum	Computer.”	IBM
Systems	Journal	39:3/4,	June	2000,	pp.	823–839.
Maxfield,	C.,	&	Brown,	A.	Bebop	BYTES	Back	(An	Unconventional	Guide	to	Computers).	Madison,	AL:
Doone	Publications,	1997.
McCartney,	S.	ENIAC:	The	Triumphs	and	Tragedies	of	the	World’s	First	Computer.	New	York:	Walker
and	Company,	1999.
Mollenhoff,	C.	R.	Atanasoff:	The	Forgotten	Father	of	the	Computer.	Ames,	IA:	Iowa	State	University
Press,	1988.
Polachek,	H.	“Before	the	ENIAC.”	IEEE	Annals	of	the	History	of	Computing	19:2,	June	1997,	pp.	25–
30.
Rochester,	J.	B.,	&	Gantz,	J.	The	Naked	Computer:	A	Layperson’s	Almanac	of	Computer	Lore,	Wizardry,
Personalities,	Memorabilia,	World	Records,	Mindblowers,	and	Tomfoolery.	New	York:	William	A.
Morrow,	1983.
Schaller,	R.	“Moore’s	Law:	Past,	Present,	and	Future.”	IEEE	Spectrum,	June	1997,	pp.	52–59.
Stallings,	W.	Operating	Systems:	Internals	and	Design	Principles,	7th	ed.	Upper	Saddle	River,	NJ:
Prentice	Hall,	2012.
Standage,	T.	The	Turk:	The	Life	and	Times	of	the	Famous	Eighteenth-Century	Chess-Playing	Machine.
New	York:	Berkley	Trade,	2003.
Tanenbaum,	A.	Structured	Computer	Organization,	6th	ed.	Upper	Saddle	River,	NJ:	Prentice	Hall,	2013.
Toole,	B.	A.	Ada,	the	Enchantress	of	Numbers:	Prophet	of	the	Computer	Age.	Mill	Valley,	CA:
Strawberry	Press,	1998.
Waldrop,	M.	M.	“Quantum	Computing.”	MIT	Technology	Review	103:3,	May/June	2000,	pp.	60–66.
REVIEW	OF	ESSENTIAL	TERMS	AND	CONCEPTS
1.		What	is	the	difference	between	computer	organization	and	computer	architecture?
2.		What	is	an	ISA?
3.		What	is	the	importance	of	the	Principle	of	Equivalence	of	Hardware	and	Software?
4.		Name	the	three	basic	components	of	every	computer.
5.		To	what	power	of	10	does	the	prefix	giga-	refer?	What	is	the	(approximate)	equivalent	power	of	2?
6.		To	what	power	of	10	does	the	prefix	micro-	refer?	What	is	the	(approximate)	equivalent	power	of	2?
7.		What	unit	is	typically	used	to	measure	the	speed	of	a	computer	clock?
8.		What	are	the	distinguishing	features	of	tablet	computers?
9.		Name	two	types	of	computer	memory.
10.		What	is	the	mission	of	the	IEEE?
11.		What	is	the	full	name	of	the	organization	that	uses	the	initials	ISO?	Is	ISO	an	acronym?

12.		ANSI	is	the	acronym	used	by	which	organization?
13.	 	 What	 is	 the	 name	 of	 the	 Swiss	 organization	 that	 devotes	 itself	 to	 matters	 concerning	 telephony,
telecommunications,	and	data	communications?
14.		Who	is	known	as	the	father	of	computing,	and	why?
15.		What	was	the	significance	of	the	punched	card?
16.		Name	two	driving	factors	in	the	development	of	computers.
17.		What	is	it	about	the	transistor	that	made	it	such	a	great	improvement	over	the	vacuum	tube?
18.		How	does	an	integrated	circuit	differ	from	a	transistor?
19.		Explain	the	differences	between	SSI,	MSI,	LSI,	and	VLSI.
20.		What	technology	spawned	the	development	of	microcomputers?	Why?
21.		What	is	meant	by	an	“open	architecture”?
22.		State	Moore’s	Law.
23.		How	is	Rock’s	Law	related	to	Moore’s	Law?
24.		Name	and	explain	the	seven	commonly	accepted	layers	of	the	Computer	Level	Hierarchy.	How	does
this	arrangement	help	us	to	understand	computer	systems?
25.		How	does	the	term	abstraction	apply	to	computer	organization	and	architecture?
26.		What	was	it	about	the	von	Neumann	architecture	that	distinguished	it	from	its	predecessors?
27.		Name	the	characteristics	present	in	von	Neumann	architecture.
28.		How	does	the	fetch-decode-execute	cycle	work?
29.		What	is	a	multicore	processor?
30.		What	are	the	key	characteristics	of	Cloud	computing?
31.		What	are	the	three	types	of	Cloud	computing	platforms?
32.		What	are	the	main	challenges	of	Cloud	computing	from	a	provider	perspective	as	well	as	a	consumer
perspective?
33.		What	are	the	advantages	and	disadvantages	of	service-oriented	computing?
34.		What	is	meant	by	parallel	computing?
35.		What	is	the	underlying	premise	of	Amdahl’s	Law?
36.		What	makes	Watson	so	different	from	traditional	computers?
EXERCISES
	1.		In	what	ways	are	hardware	and	software	different?	In	what	ways	are	they	the	same?
2.		a)		How	many	milliseconds	(ms)	are	in	1	second?
b)		How	many	microseconds	(μs)	are	in	1	second?
c)		How	many	nanoseconds	(ns)	are	in	1	millisecond?
d)		How	many	microseconds	are	in	1	millisecond?

e)		How	many	nanoseconds	are	in	1	microsecond?
f)		How	many	kilobytes	(KB)	are	in	1	gigabyte	(GB)?
g)		How	many	kilobytes	are	in	1	megabyte	(MB)?
h)		How	many	megabytes	are	in	1	gigabyte?
i)		How	many	bytes	are	in	20	megabytes?
j)		How	many	kilobytes	are	in	2	gigabytes?
	3.		By	what	order	of	magnitude	is	something	that	runs	in	nanoseconds	faster	than	something	that	runs	in
milliseconds?
4.		Pretend	you	are	ready	to	buy	a	new	computer	for	personal	use.	First,	take	a	look	at	ads	from
various	magazines	and	newspapers	and	list	terms	you	don’t	quite	understand.	Look	up	these	terms
and	give	a	brief	written	explanation.	Decide	what	factors	are	important	in	your	decision	as	to
which	computer	to	buy	and	list	them.	After	you	select	the	system	you	would	like	to	buy,	identify
which	terms	refer	to	hardware	and	which	refer	to	software.
5.	 	 Makers	 of	 tablet	 computers	 continually	 work	 within	 narrow	 constraints	 on	 cost,	 power
consumption,	 weight,	 and	 battery	 life.	 Describe	 what	 you	 feel	 would	 be	 the	 perfect	 tablet
computer.	How	large	would	the	screen	be?	Would	you	rather	have	a	longer-lasting	battery,	even	if
it	means	having	a	heavier	unit?	How	heavy	would	be	too	heavy?	Would	you	rather	have	low	cost
or	fast	performance?	Should	the	battery	be	consumer	replaceable?
6.		Pick	your	favorite	computer	language	and	write	a	small	program.	After	compiling	the	program,	see
if	you	can	determine	the	ratio	of	source	code	instructions	to	the	machine	language	instructions
generated	by	the	compiler.	If	you	add	one	line	of	source	code,	how	does	that	affect	the	machine
language	 program?	 Try	 adding	 different	 source	 code	 instructions,	 such	 as	 an	 add	 and	 then	 a
multiply.	 How	 does	 the	 size	 of	 the	 machine	 code	 file	 change	 with	 the	 different	 instructions?
Comment	on	the	result.
7.		Respond	to	the	idea	presented	in	Section	1.5:	If	invented	today,	what	name	do	you	think	would	be
given	to	the	computer?	Give	at	least	one	good	reason	for	your	answer.
8.		Briefly	explain	two	breakthroughs	in	the	history	of	computing.
9.		Would	it	be	possible	to	fool	people	with	an	automaton	like	the	Mechanical	Turk	today?	If	you	were
to	try	to	create	a	Turk	today,	how	would	it	differ	from	the	eighteenth-century	version?
	10.		Suppose	a	transistor	on	an	integrated	circuit	chip	were	2	microns	in	size.	According	to	Moore’s
Law,	 how	 large	 would	 that	 transistor	 be	 in	 2	 years?	 How	 is	 Moore’s	 Law	 relevant	 to
programmers?
11.		What	circumstances	helped	the	IBM	PC	become	so	successful?
12.		List	five	applications	of	personal	computers.	Is	there	a	limit	to	the	applications	of	computers?	Do
you	envision	any	radically	different	and	exciting	applications	in	the	near	future?	If	so,	what?
13.		In	the	von	Neumann	model,	explain	the	purpose	of	the:
a)		processing	unit
b)		program	counter
14.		Under	the	von	Neumann	architecture,	a	program	and	its	data	are	both	stored	in	memory.	It	is
therefore	possible	for	a	program,	thinking	that	a	memory	location	holds	a	piece	of	data	when	it

actually	 holds	 a	 program	 instruction,	 to	 accidentally	 (or	 on	 purpose)	 modify	 itself.	 What
implications	does	this	present	to	you	as	a	programmer?
15.		Explain	why	modern	computers	consist	of	multiple	levels	of	virtual	machines.
16.		Explain	the	three	main	types	of	Cloud	computing	platforms.
17.		What	are	the	challenges	facing	organizations	that	wish	to	move	to	a	Cloud	platform?	What	are	the
risks	and	benefits?
18.	 	 Does	 Cloud	 computing	 eliminate	 all	 of	 an	 organization’s	 concerns	 about	 its	 computing
infrastructure?
19.		Explain	what	it	means	to	“fetch”	an	instruction.
20.		Read	a	popular	local	newspaper	and	search	through	the	job	openings.	(You	can	also	check	some	of
the	more	popular	online	career	sites.)	Which	jobs	require	specific	hardware	knowledge?	Which
jobs	 imply	 knowledge	 of	 computer	 hardware?	 Is	 there	 any	 correlation	 between	 the	 required
hardware	knowledge	and	the	company	or	its	location?
21.		List	and	describe	some	common	uses	and	some	not-so-common	uses	of	computers	in	business	and
other	sectors	of	society.
22.	 	 The	 technologist’s	 notion	 of	 Moore’s	 Law	 is	 that	 the	 number	 of	 transistors	 per	 chip	 doubles
approximately	every	18	months.	In	the	1990s,	Moore’s	Law	started	to	be	described	as	the	doubling
of	microprocessor	power	every	18	months.	Given	this	new	variation	of	Moore’s	Law,	answer	the
following:
a)		After	successfully	completing	your	computer	organization	and	architecture	class,	you	have	a
brilliant	idea	for	a	new	chip	design	that	would	make	a	processor	six	times	faster	than	the	fastest
ones	on	the	market	today.	Unfortunately,	it	will	take	you	four	and	a	half	years	to	save	the	money,
create	the	prototype,	and	build	a	finished	product.	If	Moore’s	Law	holds,	should	you	spend
your	money	developing	and	producing	your	chip	or	invest	in	some	other	venture?
b)		Suppose	we	have	a	problem	that	currently	takes	100,000	hours	of	computer	time	using	current
technology	to	solve.	Which	of	the	following	would	give	us	the	solution	first:	(1)	Replace	the
algorithm	used	in	the	current	solution	with	one	that	runs	twice	as	fast	and	run	it	on	the	current
technology,	or	(2)	Wait	3	years,	assuming	Moore’s	Law	doubles	the	performance	of	a	computer
every	18	months,	and	find	the	solution	using	the	current	algorithm	with	the	new	technology?
23.		What	are	the	limitations	of	Moore’s	Law?	Why	can’t	this	law	hold	forever?	Explain.
24.		What	are	some	technical	implications	of	Moore’s	Law?	What	effect	does	it	have	on	your	future?
25.		Do	you	share	Dr.	Ferrucci’s	opinion	that	all	computers	will	become	like	Watson	someday?	If	you
had	a	tablet-sized	Watson,	what	would	you	do	with	it?

1
	What	this	principle	does	not	address	is	the	speed	with	which	the	equivalent	tasks	are	carried	out.	Hardware	implementations	are	almost
always	faster.

There	are	10	kinds	of	people	in	the	world—those	who	understand	binary	and	those	who
don’t.
—Anonymous
CHAPTER	2

Data	Representation	in	Computer	Systems
2.1			INTRODUCTION
The	organization	of	any	computer	depends	considerably	on	how	it	represents	numbers,	characters,	and
control	information.	The	converse	is	also	true:	Standards	and	conventions	established	over	the	years	have
determined	certain	aspects	of	computer	organization.	This	chapter	describes	the	various	ways	in	which
computers	 can	 store	 and	 manipulate	 numbers	 and	 characters.	 The	 ideas	 presented	 in	 the	 following
sections	form	the	basis	for	understanding	the	organization	and	function	of	all	types	of	digital	systems.
The	most	basic	unit	of	information	in	a	digital	computer	is	called	a	bit,	which	is	a	contraction	of
binary	digit.	In	the	concrete	sense,	a	bit	is	nothing	more	than	a	state	of	“on”	or	“off”	(or	“high”	and
“low”)	within	a	computer	circuit.	In	1964,	the	designers	of	the	IBM	System/360	mainframe	computer
established	a	convention	of	using	groups	of	8	bits	as	the	basic	unit	of	addressable	computer	storage.	They
called	this	collection	of	8	bits	a	byte.
Computer	words	 consist	 of	 two	 or	 more	 adjacent	 bytes	 that	 are	 sometimes	 addressed	 and	 almost
always	 are	 manipulated	 collectively.	 The	word	 size	 represents	 the	 data	 size	 that	 is	 handled	 most
efficiently	by	a	particular	architecture.	Words	can	be	16	bits,	32	bits,	64	bits,	or	any	other	size	that	makes
sense	in	the	context	of	a	computer’s	organization	(including	sizes	that	are	not	multiples	of	eight).	An	8-bit
byte	can	be	divided	into	two	4-bit	halves	called	nibbles	(or	nybbles).	Because	each	bit	of	a	byte	has	a
value	within	a	positional	numbering	system,	the	nibble	containing	the	least-valued	binary	digit	is	called
the	low-order	nibble,	and	the	other	half	the	high-order	nibble.
2.2	POSITIONAL	NUMBERING	SYSTEMS
At	some	point	during	the	middle	of	the	sixteenth	century,	Europe	embraced	the	decimal	(or	base	10)
numbering	system	that	the	Arabs	and	Hindus	had	been	using	for	nearly	a	millennium.	Today,	we	take	for
granted	that	the	number	243	means	two	hundreds,	plus	four	tens,	plus	three	units.	Notwithstanding	the	fact
that	zero	means	“nothing,”	virtually	everyone	knows	that	there	is	a	substantial	difference	between	having
1	of	something	and	having	10	of	something.
The	general	idea	behind	positional	numbering	systems	is	that	a	numeric	value	is	represented	through
increasing	 powers	 of	 a	radix	 (or	 base).	 This	 is	 often	 referred	 to	 as	 a	weighted	numbering	system
because	each	position	is	weighted	by	a	power	of	the	radix.
The	set	of	valid	numerals	for	a	positional	numbering	system	is	equal	in	size	to	the	radix	of	that	system.
For	example,	there	are	10	digits	in	the	decimal	system,	0	through	9,	and	3	digits	for	the	ternary	(base	3)
system,	0,	1,	and	2.	The	largest	valid	number	in	a	radix	system	is	one	smaller	than	the	radix,	so	8	is	not	a
valid	numeral	in	any	radix	system	smaller	than	9.	To	distinguish	among	numbers	in	different	radices,	we
use	the	radix	as	a	subscript,	such	as	in	33
10
	to	represent	the	decimal	number	33.	(In	this	text,	numbers
written	without	a	subscript	should	be	assumed	to	be	decimal.)	Any	decimal	integer	can	be	expressed
exactly	in	any	other	integral	base	system	(see	Example	2.1).
	EXAMPLE	2.1	Three	numbers	represented	as	powers	of	a	radix.

The	two	most	important	radices	in	computer	science	are	binary	(base	two),	and	hexadecimal	(base	16).
Another	radix	of	interest	is	octal	(base	8).	The	binary	system	uses	only	the	digits	0	and	1;	the	octal
system,	0	through	7.	The	hexadecimal	system	allows	the	digits	0	through	9	with	A,	B,	C,	D,	E,	and	F	being
used	to	represent	the	numbers	10	through	15.	Table	2.1	shows	some	of	the	radices.
2.3			CONVERTING	BETWEEN	BASES
Gottfried	Leibniz	(1646–1716)	was	the	first	to	generalize	the	idea	of	the	(positional)	decimal	system	to
other	bases.	Being	a	deeply	spiritual	person,	Leibniz	attributed	divine	qualities	to	the	binary	system.	He
correlated	the	fact	that	any	integer	could	be	represented	by	a	series	of	ones	and	zeros	with	the	idea	that
God	(1)	created	the	universe	out	of	nothing	(0).	Until	the	first	binary	digital	computers	were	built	in	the
late	1940s,	this	system	remained	nothing	more	than	a	mathematical	curiosity.	Today,	it	lies	at	the	heart	of
virtually	every	electronic	device	that	relies	on	digital	controls.
TABLE	2.1	Some	Numbers	to	Remember
Because	of	its	simplicity,	the	binary	numbering	system	translates	easily	into	electronic	circuitry.	It	is
also	easy	for	humans	to	understand.	Experienced	computer	professionals	can	recognize	smaller	binary
numbers	(such	as	those	shown	in	Table	2.1)	at	a	glance.	Converting	larger	values	and	fractions,	however,
usually	 requires	 a	 calculator	 or	 pencil	 and	 paper.	 Fortunately,	 the	 conversion	 techniques	 are	 easy	 to
master	with	a	little	practice.	We	show	a	few	of	the	simpler	techniques	in	the	sections	that	follow.
2.3.1		Converting	Unsigned	Whole	Numbers
We	begin	with	the	base	conversion	of	unsigned	numbers.	Conversion	of	signed	numbers	(numbers	that	can
be	positive	or	negative)	is	more	complex,	and	it	is	important	that	you	first	understand	the	basic	technique
for	conversion	before	continuing	with	signed	numbers.
Conversion	between	base	systems	can	be	done	by	using	either	repeated	subtraction	or	a	division-

remainder	method.	The	subtraction	method	is	cumbersome	and	requires	a	familiarity	with	the	powers	of
the	radix	being	used.	Because	it	is	the	more	intuitive	of	the	two	methods,	however,	we	will	explain	it
first.
As	an	example,	let’s	say	we	want	to	convert	104
10
	to	base	3.	We	know	that	3
4
	=	81	is	the	highest
power	of	3	that	is	less	than	104,	so	our	base	3	number	will	be	5	digits	wide	(one	for	each	power	of	the
radix:	0	through	4).	We	make	note	that	81	goes	once	into	104	and	subtract,	leaving	a	difference	of	23.	We
know	that	the	next	power	of	3,	3
3
	=	27,	is	too	large	to	subtract,	so	we	note	the	zero	“placeholder”	and
look	for	how	many	times	3
2
	=	9	divides	23.	We	see	that	it	goes	twice	and	subtract	18.	We	are	left	with	5,
from	which	we	subtract	3
1
	=	3,	leaving	2,	which	is	2	×	3
0
.	These	steps	are	shown	in	Example	2.2.
	EXAMPLE	2.2	Convert	104
10
	to	base	3	using	subtraction.
The	division-remainder	method	is	faster	and	easier	than	the	repeated	subtraction	method.	It	employs	the
idea	that	successive	divisions	by	the	base	are	in	fact	successive	subtractions	by	powers	of	the	base.	The
remainders	that	we	get	when	we	sequentially	divide	by	the	base	end	up	being	the	digits	of	the	result,
which	are	read	from	bottom	to	top.	This	method	is	illustrated	in	Example	2.3.
	EXAMPLE	2.3	Convert	104
10
	to	base	3	using	the	division-remainder	method.
Reading	the	remainders	from	bottom	to	top,	we	have:	104
10
	=	10212
3
.
This	method	works	with	any	base,	and	because	of	the	simplicity	of	the	calculations,	it	is	particularly

useful	in	converting	from	decimal	to	binary.	Example	2.4	shows	such	a	conversion.
	EXAMPLE	2.4	Convert	147
10
	to	binary.
Reading	the	remainders	from	bottom	to	top,	we	have:	147
10
	=	10010011
2
.
A	binary	number	with	N	bits	can	represent	unsigned	integers	from	0	to	2
N
	–	1.	For	example,	4	bits	can
represent	the	decimal	values	0	through	15,	whereas	8	bits	can	represent	the	values	0	through	255.	The
range	of	values	that	can	be	represented	by	a	given	number	of	bits	is	extremely	important	when	doing
arithmetic	 operations	 on	 binary	 numbers.	 Consider	 a	 situation	 in	 which	 binary	 numbers	 are	 4	 bits	 in
length,	and	we	wish	to	add	1111
2
	(15
10
)	to	1111
2
.	 We	 know	 that	 15	 plus	 15	 is	 30,	 but	 30	 cannot	 be
represented	using	only	4	bits.	This	is	an	example	of	a	condition	known	as	overflow,	which	occurs	in
unsigned	binary	representation	when	the	result	of	an	arithmetic	operation	is	outside	the	range	of	allowable
precision	 for	 the	 given	 number	 of	 bits.	 We	 address	 overflow	 in	 more	 detail	 when	 discussing	 signed
numbers	in	Section	2.4.
2.3.2		Converting	Fractions
Fractions	in	any	base	system	can	be	approximated	in	any	other	base	system	using	negative	powers	of	a
radix.	Radix	points	separate	the	integer	part	of	a	number	from	its	fractional	part.	In	the	decimal	system,
the	radix	point	is	called	a	decimal	point.	Binary	fractions	have	a	binary	point.
Fractions	that	contain	repeating	strings	of	digits	to	the	right	of	the	radix	point	in	one	base	may	not
necessarily	have	a	repeating	sequence	of	digits	in	another	base.	For	instance,	⅔	is	a	repeating	decimal
fraction,	but	in	the	ternary	system,	it	terminates	as	0.2
3
	(2	×	3
–1
	=	2	×	⅓).
We	can	convert	fractions	between	different	bases	using	methods	analogous	to	the	repeated	subtraction
and	division-remainder	methods	for	converting	integers.	Example	2.5	shows	how	we	can	use	repeated
subtraction	to	convert	a	number	from	decimal	to	base	5.
	EXAMPLE	2.5	Convert	0.4304
10
	to	base	5.

Reading	from	top	to	bottom,	we	have:	0.4304
10
	=	0.2034
5
.
Because	the	remainder	method	works	with	positive	powers	of	the	radix	for	conversion	of	integers,	it
stands	to	reason	that	we	would	use	multiplication	to	convert	fractions,	because	they	are	expressed	in
negative	powers	of	the	radix.	However,	instead	of	looking	for	remainders,	as	we	did	above,	we	use	only
the	integer	part	of	the	product	after	multiplication	by	the	radix.	The	answer	is	read	from	top	to	bottom
instead	of	bottom	to	top.	Example	2.6	illustrates	the	process.
	EXAMPLE	2.6	Convert	0.4304
10
	to	base	5.
Reading	from	top	to	bottom,	we	have	0.4304
10
	=	0.2034
5
.
This	example	was	contrived	so	that	the	process	would	stop	after	a	few	steps.	Often	things	don’t	work	out
quite	so	evenly,	and	we	end	up	with	repeating	fractions.	Most	computer	systems	implement	specialized
rounding	algorithms	to	provide	a	predictable	degree	of	accuracy.	For	the	sake	of	clarity,	however,	we
will	simply	discard	(or	truncate)	our	answer	when	the	desired	accuracy	has	been	achieved,	as	shown	in
Example	2.7.
	EXAMPLE	2.7	Convert	0.34375
10
	to	binary	with	4	bits	to	the	right	of	the	binary	point.

Reading	from	top	to	bottom,	0.34375
10
	=	0.0101
2
	to	four	binary	places.
The	methods	just	described	can	be	used	to	directly	convert	any	number	in	any	base	to	any	other	base,	say
from	base	4	to	base	3	(as	in	Example	2.8).	However,	in	most	cases,	it	is	faster	and	more	accurate	to	first
convert	to	base	10	and	then	to	the	desired	base.	One	exception	to	this	rule	is	when	you	are	working
between	bases	that	are	powers	of	two,	as	you’ll	see	in	the	next	section.
	EXAMPLE	2.8	Convert	3121
4
	to	base	3.
First,	convert	to	decimal:
Then	convert	to	base	3:
2.3.3		Converting	Between	Power-of-Two	Radices
Binary	numbers	are	often	expressed	in	hexadecimal—and	sometimes	octal—to	improve	their	readability.
Because	 16	 =	 2
4
,	 a	 group	 of	 4	 bits	 (called	 a	hextet)	 is	 easily	 recognized	 as	 a	 hexadecimal	 digit.
Similarly,	with	8	=	2
3
,	a	group	of	3	bits	(called	an	octet)	is	expressible	as	one	octal	digit.	Using	these
relationships,	we	can	therefore	convert	a	number	from	binary	to	octal	or	hexadecimal	by	doing	little	more
than	looking	at	it.

	EXAMPLE	2.9	Convert	110010011101
2
	to	octal	and	hexadecimal.
If	there	are	too	few	bits,	leading	zeros	can	be	added.
2.4			SIGNED	INTEGER	REPRESENTATION
We	have	seen	how	to	convert	an	unsigned	integer	from	one	base	to	another.	Signed	numbers	require	that
additional	issues	be	addressed.	When	an	integer	variable	is	declared	in	a	program,	many	programming
languages	automatically	allocate	a	storage	area	that	includes	a	sign	as	the	first	bit	of	the	storage	location.
By	convention,	a	“1”	in	the	high-order	bit	indicates	a	negative	number.	The	storage	location	can	be	as
small	as	an	8-bit	byte	or	as	large	as	several	words,	depending	on	the	programming	language	and	the
computer	system.	The	remaining	bits	(after	the	sign	bit)	are	used	to	represent	the	number	itself.
How	 this	 number	 is	 represented	 depends	 on	 the	 method	 used.	 There	 are	 three	 commonly	 used
approaches.	 The	 most	 intuitive	 method,	 signed	 magnitude,	 uses	 the	 remaining	 bits	 to	 represent	 the
magnitude	 of	 the	 number.	 This	 method	 and	 the	 other	 two	 approaches,	 which	 both	 use	 the	 concept	 of
complements,	are	introduced	in	the	following	sections.
2.4.1		Signed	Magnitude
Up	to	this	point,	we	have	ignored	the	possibility	of	binary	representations	for	negative	numbers.	The	set
of	 positive	 and	 negative	 integers	 is	 referred	 to	 as	 the	 set	 of	signed	 integers.	 The	 problem	 with
representing	signed	integers	as	binary	values	is	the	sign—how	should	we	encode	the	actual	sign	of	the
number?	Signed-magnitude	representation	is	one	method	of	solving	this	problem.	As	its	name	implies,
a	signed-magnitude	number	has	a	sign	as	its	leftmost	bit	(also	referred	to	as	the	high-order	bit	or	the	most
significant	bit)	whereas	the	remaining	bits	represent	the	magnitude	(or	absolute	value)	of	the	numeric
value.	For	example,	in	an	8-bit	word,	–1	would	be	represented	as	10000001,	and	+1	as	00000001.	In	a
computer	system	that	uses	signed-magnitude	representation	and	8	bits	to	store	integers,	7	bits	can	be	used
for	the	actual	representation	of	the	magnitude	of	the	number.	This	means	that	the	largest	integer	an	8-bit
word	can	represent	is	2
7
	–	1,	or	127	(a	zero	in	the	high-order	bit,	followed	by	7	ones).	The	smallest
integer	is	8	ones,	or	–127.	Therefore,	N	bits	can	represent	–(2
(N–1)
	–	1)	to	2
(N–1)
	–	1.
Computers	must	be	able	to	perform	arithmetic	calculations	on	integers	that	are	represented	using	this
notation.	Signed-magnitude	arithmetic	is	carried	out	using	essentially	the	same	methods	that	humans	use
with	 pencil	 and	 paper,	 but	 it	 can	 get	 confusing	 very	 quickly.	 As	 an	 example,	 consider	 the	 rules	 for
addition:	(1)	If	the	signs	are	the	same,	add	the	magnitudes	and	use	that	same	sign	for	the	result;	(2)	If	the
signs	differ,	you	must	determine	which	operand	has	the	larger	magnitude.	The	sign	of	the	result	is	the	same
as	the	sign	of	the	operand	with	the	larger	magnitude,	and	the	magnitude	must	be	obtained	by	subtracting

(not	adding)	the	smaller	one	from	the	larger	one.	If	you	consider	these	rules	carefully,	this	is	the	method
you	use	for	signed	arithmetic	by	hand.
We	arrange	the	operands	in	a	certain	way	based	on	their	signs,	perform	the	calculation	without	regard
to	the	signs,	and	then	supply	the	sign	as	appropriate	when	the	calculation	is	complete.	When	modeling	this
idea	in	an	8-bit	word,	we	must	be	careful	to	include	only	7	bits	in	the	magnitude	of	the	answer,	discarding
any	carries	that	take	place	over	the	high-order	bit.
	EXAMPLE	2.10	Add	01001111
2
	to	00100011
2
	using	signed-magnitude	arithmetic.
The	arithmetic	proceeds	just	as	in	decimal	addition,	including	the	carries,	until	we	get	to	the	seventh	bit
from	 the	 right.	 If	 there	 is	 a	 carry	 here,	 we	 say	 that	 we	 have	 an	 overflow	 condition	 and	 the	 carry	 is
discarded,	resulting	in	an	incorrect	sum.	There	is	no	overflow	in	this	example.
We	find	that	01001111
2
	+	00100011
2
	=	01110010
2
	in	signed-magnitude	representation.
Sign	bits	are	segregated	because	they	are	relevant	only	after	the	addition	is	complete.	In	this	case,	we
have	the	sum	of	two	positive	numbers,	which	is	positive.	Overflow	(and	thus	an	erroneous	result)	in
signed	numbers	occurs	when	the	sign	of	the	result	is	incorrect.
In	signed	magnitude,	the	sign	bit	is	used	only	for	the	sign,	so	we	can’t	“carry	into”	it.	If	there	is	a	carry
emitting	from	the	seventh	bit,	our	result	will	be	truncated	as	the	seventh	bit	overflows,	giving	an	incorrect
sum.	 (Example	 2.11	 illustrates	 this	 overflow	 condition.)	 Prudent	 programmers	 avoid	 “million-dollar”
mistakes	by	checking	for	overflow	conditions	whenever	there	is	the	slightest	possibility	they	could	occur.
If	we	did	not	discard	the	overflow	bit,	it	would	carry	into	the	sign,	causing	the	more	outrageous	result	of
the	sum	of	two	positive	numbers	being	negative.	(Imagine	what	would	happen	if	the	next	step	in	a	program
were	to	take	the	square	root	or	log	of	that	result!)
	EXAMPLE	2.11	Add	01001111
2
	to	01100011
2
	using	signed-magnitude	arithmetic.
We	obtain	the	erroneous	result	of	79	+	99	=	50.
Dabbling	on	the	Double
The	fastest	way	to	convert	a	binary	number	to	decimal	is	a	method	called	double-dabble	(or	double-
dibble).	This	method	builds	on	the	idea	that	a	subsequent	power	of	two	is	double	the	previous	power

of	two	in	a	binary	number.	The	calculation	starts	with	the	leftmost	bit	and	works	toward	the	rightmost
bit.	The	first	bit	is	doubled	and	added	to	the	next	bit.	This	sum	is	then	doubled	and	added	to	the
following	bit.	The	process	is	repeated	for	each	bit	until	the	rightmost	bit	has	been	used.
EXAMPLE	1
Convert	10010011
2
	to	decimal.
Step	1:	Write	down	the	binary	number,	leaving	space	between	the	bits.
Step	2:	Double	the	high-order	bit	and	copy	it	under	the	next	bit.
Step	3:	Add	the	next	bit	and	double	the	sum.	Copy	this	result	under	the	next	bit.
Step	4:	Repeat	Step	3	until	you	run	out	of	bits.
When	we	combine	hextet	grouping	(in	reverse)	with	the	double-dabble	method,	we	find	that	we	can
convert	hexadecimal	to	decimal	with	ease.
EXAMPLE	2
Convert	02CA
16
	to	decimal.
First,	convert	the	hex	to	binary	by	grouping	into	hextets.

Then	apply	the	double-dabble	method	on	the	binary	form:
As	with	addition,	signed-magnitude	subtraction	is	carried	out	in	a	manner	similar	to	pencil-and-paper
decimal	arithmetic,	where	it	is	sometimes	necessary	to	borrow	from	digits	in	the	minuend.
	EXAMPLE	2.12	Subtract	010011112	from	011000112	using	signed-magnitude	arithmetic.
We	find	that	01100011
2
	–	01001111
2
	=	00010100
2
	in	signed-magnitude	representation.
	EXAMPLE	2.13	Subtract	011000112	(99)	from	010011112	(79)	using	signed-magnitude	arithmetic.
By	inspection,	we	see	that	the	subtrahend,	01100011,	is	larger	than	the	minuend,	01001111.	With	the
result	obtained	in	Example	2.12,	we	know	that	the	difference	of	these	two	numbers	is	0010100
2
.	Because
the	subtrahend	is	larger	than	the	minuend,	all	we	need	to	do	is	change	the	sign	of	the	difference.	So	we
find	that	01001111
2
	–	01100011
2
	=	10010100
2
	in	signed-magnitude	representation.
We	know	that	subtraction	is	the	same	as	“adding	the	opposite,”	which	equates	to	negating	the	value	we
wish	to	subtract	and	then	adding	instead	(which	is	often	much	easier	than	performing	all	the	borrows
necessary	for	subtraction,	particularly	in	dealing	with	binary	numbers).	Therefore,	we	need	to	look	at
some	examples	involving	both	positive	and	negative	numbers.	Recall	the	rules	for	addition:	(1)	If	the
signs	are	the	same,	add	the	magnitudes	and	use	that	same	sign	for	the	result;	(2)	If	the	signs	differ,	you
must	determine	which	operand	has	the	larger	magnitude.	The	sign	of	the	result	is	the	same	as	the	sign	of
the	operand	with	the	larger	magnitude,	and	the	magnitude	must	be	obtained	by	subtracting	(not	adding)	the
smaller	one	from	the	larger	one.
	EXAMPLE	2.14	Add	100100112	(–19)	to	000011012	(+13)	using	signed-magnitude	arithmetic.
The	first	number	(the	augend)	is	negative	because	its	sign	bit	is	set	to	1.	The	second	number	(the
addend)	is	positive.	What	we	are	asked	to	do	is	in	fact	a	subtraction.	First,	we	determine	which	of	the
two	numbers	is	larger	in	magnitude	and	use	that	number	for	the	augend.	Its	sign	will	be	the	sign	of	the
result.

With	the	inclusion	of	the	sign	bit,	we	see	that	10010011
2
	–	00001101
2
	=	10000110
2
	in	signed-magnitude
representation.
	EXAMPLE	2.15	Subtract	10011000
2
	(–24)	from	10101011
2
	(–43)	using	signed-magnitude	arithmetic.
We	can	convert	the	subtraction	to	an	addition	by	negating	–24,	which	gives	us	24,	and	then	we	can	add
this	to	–43,	giving	us	a	new	problem	of	–43	+	24.	However,	we	know	from	the	addition	rules	above	that
because	the	signs	now	differ,	we	must	actually	subtract	the	smaller	magnitude	from	the	larger	magnitude
(or	subtract	24	from	43)	and	make	the	result	negative	(because	43	is	larger	than	24).
Note	that	we	are	not	concerned	with	the	sign	until	we	have	performed	the	subtraction.	We	know	the
answer	must	be	negative.	So	we	end	up	with	10101011
2
	–	10011000
2
	=	10010011
2
	in	signed-magnitude
representation.
While	 reading	 the	 preceding	 examples,	 you	 may	 have	 noticed	 how	 many	 questions	 we	 had	 to	 ask
ourselves:	Which	number	is	larger?	Am	I	subtracting	a	negative	number?	How	many	times	do	I	have	to
borrow	from	the	minuend?	A	computer	engineered	to	perform	arithmetic	in	this	manner	must	make	just	as
many	decisions	(though	a	whole	lot	faster).	The	logic	(and	circuitry)	is	further	complicated	by	the	fact	that
signed	 magnitude	 has	 two	 representations	 for	 zero,	 10000000	 and	 00000000	 (and	 mathematically
speaking,	this	simply	shouldn’t	happen!).	Simpler	methods	for	representing	signed	numbers	would	allow
simpler	and	less	expensive	circuits.	These	simpler	methods	are	based	on	radix	complement	systems.
2.4.2		Complement	Systems
Number	theorists	have	known	for	hundreds	of	years	that	one	decimal	number	can	be	subtracted	from
another	by	adding	the	difference	of	the	subtrahend	from	all	nines	and	adding	back	a	carry.	This	is	called
taking	 the	 nine’s	 complement	 of	 the	 subtrahend	 or,	 more	 formally,	 finding	 the	diminished	 radix
complement	of	the	subtrahend.	Let’s	say	we	wanted	to	find	167	–	52.	Taking	the	difference	of	52	from
999,	we	have	947.	Thus,	in	nine’s	complement	arithmetic,	we	have	167	–	52	=	167	+	947	=	1114.	The
“carry”	from	the	hundreds	column	is	added	back	to	the	units	place,	giving	us	a	correct	167	–	52	=	115.
This	 method	 was	 commonly	 called	 “casting	 out	 9s”	 and	 has	 been	 extended	 to	 binary	 operations	 to
simplify	computer	arithmetic.	The	advantage	that	complement	systems	give	us	over	signed	magnitude	is
that	there	is	no	need	to	process	sign	bits	separately,	but	we	can	still	easily	check	the	sign	of	a	number	by
looking	at	its	high-order	bit.
Another	way	to	envision	complement	systems	is	to	imagine	an	odometer	on	a	bicycle.	Unlike	cars,
when	you	go	backward	on	a	bike,	the	odometer	will	go	backward	as	well.	Assuming	an	odometer	with

three	digits,	if	we	start	at	zero	and	end	with	700,	we	can’t	be	sure	whether	the	bike	went	forward	700
miles	or	backward	300	miles!	The	easiest	solution	to	this	dilemma	is	simply	to	cut	the	number	space	in
half	and	use	001–500	for	positive	miles	and	501–999	for	negative	miles.	We	have,	effectively,	cut	down
the	distance	our	odometer	can	measure.	But	now	if	it	reads	997,	we	know	the	bike	has	backed	up	3	miles
instead	of	riding	forward	997	miles.	The	numbers	501–999	represent	the	radix	complements	(the	second
of	the	two	methods	introduced	below)	of	the	numbers	001–500	and	are	being	used	to	represent	negative
distance.
One’s	Complement
As	illustrated	above,	the	diminished	radix	complement	of	a	number	in	base	10	is	found	by	subtracting	the
subtrahend	from	the	base	minus	one,	which	is	9	in	decimal.	More	formally,	given	a	number	N	in	base	r
having	d	digits,	the	diminished	radix	complement	of	N	is	defined	to	be	(r	
d
	–	1)	–	N.	For	decimal	numbers,
r	=	10,	and	the	diminished	radix	is	10	–	1	=	9.	For	example,	the	nine’s	complement	of	2468	is	9999	–
2468	=	7531.	For	an	equivalent	operation	in	binary,	we	subtract	from	one	less	the	base	(2),	which	is	1.
For	 example,	 the	 one’s	 complement	 of	 0101
2
	 is	 1111
2
	 –	 0101	 =	 1010.	 Although	 we	 could	 tediously
borrow	and	subtract	as	discussed	above,	a	few	experiments	will	convince	you	that	forming	the	one’s
complement	of	a	binary	number	amounts	to	nothing	more	than	switching	all	of	the	1s	with	0s	and	vice
versa.	This	sort	of	bit-flipping	is	very	simple	to	implement	in	computer	hardware.
It	is	important	to	note	at	this	point	that	although	we	can	find	the	nine’s	complement	of	any	decimal
number	 or	 the	 one’s	 complement	 of	 any	 binary	 number,	 we	 are	 most	 interested	 in	 using	 complement
notation	to	represent	negative	numbers.	We	know	that	performing	a	subtraction,	such	as	10	–	7,	can	also
be	 thought	 of	 as	 “adding	 the	 opposite,”	 as	 in	 10	 +	 (–7).	 Complement	 notation	 allows	 us	 to	 simplify
subtraction	 by	 turning	 it	 into	 addition,	 but	 it	 also	 gives	 us	 a	 method	 to	 represent	 negative	 numbers.
Because	 we	 do	 not	 wish	 to	 use	 a	 special	 bit	 to	 represent	 the	 sign	 (as	 we	 did	 in	 signed-magnitude
representation),	we	need	to	remember	that	if	a	number	is	negative,	we	should	convert	it	to	its	complement.
The	result	should	have	a	1	in	the	leftmost	bit	position	to	indicate	that	the	number	is	negative.
Although	 the	 one’s	 complement	 of	 a	 number	 is	 technically	 the	 value	 obtained	 by	 subtracting	 that
number	from	a	large	power	of	two,	we	often	refer	to	a	computer	using	one’s	complement	for	negative
numbers	as	a	one’s	complement	system,	or	a	computer	that	uses	one’s	complement	arithmetic.	This	can	be
somewhat	 misleading,	 as	 positive	 numbers	 do	 not	 need	 to	 be	 complemented;	 we	 only	 complement
negative	numbers	so	we	can	get	them	into	a	format	the	computer	will	understand.	Example	2.16	illustrates
these	concepts.
	EXAMPLE	2.16	Express	23
10
	and	–9
10
	in	8-bit	binary,	assuming	a	computer	is	using	one’s	complement
representation.
Unlike	signed	magnitude,	in	one’s	complement	addition	there	is	no	need	to	maintain	the	sign	bit	separate
from	the	other	bits.	The	sign	takes	care	of	itself.	Compare	Example	2.17	with	Example	2.10.
	EXAMPLE	2.17	Add	01001111
2
	to	00100011
2
	using	one’s	complement	addition.

Suppose	we	wish	to	subtract	9	from	23.	To	carry	out	a	one’s	complement	subtraction,	we	first	express	the
subtrahend	(9)	in	one’s	complement,	then	add	it	to	the	minuend	(23);	we	are	effectively	now	adding	–9	to
23.	The	high-order	bit	will	have	a	1	or	a	0	carry,	which	is	added	to	the	low-order	bit	of	the	sum.	(This	is
called	end	carry-around	and	results	from	using	the	diminished	radix	complement.)
	EXAMPLE	2.18	Add	23
10
	to	–9
10
	using	one’s	complement	arithmetic.
	EXAMPLE	2.19	Add	9
10
	to	–23
10
	using	one’s	complement	arithmetic.
How	do	we	know	that	11110001
2
	is	actually	–14
10
?	We	simply	need	to	take	the	one’s	complement	of	this
binary	 number	 (remembering	 it	 must	 be	 negative	 because	 the	 leftmost	 bit	 is	 negative).	 The	 one’s
complement	of	11110001
2
	is	00001110
2
,	which	is	14.
The	primary	disadvantage	of	one’s	complement	is	that	we	still	have	two	representations	for	zero:
00000000	and	11111111.	For	this	and	other	reasons,	computer	engineers	long	ago	stopped	using	one’s
complement	in	favor	of	the	more	efficient	two’s	complement	representation	for	binary	numbers.
Two’s	Complement
Two’s	complement	is	an	example	of	a	radix	complement.	Given	a	number	N	in	base	r	having	d	digits,	the
radix	complement	of	N	is	defined	as	r	
d
	–	N	for	N	≠	0	and	0	for	N	=	0.	The	radix	complement	is	often
more	intuitive	than	the	diminished	radix	complement.	Using	our	odometer	example,	the	ten’s	complement
of	going	forward	2	miles	is	10
3
	–	2	=	998,	which	we	have	already	agreed	indicates	a	negative	(backward)
distance.	Similarly,	in	binary,	the	two’s	complement	of	the	4-bit	number	0011
2
	is	2
4
	–	0011
2
	=	10000
2
	–
0011
2
	=	1101
2
.
Upon	 closer	 examination,	 you	 will	 discover	 that	 two’s	 complement	 is	 nothing	 more	 than	 one’s
complement	incremented	by	1.	To	find	the	two’s	complement	of	a	binary	number,	simply	flip	bits	and	add

1.	This	simplifies	addition	and	subtraction	as	well.	Because	the	subtrahend	(the	number	we	complement
and	add)	is	incremented	at	the	outset,	however,	there	is	no	end	carry-around	to	worry	about.	We	simply
discard	any	carries	involving	the	high-order	bits.	Just	as	with	one’s	complement,	two’s	complement	refers
to	the	complement	of	a	number,	whereas	a	computer	using	this	notation	to	represent	negative	numbers	is
said	to	be	a	two’s	complement	system,	or	uses	two’s	complement	arithmetic.	As	before,	positive	numbers
can	be	left	alone;	we	only	need	to	complement	negative	numbers	to	get	them	into	their	two’s	complement
form.	Example	2.20	illustrates	these	concepts.
	EXAMPLE	2.20	Express	23
10
,	–23
10
,	and	–9
10
	in	8-bit	binary,	assuming	a	computer	is	using	two’s
complement	representation.
Because	the	representation	of	positive	numbers	is	the	same	in	one’s	complement	and	two’s	complement
(as	well	as	signed-magnitude),	the	process	of	adding	two	positive	binary	numbers	is	the	same.	Compare
Example	2.21	with	Example	2.17	and	Example	2.10.
	EXAMPLE	2.21	Add	01001111
2
	to	00100011
2
	using	two’s	complement	addition.
Suppose	we	are	given	the	binary	representation	for	a	number	and	want	to	know	its	decimal	equivalent.
Positive	numbers	are	easy.	For	example,	to	convert	the	two’s	complement	value	of	00010111
2
	to	decimal,
we	 simply	 convert	 this	 binary	 number	 to	 a	 decimal	 number	 to	 get	 23.	 However,	 converting	 two’s
complement	negative	numbers	requires	a	reverse	procedure	similar	to	the	conversion	from	decimal	to
binary.	Suppose	we	are	given	the	two’s	complement	binary	value	of	11110111
2
,	and	we	want	to	know	the
decimal	equivalent.	We	know	this	is	a	negative	number	but	must	remember	it	is	represented	using	two’s
complement.	We	first	flip	the	bits	and	then	add	1	(find	the	one’s	complement	and	add	1).	This	results	in
the	following:	00001000
2
	+	1	=	00001001
2
.	This	is	equivalent	to	the	decimal	value	9.	However,	the
original	 number	 we	 started	 with	 was	 negative,	 so	 we	 end	 up	 with	 –9	 as	 the	 decimal	 equivalent	 to
11110111
2
.
The	following	two	examples	illustrate	how	to	perform	addition	(and	hence	subtraction,	because	we
subtract	a	number	by	adding	its	opposite)	using	two’s	complement	notation.
	EXAMPLE	2.22	Add	9
10
	to	–23
10
	using	two’s	complement	arithmetic.

It	is	left	as	an	exercise	for	you	to	verify	that	11110010
2
	is	actually	–14
10
	using	two’s	complement	notation.
	EXAMPLE	2.23	Find	the	sum	of	23
10
	and	–9
10
	in	binary	using	two’s	complement	arithmetic.
In	two’s	complement,	the	addition	of	two	negative	numbers	produces	a	negative	number,	as	we	might
expect.
	EXAMPLE	2.24	 Find	 the	 sum	 of	 11101001
2
	 (–23)	 and	 11110111
2
	 (–9)	 using	 two’s	 complement
addition.
Notice	 that	 the	 discarded	 carries	 in	 Examples	 2.23	 and	 2.24	 did	 not	 cause	 an	 erroneous	 result.	 An
overflow	occurs	if	two	positive	numbers	are	added	and	the	result	is	negative,	or	if	two	negative	numbers
are	added	and	the	result	is	positive.	It	is	not	possible	to	have	overflow	when	using	two’s	complement
notation	if	a	positive	and	a	negative	number	are	being	added	together.
INTEGER	MULTIPLICATION	AND	DIVISION
Unless	 sophisticated	 algorithms	 are	 used,	 multiplication	 and	 division	 can	 consume	 a	 considerable
number	 of	 computation	 cycles	 before	 a	 result	 is	 obtained.	 Here,	 we	 discuss	 only	 the	 most
straightforward	approach	to	these	operations.	In	real	systems,	dedicated	hardware	is	used	to	optimize
throughput,	sometimes	carrying	out	portions	of	the	calculation	in	parallel.	Curious	readers	will	want	to
investigate	some	of	these	advanced	methods	in	the	references	cited	at	the	end	of	this	chapter.
The	simplest	multiplication	algorithms	used	by	computers	are	similar	to	traditional	pencil-and-
paper	methods	 used	by	 humans.	 The	complete	 multiplication	table	 for	 binary	numbers	 couldn’t	 be
simpler:	zero	times	any	number	is	zero,	and	one	times	any	number	is	that	number.
To	 illustrate	 simple	 computer	 multiplication,	 we	 begin	 by	 writing	 the	 multiplicand	 and	 the
multiplier	to	two	separate	storage	areas.	We	also	need	a	third	storage	area	for	the	product.	Starting

with	the	low-order	bit,	a	pointer	is	set	to	each	digit	of	the	multiplier.	For	each	digit	in	the	multiplier,
the	multiplicand	is	“shifted”	one	bit	to	the	left.	When	the	multiplier	is	1,	the	“shifted”	multiplicand	is
added	to	a	running	sum	of	partial	products.	Because	we	shift	the	multiplicand	by	one	bit	for	each	bit	in
the	multiplier,	a	product	requires	double	the	working	space	of	either	the	multiplicand	or	the	multiplier.
There	 are	 two	 simple	 approaches	 to	 binary	 division:	 We	 can	 either	 iteratively	 subtract	 the
denominator	from	the	divisor,	or	we	can	use	the	same	trial-and-error	method	of	long	division	that	we
were	 taught	 in	 grade	 school.	 As	 with	 multiplication,	 the	 most	 efficient	 methods	 used	 for	 binary
division	are	beyond	the	scope	of	this	text	and	can	be	found	in	the	references	at	the	end	of	this	chapter.
Regardless	of	the	relative	efficiency	of	any	algorithms	that	are	used,	division	is	an	operation	that
can	always	cause	a	computer	to	crash.	This	is	the	case	particularly	when	division	by	zero	is	attempted
or	when	two	numbers	of	enormously	different	magnitudes	are	used	as	operands.	When	the	divisor	is
much	smaller	than	the	dividend,	we	get	a	condition	known	as	divide	underflow,	which	the	computer
sees	as	the	equivalent	of	division	by	zero,	which	is	impossible.
Computers	make	a	distinction	between	integer	division	and	floating-point	division.	With	integer
division,	the	answer	comes	in	two	parts:	a	quotient	and	a	remainder.	Floating-point	division	results	in
a	number	that	is	expressed	as	a	binary	fraction.	These	two	types	of	division	are	sufficiently	different
from	each	other	as	to	warrant	giving	each	its	own	special	circuitry.	Floating-point	calculations	are
carried	out	in	dedicated	circuits	called	floating-point	units,	or	FPUs.
	EXAMPLE	Find	the	product	of	00000110
2
	and	00001011
2
.
Simple	 computer	 circuits	 can	 easily	 detect	 an	 overflow	 condition	 using	 a	 rule	 that	 is	 easy	 to
remember.	You’ll	notice	in	both	Examples	2.23	and	2.24	that	the	carry	going	into	the	sign	bit	(a	1	is
carried	from	the	previous	bit	position	into	the	sign	bit	position)	is	the	same	as	the	carry	going	out	of	the
sign	bit	(a	1	is	carried	out	and	discarded).	When	these	carries	are	equal,	no	overflow	occurs.	When	they
differ,	an	overflow	indicator	is	set	in	the	arithmetic	logic	unit,	indicating	the	result	is	incorrect.
A	Simple	Rule	for	Detecting	an	Overflow	Condition	in	Signed	Numbers:	If	the	carry	into	the
sign	bit	equals	the	carry	out	of	the	bit,	no	overflow	has	occurred.	If	the	carry	into	the	sign	bit	is
different	from	the	carry	out	of	the	sign	bit,	overflow	(and	thus	an	error)	has	occurred.
The	hard	part	is	getting	programmers	(or	compilers)	to	consistently	check	for	the	overflow	condition.
Example	2.25	indicates	overflow	because	the	carry	into	the	sign	bit	(a	1	is	carried	in)	is	not	equal	to	the
carry	out	of	the	sign	bit	(a	0	is	carried	out).

	EXAMPLE	2.25	Find	the	sum	of	126
10
	and	8
10
	in	binary	using	two’s	complement	arithmetic.
A	one	is	carried	into	the	leftmost	bit,	but	a	zero	is	carried	out.	Because	these	carries	are	not	equal,	an
overflow	has	occurred.	(We	can	easily	see	that	two	positive	numbers	are	being	added	but	the	result	is
negative.)	We	return	to	this	topic	in	Section	2.4.6.
Two’s	complement	is	the	most	popular	choice	for	representing	signed	numbers.	The	algorithm	for	adding
and	subtracting	is	quite	easy,	has	the	best	representation	for	0	(all	0	bits),	is	self-inverting,	and	is	easily
extended	to	larger	numbers	of	bits.	The	biggest	drawback	is	in	the	asymmetry	seen	in	the	range	of	values
that	 can	 be	 represented	 by	N	 bits.	 With	 signed-magnitude	 numbers,	 for	 example,	 4	 bits	 allow	 us	 to
represent	the	values	–7	through	+7.	However,	using	two’s	complement,	we	can	represent	the	values	–8
through	+7,	which	is	often	confusing	to	anyone	learning	about	complement	representations.	To	see	why	+7
is	 the	 largest	 number	 we	 can	 represent	 using	 4-bit	 two’s	 complement	 representation,	 we	 need	 only
remember	that	the	first	bit	must	be	0.	If	the	remaining	bits	are	all	1s	(giving	us	the	largest	magnitude
possible),	we	have	0111
2
,	which	is	7.	An	immediate	reaction	to	this	is	that	the	smallest	negative	number
should	then	be	1111
2
,	but	we	can	see	that	1111
2
	is	actually	–1	(flip	the	bits,	add	one,	and	make	the	number
negative).	So	how	do	we	represent	–8	in	two’s	complement	notation	using	4	bits?	It	is	represented	as
1000
2
.	We	know	this	is	a	negative	number.	If	we	flip	the	bits	(0111),	add	1	(to	get	1000,	which	is	8),	and
make	it	negative,	we	get	–8.
2.4.3		Excess-M	Representation	for	Signed	Numbers
Recall	 the	 bicycle	 example	 that	 we	 discussed	 when	 introducing	 complement	 systems.	 We	 selected	 a
particular	 value	 (500)	 as	 the	 cutoff	 for	 positive	 miles,	 and	 we	 assigned	 values	 from	 501	 to	 999	 to
negative	miles.	We	didn’t	need	signs	because	we	used	the	range	to	determine	whether	the	number	was
positive	 or	 negative.	Excess-M	 representation	 (also	 called	offset	 binary	 representation)	 does
something	very	similar;	unsigned	binary	values	are	used	to	represent	signed	integers.	However,	excess-M
representation,	 unlike	 signed	 magnitude	 and	 the	 complement	 encodings,	 is	 more	 intuitive	 because	 the
binary	string	with	all	0s	represents	the	smallest	number,	whereas	the	binary	string	with	all	1s	represents
the	largest	value;	in	other	words,	ordering	is	preserved.
The	unsigned	binary	representation	for	integer	M	(called	the	bias)	represents	the	value	0,	whereas	all
zeros	in	the	bit	pattern	represents	the	integer	–M.	Essentially,	a	decimal	integer	is	“mapped”	(as	in	our
bicycle	 example)	 to	 an	 unsigned	binary	 integer,	 but	 interpreted	 as	 positive	 or	 negative	 depending	 on
where	it	falls	in	the	range.	If	we	are	using	n	bits	for	the	binary	representation,	we	need	to	select	the	bias
in	such	a	manner	that	we	split	the	range	equally.	We	typically	do	this	by	choosing	a	bias	of	2
n–1
	–	1.	For
example,	if	we	were	using	4-bit	representation,	the	bias	should	be	2
4–1
	–	1	=	7.	Just	as	with	signed
magnitude,	 one’s	 complement,	 and	 two’s	 complement,	 there	 is	 a	 specific	 range	 of	 values	 that	 can	 be
expressed	in	n	bits.
The	unsigned	binary	value	for	a	signed	integer	using	excess-M	representation	is	determined	simply	by

adding	M	to	that	integer.	For	example,	assuming	that	we	are	using	excess-7	representation,	the	integer	0
10
would	be	represented	as	0	+	7	=	7
10
	=	0111
2
;	the	integer	3
10
	would	be	represented	as	3	+	7	=	10
10
	=	1010
2
;
and	the	integer	–7	would	be	represented	as	–7	+	7	=	0
10
	=	0000
2
.	Using	excess-7	notation	and	given	the
binary	number	1111
2
,	to	find	the	decimal	value	it	represents,	we	simply	subtract	7:	1111
2
	=	15
10
,	and	15	–
7	=	8;	therefore,	the	value	1111
2
,	using	excess-7	representation,	is	+8
10
.
Let’s	compare	the	encoding	schemes	we	have	seen	so	far,	assuming	8-bit	numbers:
Excess-M	representation	allows	us	to	use	unsigned	binary	values	to	represent	signed	integers;	it	is
important	to	note,	however,	that	two	parameters	must	be	specified:	the	number	of	bits	being	used	in	the
representation	and	the	bias	value	itself.	In	addition,	a	computer	is	unable	to	perform	addition	on	excess-M
values	 using	 hardware	 designed	 for	 unsigned	 numbers;	 special	 circuits	 are	 required.	 Excess-M
representation	is	important	because	of	its	use	in	representing	integer	exponents	in	floating-point	numbers,
as	we	will	see	in	Section	2.5.
2.4.4		Unsigned	Versus	Signed	Numbers
We	introduced	our	discussion	of	binary	integers	with	unsigned	numbers.	Unsigned	numbers	are	used	to
represent	 values	 that	 are	 guaranteed	 not	 to	 be	 negative.	 A	 good	 example	 of	 an	 unsigned	 number	 is	 a
memory	address.	If	the	4-bit	binary	value	1101	is	unsigned,	then	it	represents	the	decimal	value	13,	but	as
a	signed	two’s	complement	number,	it	represents	–3.	Signed	numbers	are	used	to	represent	data	that	can
be	either	positive	or	negative.
A	computer	programmer	must	be	able	to	manage	both	signed	and	unsigned	numbers.	To	do	so,	the
programmer	must	first	identify	numeric	values	as	either	signed	or	unsigned	numbers.	This	is	done	by
declaring	the	value	as	a	specific	type.	For	instance,	the	C	programming	language	has	int	and	unsigned	int
as	possible	types	for	integer	variables,	defining	signed	and	unsigned	integers,	respectively.	In	addition	to
different	type	declarations,	many	languages	have	different	arithmetic	operations	for	use	with	signed	and
unsigned	numbers.	A	language	may	have	one	subtraction	instruction	for	signed	numbers	and	a	different
subtraction	instruction	for	unsigned	numbers.	In	most	assembly	languages,	programmers	can	choose	from
a	signed	comparison	operator	or	an	unsigned	comparison	operator.
It	is	interesting	to	compare	what	happens	with	unsigned	and	signed	numbers	when	we	try	to	store
values	that	are	too	large	for	the	specified	number	of	bits.	Unsigned	numbers	simply	wrap	around	and	start
over	at	zero.	For	example,	if	we	are	using	4-bit	unsigned	binary	numbers,	and	we	add	1	to	1111,	we	get
0000.	This	“return	to	zero”	wraparound	is	familiar—perhaps	you	have	seen	a	high-mileage	car	in	which
the	odometer	has	wrapped	back	around	to	zero.	However,	signed	numbers	devote	half	their	space	to
positive	numbers	and	the	other	half	to	negative	numbers.	If	we	add	1	to	the	largest	positive	4-bit	two’s
complement	number	0111	(+7),	we	get	1000	(–8).	This	wraparound	with	the	unexpected	change	in	sign
has	been	problematic	to	inexperienced	programmers,	resulting	in	multiple	hours	of	debugging	time.	Good

programmers	understand	this	condition	and	make	appropriate	plans	to	deal	with	the	situation	before	it
occurs.
2.4.5		Computers,	Arithmetic,	and	Booth’s	Algorithm
Computer	arithmetic	as	introduced	in	this	chapter	may	seem	simple	and	straight-forward,	but	it	is	a	field
of	major	study	in	computer	architecture.	The	basic	focus	is	on	the	implementation	of	arithmetic	functions,
which	can	be	realized	in	software,	firmware,	or	hardware.	Researchers	in	this	area	are	working	toward
designing	superior	central	processing	units	(CPUs),	developing	high-performance	arithmetic	circuits,	and
contributing	 to	 the	 area	 of	 embedded	 systems	 application-specific	 circuits.	 They	 are	 working	 on
algorithms	and	new	hardware	implementations	for	fast	addition,	subtraction,	multiplication,	and	division,
as	well	 as	fast	 floating-point	 operations.	Researchers	 are	looking	 for	 schemes	that	 use	 nontraditional
approaches,	such	as	the	fast	carry	look-ahead	principle,	residue	arithmetic,	and	Booth’s	algorithm.
Booth’s	algorithm	is	a	good	example	of	one	such	scheme	and	is	introduced	here	in	the	context	of	signed
two’s	complement	numbers	to	give	you	an	idea	of	how	a	simple	arithmetic	operation	can	be	enhanced	by
a	clever	algorithm.
Although	 Booth’s	 algorithm	 usually	 yields	 a	 performance	 increase	 when	 multiplying	 two’s
complement	 numbers,	 there	 is	 another	 motivation	 for	 introducing	 this	 algorithm.	 In	 Section	 2.4.2,	 we
covered	examples	of	two’s	complement	addition	and	saw	that	the	numbers	could	be	treated	as	unsigned
values.	We	simply	perform	“regular”	addition,	as	the	following	example	illustrates:
The	same	is	true	for	two’s	complement	subtraction.	However,	now	consider	the	standard	pencil-and-
paper	method	for	multiplying	the	following	two’s	complement	numbers:
“Regular”	 multiplication	 clearly	 yields	 the	 incorrect	 result.	 There	 are	 a	 number	 of	 solutions	 to	 this
problem,	such	as	converting	both	values	to	positive	numbers,	performing	conventional	multiplication,	and
then	remembering	if	one	or	both	values	were	negative	to	determine	whether	the	result	should	be	positive
or	negative.	Booth’s	algorithm	not	only	solves	this	dilemma,	but	also	speeds	up	multiplication	in	the
process.
The	general	idea	of	Booth’s	algorithm	is	to	increase	the	speed	of	a	multiplication	when	there	are
consecutive	zeros	or	ones	in	the	multiplier.	It	is	easy	to	see	that	consecutive	zeros	help	performance.	For
example,	if	we	use	the	tried	and	true	pencil-and-paper	method	and	find	978	×	1001,	the	multiplication	is
much	easier	than	if	we	take	978	×	999.	This	is	because	of	the	two	zeros	found	in	1001.	However,	if	we
rewrite	the	two	problems	as	follows:

we	see	that	the	problems	are	in	fact	equal	in	difficulty.
Our	goal	is	to	use	a	string	of	ones	in	a	binary	number	to	our	advantage	in	much	the	same	way	that	we
use	a	string	of	zeros	to	our	advantage.	We	can	use	the	rewriting	idea	from	above.	For	example,	the	binary
number	0110	can	be	rewritten	1000	–	0010	=	–0010	+	1000.	The	two	ones	have	been	replaced	by	a
“subtract”	(determined	by	the	rightmost	1	in	the	string)	followed	by	an	“add”	(determined	by	moving	one
position	left	of	the	leftmost	1	in	the	string).
Consider	the	following	standard	multiplication	example:
The	idea	of	Booth’s	algorithm	is	to	replace	the	string	of	ones	in	the	multiplier	with	an	initial	subtract
when	we	see	the	rightmost	1	of	the	string	(or	subtract	0011)	and	then	later	add	for	the	bit	after	the	last	1
(or	add	001100).	In	the	middle	of	the	string,	we	can	now	use	simple	shifting:
In	Booth’s	algorithm,	if	the	multiplicand	and	multiplier	are	n-bit	two’s	complement	numbers,	the	result	is
a	2n-bit	two’s	complement	value.	Therefore,	when	we	perform	our	intermediate	steps,	we	must	extend
our	n-bit	numbers	to	2n-bit	numbers.	If	a	number	is	negative	and	we	extend	it,	we	must	extend	the	sign.
For	example,	the	value	1000	(–8)	extended	to	8	bits	would	be	11111000.	We	continue	to	work	with	bits	in
the	multiplier,	shifting	each	time	we	complete	a	step.	However,	we	are	interested	in	pairs	of	bits	in	the
multiplier	and	proceed	according	to	the	following	rules:
1.		If	the	current	multiplier	bit	is	1	and	the	preceding	bit	was	0,	we	are	at	the	beginning	of	a	string	of
ones,	so	subtract	the	multiplicand	from	the	product	(or	add	the	opposite).
2.		If	the	current	multiplier	bit	is	0	and	the	preceding	bit	was	1,	we	are	at	the	end	of	a	string	of	ones,	so
add	the	multiplicand	to	the	product.
3.		If	it	is	a	00	pair,	or	a	11	pair,	do	no	arithmetic	operation	(we	are	in	the	middle	of	a	string	of	zeros	or	a
string	of	ones).	Simply	shift.	The	power	of	the	algorithm	is	in	this	step:	We	can	now	treat	a	string	of
ones	as	a	string	of	zeros	and	do	nothing	more	than	shift.
Note:	 The	 first	 time	 we	 pick	 a	 pair	 of	 bits	 in	 the	 multiplier,	 we	 should	 assume	 a	 mythical	 0	 as	 the
“previous”	bit.	Then	we	simply	move	left	one	bit	for	our	next	pair.

Example	2.26	illustrates	the	use	of	Booth’s	algorithm	to	multiply	–3	×	5	using	signed	4-bit	two’s
complement	numbers.
	EXAMPLE	2.26	Negative	3	in	4-bit	two’s	complement	is	1101.	Extended	to	8	bits,	it	is	11111101.	Its
complement	is	00000011.	When	we	see	the	rightmost	1	in	the	multiplier,	it	is	the	beginning	of	a	string	of
1s,	so	we	treat	it	as	if	it	were	the	string	10:
Ignore	extended	sign	bits	that	go	beyond	2n.
	EXAMPLE	2.27	Let’s	look	at	the	larger	example	of	53	×	126:
Note	that	we	have	not	shown	the	extended	sign	bits	that	go	beyond	what	we	need	and	use	only	the	16
rightmost	bits.	The	entire	string	of	ones	in	the	multiplier	was	replaced	by	a	subtract	(adding	11001011)
followed	 by	 an	 add.	 Everything	 in	 the	 middle	 is	 simply	 shifting—something	 that	 is	 very	 easy	 for	 a
computer	 to	 do	 (as	 we	 will	 see	 in	Chapter	3).	 If	 the	 time	 required	 for	 a	 computer	 to	 do	 an	 add	 is
sufficiently	larger	than	that	required	to	do	a	shift,	Booth’s	algorithm	can	provide	a	considerable	increase
in	performance.	This	depends	somewhat,	of	course,	on	the	multiplier.	If	the	multiplier	has	strings	of	zeros
and/or	ones,	the	algorithm	works	well.	If	the	multiplier	consists	of	an	alternating	string	of	zeros	and	ones
(the	 worst	 case),	 using	 Booth’s	 algorithm	 might	 very	 well	 require	 more	 operations	 than	 the	 standard
approach.
Computers	perform	Booth’s	algorithm	by	adding	and	shifting	values	stored	in	registers.	A	special	type
of	shift	called	an	arithmetic	shift	is	necessary	to	preserve	the	sign	bit.	Many	books	present	Booth’s
algorithm	in	terms	of	arithmetic	shifts	and	add	operations	on	registers	only,	and	may	appear	quite	different
from	the	preceding	method.	We	have	presented	Booth’s	algorithm	so	that	it	more	closely	resembles	the
pencil-and-paper	 method	 with	 which	 we	 are	 all	 familiar,	 although	 it	 is	 equivalent	 to	 the	 computer
algorithms	presented	elsewhere.

There	have	been	many	algorithms	developed	for	fast	multiplication,	but	many	do	not	hold	for	signed
multiplication.	Booth’s	algorithm	not	only	allows	multiplication	to	be	performed	faster	in	most	cases,	but
it	also	has	the	added	bonus	in	that	it	works	correctly	on	signed	numbers.
2.4.6		Carry	Versus	Overflow
The	wraparound	referred	to	in	the	preceding	section	is	really	overflow.	CPUs	often	have	flags	to	indicate
both	carry	and	overflow.	However,	the	overflow	flag	is	used	only	with	signed	numbers	and	means	nothing
in	the	context	of	unsigned	numbers,	which	use	the	carry	flag	instead.	If	carry	(which	means	carry	out	of
the	leftmost	bit)	occurs	in	unsigned	numbers,	we	know	we	have	overflow	(the	new	value	is	too	large	to
be	stored	in	the	given	number	of	bits)	but	the	overflow	bit	is	not	set.	Carry	out	can	occur	in	signed
numbers	 as	 well;	 however,	 its	 occurrence	 in	 signed	 numbers	 is	 neither	 sufficient	 nor	 necessary	 for
overflow.	We	have	already	seen	that	overflow	in	signed	numbers	can	be	determined	if	the	carry	in	to	the
leftmost	bit	and	the	carry	out	of	the	leftmost	bit	differ.	However,	carry	out	of	the	leftmost	bit	in	unsigned
operations	always	indicates	overflow.
TABLE	2.2	Examples	of	Carry	and	Overflow	in	Signed	Numbers
To	illustrate	these	concepts,	consider	4-bit	unsigned	and	signed	numbers.	If	we	add	the	two	unsigned
values	0111	(7)	and	0001	(1),	we	get	1000	(8).	There	is	no	carry	(out),	and	thus	no	error.	However,	if	we
add	the	two	unsigned	values	0111	(7)	and	1011	(11),	we	get	0010	with	a	carry,	indicating	that	there	is	an
error	 (indeed,	 7	 +	 11	 is	 not	 2).	 This	 wraparound	 would	 cause	 the	 carry	 flag	 in	 the	 CPU	 to	 be	 set.
Essentially,	carry	out	in	the	context	of	unsigned	numbers	means	an	overflow	has	occurred,	even	though	the
overflow	flag	is	not	set.
We	 said	 carry	 (out)	 is	 neither	 sufficient	 nor	 necessary	 for	 overflow	 in	 signed	 numbers.	 Consider
adding	the	two’s	complement	integers	0101	(+5)	and	0011	(+3).	The	result	is	1000	(–8),	which	is	clearly
incorrect.	The	problem	is	that	we	have	a	carry	in	to	the	sign	bit,	but	no	carry	out,	which	indicates	that	we
have	an	overflow	(therefore,	carry	is	not	necessary	for	overflow).	However,	if	we	now	add	0111	(+7)
and	1011	(–5),	we	get	the	correct	result:	0010	(+2).	We	have	both	a	carry	in	to	and	a	carry	out	of	the
leftmost	bit,	so	there	is	no	error	(so	carry	is	not	sufficient	for	overflow).	The	carry	flag	would	be	set,	but
the	 overflow	 flag	 would	 not	 be	 set.	 Thus	 carry	 out	 does	 not	 necessarily	 indicate	 an	 error	 in	 signed
numbers,	nor	does	the	lack	of	carry	out	indicate	that	the	answer	is	correct.
To	summarize,	the	rule	of	thumb	used	to	determine	when	carry	indicates	an	error	depends	on	whether
we	are	using	signed	or	unsigned	numbers.	For	unsigned	numbers,	a	carry	(out	of	the	leftmost	bit)	indicates
the	total	number	of	bits	was	not	large	enough	to	hold	the	resulting	value,	and	overflow	has	occurred.	For
signed	numbers,	if	the	carry	in	to	the	sign	bit	and	the	carry	(out	of	the	sign	bit)	differ,	then	overflow	has
occurred.	The	overflow	flag	is	set	only	when	overflow	occurs	with	signed	numbers.
Carry	 and	 overflow	 clearly	 occur	 independently	 of	 each	 other.	 Examples	 using	 signed	 two’s
complement	representation	are	given	in	Table	2.2.	Carry	in	to	the	sign	bit	is	not	indicated	in	the	table.

2.4.7		Binary	Multiplication	and	Division	Using	Shifting
Shifting	a	binary	number	simply	means	moving	the	bits	left	or	right	by	a	certain	amount.	For	example,	the
binary	value	00001111	shifted	left	one	place	results	in	00011110	(if	we	fill	with	a	zero	on	the	right).	The
first	number	is	equivalent	to	decimal	value	15;	the	second	is	decimal	30,	which	is	exactly	double	the	first
value.	This	is	no	coincidence!
When	working	with	signed	two’s	complement	numbers,	we	can	use	a	special	type	of	shift,	called	an
arithmetic	shift,	to	perform	quick	and	easy	multiplication	and	division	by	2.	Recall	that	the	leftmost	bit	in
a	two’s	complement	number	determines	its	sign,	so	we	must	be	careful	when	shifting	these	values	that	we
don’t	change	the	sign	bit,	as	multiplying	or	dividing	by	2	should	not	change	the	sign	of	the	number.
We	can	perform	a	left	arithmetic	shift	(which	multiples	a	number	by	2)	or	a	right	arithmetic	shift
(which	divides	a	number	by	2).	Assuming	that	bits	are	numbered	right	to	left	beginning	with	zero,	we	have
the	following	definitions	for	left	and	right	arithmetic	shifts.
A	left	arithmetic	shift	inserts	a	0	in	for	bit	b
0
,	and	shifts	all	other	bits	left	one	position,	resulting	in
bit	b
n–1
	being	replaced	by	bit	b
n–2
.	Because	bit	b
n–1
	is	the	sign	bit,	if	the	value	in	this	bit	changes,	the
operation	has	caused	overflow.	Multiplication	by	2	always	results	in	a	binary	number	with	the	rightmost
bit	equal	to	0,	which	is	an	even	number,	and	thus	explains	why	we	pad	with	a	zero	on	the	right.	Consider
the	following	examples:
	EXAMPLE	 2.28	 Multiply	 the	 value	 11	 (expressed	 using	 8-bit	 signed	 two’s	 complement
representation)	by	2.
We	start	with	the	binary	value	for	11:
0	0	0	0	1	0	1	1
and	we	shift	left	one	place,	resulting	in:
0	0	0	1	0	1	1	0
which	is	decimal	2	=	11	×	2.	No	overflow	has	occurred,	so	the	value	is	correct.
	EXAMPLE	 2.29	 Multiply	 the	 value	 12	 (expressed	 using	 8-bit	 signed	 two’s	 complement
representation)	by	4.
We	start	with	the	binary	value	for	12:
0	0	0	0	1	1	0	0
and	we	shift	left	two	places	(each	shift	multiplies	by	2,	so	two	shifts	is	equivalent	to	multiplying	by	4),
resulting	in:
0	0	1	1	0	0	0	0
which	is	decimal	48	=	12	×	4.	No	overflow	has	occurred,	so	the	value	is	correct.
	EXAMPLE	 2.30	 Multiply	 the	 value	 66	 (expressed	 using	 8-bit	 signed	 two’s	 complement
representation)	by	2.
We	start	with	the	binary	value	for	66:
0	1	0	0	0	0	1	0

and	we	shift	left	one	place,	resulting	in:
1	0	0	0	0	1	0	0
but	the	sign	bit	has	changed,	so	overflow	has	occurred	(66	×	2	=	132,	which	is	too	large	to	be	expressed
using	8	bits	in	signed	two’s	complement	notation).
A	right	arithmetic	shift	moves	all	bits	to	the	right,	but	carries	(copies)	the	sign	bit	from	bit	b
n–1
	to	b
n–2
.
Because	we	copy	the	sign	bit	from	right	to	left,	overflow	is	not	a	problem.	However,	division	by	2	may
have	a	remainder	of	1;	division	using	this	method	is	strictly	integer	division,	so	the	remainder	is	not
stored	in	any	way.	Consider	the	following	examples:
	EXAMPLE	2.31	Divide	the	value	12	(expressed	using	8-bit	signed	two’s	complement	representation)
by	2.
We	start	with	the	binary	value	for	12:
0	0	0	0	1	1	0	0
and	we	shift	right	one	place,	copying	the	sign	bit	of	0,	resulting	in:
0	0	0	0	0	1	1	0
which	is	decimal	6	=	12	÷	2.
	EXAMPLE	2.32	Divide	the	value	12	(expressed	using	8-bit	signed	two’s	complement	representation)
by	4.
We	start	with	the	binary	value	for	12:
0	0	0	0	1	1	0	0
and	we	shift	right	two	places,	resulting	in:
0	0	0	0	0	0	1	1
which	is	decimal	3	=	12	÷	4.
	EXAMPLE	 2.33	 Divide	 the	 value	 –14	 (expressed	 using	 8-bit	 signed	 two’s	 complement
representation)	by	2.
We	start	with	the	two’s	complement	representation	for	–14:
1	1	1	1	0	0	1	0
and	we	shift	right	one	place	(carrying	across	the	sign	bit),	resulting	in:
1	1	1	1	1	0	0	1
which	is	decimal	–7	=	–14	÷	2.
Note	that	if	we	had	divided	–15	by	2	(in	Example	2.33),	the	result	would	be	11110001	shifted	one	to	the
left	to	yield	11111000,	which	is	–8.	Because	we	are	doing	integer	division,	–15	divided	by	2	is	indeed
equal	to	–8.

2.5			FLOATING-POINT	REPRESENTATION
If	 we	 wanted	 to	 build	 a	 real	 computer,	 we	 could	 use	 any	 of	 the	 integer	 representations	 that	 we	 just
studied.	We	would	pick	one	of	them	and	proceed	with	our	design	tasks.	Our	next	step	would	be	to	decide
the	word	size	of	our	system.	If	we	want	our	system	to	be	really	inexpensive,	we	would	pick	a	small	word
size,	say,	16	bits.	Allowing	for	the	sign	bit,	the	largest	integer	this	system	could	store	is	32,767.	So	now
what	 do	 we	 do	 to	 accommodate	 a	 potential	 customer	 who	 wants	 to	 keep	 a	 tally	 of	 the	 number	 of
spectators	paying	admission	to	professional	sports	events	in	a	given	year?	Certainly,	the	number	is	larger
than	32,767.	No	problem.	Let’s	just	make	the	word	size	larger.	Thirty-two	bits	ought	to	do	it.	Our	word	is
now	big	enough	for	just	about	anything	that	anyone	wants	to	count.	But	what	if	this	customer	also	needs	to
know	the	amount	of	money	each	spectator	spends	per	minute	of	playing	time?	This	number	is	likely	to	be	a
decimal	fraction.	Now	we’re	really	stuck.
The	easiest	and	cheapest	approach	to	this	problem	is	to	keep	our	16-bit	system	and	say,	“Hey,	we’re
building	a	cheap	system	here.	If	you	want	to	do	fancy	things	with	it,	get	yourself	a	good	programmer.”
Although	this	position	sounds	outrageously	flippant	in	the	context	of	today’s	technology,	it	was	a	reality	in
the	earliest	days	of	each	generation	of	computers.	There	simply	was	no	such	thing	as	a	floating-point	unit
in	many	of	the	first	mainframes	or	microcomputers.	For	many	years,	clever	programming	enabled	these
integer	systems	to	act	as	if	they	were,	in	fact,	floating-point	systems.
If	you	are	familiar	with	scientific	notation,	you	may	already	be	thinking	of	how	you	could	handle
floating-point	operations—how	you	could	provide	floating-point	emulation—in	 an	 integer	 system.	 In
scientific	notation,	numbers	are	expressed	in	two	parts:	a	fractional	part	and	an	exponential	part	that
indicates	the	power	of	ten	to	which	the	fractional	part	should	be	raised	to	obtain	the	value	we	need.	So	to
express	32,767	in	scientific	notation,	we	could	write	3.2767	×	10
4
.	Scientific	notation	simplifies	pencil-
and-paper	calculations	that	involve	very	large	or	very	small	numbers.	It	is	also	the	basis	for	floating-point
computation	in	today’s	digital	computers.
2.5.1		A	Simple	Model
In	 digital	 computers,	 floating-point	 numbers	 consist	 of	 three	 parts:	 a	 sign	 bit,	 an	 exponent	 part
(representing	the	exponent	on	a	power	of	2),	and	a	fractional	part	(which	has	sparked	considerable	debate
regarding	 appropriate	 terminology).	 The	 term	mantissa	 is	 widely	 accepted	 when	 referring	 to	 this
fractional	part.	However,	many	people	take	exception	to	this	term	because	it	also	denotes	the	fractional
part	of	a	logarithm,	which	is	not	the	same	as	the	fractional	part	of	a	floating-point	number.	The	Institute	of
Electrical	and	Electronics	Engineers	(IEEE)	introduced	the	term	significand	to	refer	to	the	fractional	part
of	a	floating-point	number	combined	with	the	implied	binary	point	and	implied	1	(which	we	discuss	at	the
end	of	this	section).	Regrettably,	the	two	terms	mantissa	and	significand	have	become	interchangeable
when	 referring	 to	 the	 fractional	 part	 of	 a	 floating-point	 number,	 even	 though	 they	 are	 not	 technically
equivalent.	Throughout	this	text,	we	refer	to	the	fractional	part	as	the	significand,	regardless	of	whether	it
includes	the	implied	1	as	intended	by	IEEE.
The	 number	 of	 bits	 used	 for	 the	 exponent	 and	 significand	 depends	 on	 whether	 we	 would	 like	 to
optimize	for	range	(more	bits	in	the	exponent)	or	precision	(more	bits	in	the	significand).	(We	discuss
range	and	precision	in	more	detail	in	Section	2.5.7.)	For	the	remainder	of	this	section,	we	will	use	a	14-
bit	model	with	a	5-bit	exponent,	an	8-bit	significand,	and	a	sign	bit	(see	Figure	2.1).	More	general	forms
are	described	in	Section	2.5.2.

FIGURE	2.1	Simple	Model	Floating-Point	Representation
Let’s	say	that	we	wish	to	store	the	decimal	number	17	in	our	model.	We	know	that	17	=	17.0	×	10
0
	=
1.7	×	10
1
	=	0.17	×	10
2
.	Analogously,	in	binary,	17
10
	=	10001
2
	×	2
0
	=	1000.1
2
	×	2
1
	=	100.01
2
	×	2
2
	=
10.001
2
	×	2
3
	=	1.0001
2
	×	2
4
	=	0.10001
2
	×	2
5
.	If	we	use	this	last	form,	our	fractional	part	will	be	10001000
and	our	exponent	will	be	00101,	as	shown	here:
Using	this	form,	we	can	store	numbers	of	much	greater	magnitude	than	we	could	using	a	fixed-point
representation	of	14	bits	(which	uses	a	total	of	14	binary	digits	plus	a	binary,	or	radix,	point).	If	we	want
to	represent	65536	=	0.1
2
	×	2
17
	in	this	model,	we	have:
One	obvious	problem	with	this	model	is	that	we	haven’t	provided	for	negative	exponents.	If	we	wanted	to
store	 0.25,	 we	 would	 have	 no	 way	 of	 doing	 so	 because	 0.25	 is	 2
–2
	 and	 the	 exponent	 –2	 cannot	 be
represented.	We	could	fix	the	problem	by	adding	a	sign	bit	to	the	exponent,	but	it	turns	out	that	it	is	more
efficient	to	use	a	biased	exponent,	because	we	can	use	simpler	integer	circuits	designed	specifically	for
unsigned	numbers	when	comparing	the	values	of	two	floating-point	numbers.
Recall	from	Section	2.4.3	that	the	idea	behind	using	a	bias	value	is	to	convert	every	integer	in	the
range	into	a	nonnegative	integer,	which	is	then	stored	as	a	binary	numeral.	The	integers	in	the	desired
range	of	exponents	are	first	adjusted	by	adding	this	fixed	bias	value	to	each	exponent.	The	bias	value	is	a
number	near	the	middle	of	the	range	of	possible	values	that	we	select	to	represent	zero.	In	this	case,	we
would	select	15	because	it	is	midway	between	0	and	31	(our	exponent	has	5	bits,	thus	allowing	for	2
5
	or
32	values).	Any	number	larger	than	15	in	the	exponent	field	represents	a	positive	value.	Values	less	than
15	indicate	negative	values.	This	is	called	an	excess-15	representation	because	we	have	to	subtract	15	to
get	the	true	value	of	the	exponent.	Note	that	exponents	of	all	zeros	or	all	ones	are	typically	reserved	for
special	numbers	(such	as	zero	or	infinity).	In	our	simple	model,	we	allow	exponents	of	all	zeros	and	ones.
Returning	to	our	example	of	storing	17,	we	calculated	17
10
	=	0.10001
2
	×	2
5
.	The	biased	exponent	is
now	15	+	5	=	20:
If	we	wanted	to	store	0.25	=	0.1	×	2
–1
,	we	would	have:
There	is	still	one	rather	large	problem	with	this	system:	We	do	not	have	a	unique	representation	for	each
number.	All	of	the	following	are	equivalent:

Because	synonymous	forms	such	as	these	are	not	well-suited	for	digital	computers,	floating-point	numbers
must	be	normalized—that	is,	the	leftmost	bit	of	the	significand	must	always	be	1.	This	process	is	called
normalization.	This	convention	has	the	additional	advantage	that	if	the	1	is	implied,	we	effectively	gain
an	extra	bit	of	precision	in	the	significand.	Normalization	works	well	for	every	value	except	zero,	which
contains	no	nonzero	bits.	For	that	reason,	any	model	used	to	represent	floating-point	numbers	must	treat
zero	as	a	special	case.	We	will	see	in	the	next	section	that	the	IEEE-754	floating-point	standard	makes	an
exception	to	the	rule	of	normalization.
	EXAMPLE	2.34	Express	0.03125
10
	in	normalized	floating-point	form	using	the	simple	model	with
excess-15	bias.
0.03125
10
	=	0.00001
2
	×	2
0
	=	0.0001	×	2
–1
	=	0.001	×	2
–2
	=	0.01	×	2
–3
	=	0.1	×	2
–4
.	Applying	the	bias,	the
exponent	field	is	15	–	4	=	11.
Note	that	in	our	simple	model	we	have	not	expressed	the	number	using	the	normalization	notation	that
implies	the	1,	which	is	introduced	in	Section	2.5.4.
2.5.2		Floating-Point	Arithmetic
If	we	wanted	to	add	two	decimal	numbers	that	are	expressed	in	scientific	notation,	such	as	1.5	×	10
2
	+	3.5
×	10
3
,	we	would	change	one	of	the	numbers	so	that	both	of	them	are	expressed	in	the	same	power	of	the
base.	In	our	example,	1.5	×	10
2
	+	3.5	×	10
3
	=	0.15	×	10
3
	+	3.5	×	10
3
	=	3.65	×	10
3
.	Floating-point	addition
and	subtraction	work	the	same	way,	as	illustrated	below.
	EXAMPLE	2.35	Add	the	following	binary	numbers	as	represented	in	a	normalized	14-bit	format,
using	the	simple	model	with	a	bias	of	15.
We	see	that	the	addend	is	raised	to	the	second	power	and	that	the	augend	is	to	the	zero	power.	Alignment

of	these	two	operands	on	the	binary	point	gives	us:
Renormalizing,	we	retain	the	larger	exponent	and	truncate	the	low-order	bit.	Thus,	we	have:
However,	because	our	simple	model	requires	a	normalized	significand,	we	have	no	way	to	represent
zero.	This	is	easily	remedied	by	allowing	the	string	of	all	zeros	(a	zero	sign,	a	zero	exponent,	and	a	zero
significand)	to	represent	the	value	zero.	In	the	next	section,	we	will	see	that	IEEE-754	also	reserves
special	meaning	for	certain	bit	patterns.
Multiplication	 and	 division	 are	 carried	 out	 using	 the	 same	 rules	 of	 exponents	 applied	 to	 decimal
arithmetic,	such	as	2
–3
	×	2
4
	=	2
1
,	for	example.
	EXAMPLE	2.36	Assuming	a	15-bit	bias,	multiply:
Multiplication	 of	 0.11001000	 by	 0.10011010	 yields	 a	 product	 of	 0.0111100001010000,	 and	 then
multiplying	by	2
3
	×	2
1
	=	2
4
	yields	111.10000101.	Renormalizing	and	supplying	the	appropriate	exponent,
the	floating-point	product	is:
2.5.3		Floating-Point	Errors
When	we	use	pencil	and	paper	to	solve	a	trigonometry	problem	or	compute	the	interest	on	an	investment,
we	intuitively	understand	that	we	are	working	in	the	system	of	real	numbers.	We	know	that	this	system	is
infinite,	because	given	any	pair	of	real	numbers,	we	can	always	find	another	real	number	that	is	smaller
than	one	and	greater	than	the	other.
Unlike	the	mathematics	in	our	imaginations,	computers	are	finite	systems,	with	finite	storage.	When
we	call	upon	our	computers	to	carry	out	floating-point	calculations,	we	are	modeling	the	infinite	system	of
real	numbers	in	a	finite	system	of	integers.	What	we	have,	in	truth,	is	an	approximation	of	the	real	number
system.	The	more	bits	we	use,	the	better	the	approximation.	However,	there	is	always	some	element	of
error,	no	matter	how	many	bits	we	use.
Floating-point	 errors	 can	 be	 blatant,	 subtle,	 or	 unnoticed.	 The	 blatant	 errors,	 such	 as	 numeric
overflow	 or	 underflow,	 are	 the	 ones	 that	 cause	 programs	 to	 crash.	 Subtle	 errors	 can	 lead	 to	 wildly

erroneous	results	that	are	often	hard	to	detect	before	they	cause	real	problems.	For	example,	in	our	simple
model,	we	can	express	normalized	numbers	in	the	range	of	–.11111111
2
	×	2
16
	through	+.11111111	×	2
16
.
Obviously,	we	cannot	store	2
–19
	or	2
128
;	they	simply	don’t	fit.	It	is	not	quite	so	obvious	that	we	cannot
accurately	store	128.5,	which	is	well	within	our	range.	Converting	128.5	to	binary,	we	have	10000000.1,
which	is	9	bits	wide.	Our	significand	can	hold	only	eight.	Typically,	the	low-order	bit	is	dropped	or
rounded	into	the	next	bit.	No	matter	how	we	handle	it,	however,	we	have	introduced	an	error	into	our
system.
We	can	compute	the	relative	error	in	our	representation	by	taking	the	ratio	of	the	absolute	value	of	the
error	to	the	true	value	of	the	number.	Using	our	example	of	128.5,	we	find:
If	we	are	not	careful,	such	errors	can	propagate	through	a	lengthy	calculation,	causing	substantial	loss	of
precision.	Table	2.3	illustrates	the	error	propagation	as	we	iteratively	multiply	16.24	by	0.91	using	our
14-bit	simple	model.	Upon	converting	these	numbers	to	8-bit	binary,	we	see	that	we	have	a	substantial
error	from	the	outset.
As	 you	 can	 see,	 in	 six	 iterations,	 we	 have	 more	 than	 tripled	 the	 error	 in	 the	 product.	 Continued
iterations	will	produce	an	error	of	100%	because	the	product	eventually	goes	to	zero.	Although	this	14-bit
model	is	so	small	that	it	exaggerates	the	error,	all	floating-point	systems	behave	the	same	way.	There	is
always	some	degree	of	error	involved	when	representing	real	numbers	in	a	finite	system,	no	matter	how
large	 we	 make	 that	 system.	 Even	 the	 smallest	 error	 can	 have	 catastrophic	 results,	 particularly	 when
computers	are	used	to	control	physical	events	such	as	in	military	and	medical	applications.	The	challenge
to	 computer	 scientists	 is	 to	 find	 efficient	 algorithms	 for	 controlling	 such	 errors	 within	 the	 bounds	 of
performance	and	economics.
TABLE	2.3	Error	Propagation	in	a	14-Bit	Floating-Point	Number
2.5.4		The	IEEE-754	Floating-Point	Standard

The	floating-point	model	we	have	been	using	in	this	section	is	designed	for	simplicity	and	conceptual
understanding.	 We	 could	 extend	 this	 model	 to	 include	 whatever	 number	 of	 bits	 we	 wanted.	 Until	 the
1980s,	these	kinds	of	decisions	were	purely	arbitrary,	resulting	in	numerous	incompatible	representations
across	various	manufacturers’	systems.	In	1985,	the	IEEE	published	a	floating-point	standard	for	both
single-	 and	 double-precision	 floating-point	 numbers.	 This	 standard	 is	 officially	 known	 as	 IEEE-754
(1985)	and	includes	two	formats:	single	precision	and	double	precision.	The	IEEE-754	standard	not	only
defines	binary	floating-point	representations,	but	also	specifies	basic	operations,	exception	conditions,
conversions,	 and	 arithmetic.	 Another	 standard,	 IEEE	 854-1987,	 provides	 similar	 specifications	 for
decimal	arithmetic.	In	2008,	IEEE	revised	the	754	standard,	and	it	became	known	as	IEEE	754-2008.	It
carried	over	the	single	and	double	precision	from	754,	and	added	support	for	decimal	arithmetic	and
formats,	superseding	both	754	and	854.	We	discuss	only	the	single	and	double	representation	for	floating-
point	numbers.
The	 IEEE-754	 single-precision	 standard	 uses	 an	 excess	 127	 bias	 over	 an	 8-bit	 exponent.	 The
significand	assumes	an	implied	1	to	the	left	of	the	radix	point	and	is	23	bits.	This	implied	1	is	referred	to
as	the	hidden	bit	or	hidden	1	and	allows	an	actual	significand	of	23	+	1	=	24	bits.	With	the	sign	bit
included,	the	total	word	size	is	32	bits,	as	shown	in	Figure	2.2.
FIGURE	2.2	IEEE-754	Single-Precision	Floating-Point	Representation
We	mentioned	earlier	that	IEEE-754	makes	an	exception	to	the	rule	of	normalization.	Because	this
standard	assumes	an	implied	1	to	the	left	of	the	radix	point,	the	leading	bit	in	the	significand	can	indeed	be
zero.	For	example,	the	number	5.5	=	101.1	=	.1011	×	2
3
.	IEEE-754	assumes	an	implied	1	to	the	left	of	the
radix	point	and	thus	represents	5.5	as	1.011	×	2
2
.	Because	the	1	is	implied,	the	significand	is	011	and	does
not	begin	with	a	1.
Table	2.4	shows	the	single-precision	representation	of	several	floating-point	numbers,	including	some
special	ones.	One	should	note	that	zero	is	not	directly	representable	in	the	given	format,	because	of	a
required	hidden	bit	in	the	significand.	Therefore,	zero	is	a	special	value	denoted	using	an	exponent	of	all
zeros	and	a	significand	of	all	zeros.	IEEE-754	does	allow	for	both	–0	and	+0,	although	they	are	equal
values.	For	this	reason,	programmers	should	use	caution	when	comparing	a	floating-point	value	to	zero.

Floating-Point	NumberSingle-Precision	Representation
1.0			0	01111111	00000000000000000000000
0.5			0	01111110	00000000000000000000000
19.5			0	10000011	00111000000000000000000
–3.75			1	10000000	11100000000000000000000
Zero			0	00000000	00000000000000000000000
±	Infinity0/1	11111111	00000000000000000000000

NaN0/1	11111111	any	nonzero	significand
Denormalized	Number0/1	00000000	any	nonzero	significand
TABLE	2.4	Some	Example	IEEE-754	Single-Precision	Floating-Point	Numbers
When	the	exponent	is	255,	the	quantity	represented	is	±	infinity	(which	has	a	zero	significand)	or	“not
a	number”	(which	has	a	nonzero	significand).	“Not	a	number,”	or	NaN,	is	used	to	represent	a	value	that	is
not	a	real	number	(such	as	the	square	root	of	a	negative	number)	or	as	an	error	indicator	(such	as	in	a
“division	by	zero”	error).
Under	the	IEEE-754	standard,	most	numeric	values	are	normalized	and	have	an	implicit	leading	1	in
their	significands	(that	is	assumed	to	be	to	the	left	of	the	radix	point).	Another	important	convention	is
when	the	exponent	is	all	zeros	but	the	significand	is	nonzero.	This	represents	a	denormalized	number	in
which	there	is	no	hidden	bit	assumed.
FIGURE	2.3	Range	of	IEEE-754	Double-Precision	Numbers
The	largest	magnitude	value	we	can	represent	(forget	the	sign	for	the	time	being)	with	the	single-
precision	floating-point	format	is	2
127
	×	1.1111111111	1111111111111
2
	(let’s	call	this	value	MAX).	We
can’t	use	an	exponent	of	all	ones	because	that	is	reserved	for	NaN.	The	smallest	magnitude	number	we
can	represent	is	2
–127
	×	.00000000000000000000001
2
	(let’s	call	this	value	MIN).	We	can	use	an	exponent
of	all	zeros	(which	means	the	number	is	denormalized)	because	the	significand	is	nonzero	(and	represents
2
–23
).	Due	to	the	preceding	special	values	and	the	limited	number	of	bits,	there	are	four	numerical	ranges
that	 single-precision	 floating-point	 numbers	 cannot	 represent:	 negative	 numbers	 less	 than	 –MAX
(negative	overflow);	negative	numbers	greater	than	–MIN	(negative	underflow);	positive	numbers	less
than	+MIN	(positive	underflow);	and	positive	numbers	greater	than	+MAX	(positive	overflow).
Double-precision	numbers	use	a	signed	64-bit	word	consisting	of	an	11-bit	exponent	and	a	52-bit
significand.	The	bias	is	1023.	The	range	of	numbers	that	can	be	represented	in	the	IEEE	double-precision
model	is	shown	in	Figure	2.3.	NaN	is	indicated	when	the	exponent	is	2047.	Representations	for	zero	and
infinity	correspond	to	the	single-precision	model.
At	 a	 slight	 cost	 in	 performance,	 most	 FPUs	 use	 only	 the	 64-bit	 model	 so	 that	 only	 one	 set	 of
specialized	circuits	needs	to	be	designed	and	implemented.
Virtually	every	recently	designed	computer	system	has	adopted	the	IEEE-754	floating-point	model.
Unfortunately,	by	the	time	this	standard	came	along,	many	mainframe	computer	systems	had	established
their	own	floating-point	systems.	Changing	to	the	newer	system	has	taken	decades	for	well-established
architectures	such	as	IBM	mainframes,	which	now	support	both	their	traditional	floating-point	system	and
IEEE-754.	Before	1998,	however,	IBM	systems	had	been	using	the	same	architecture	for	floating-point
arithmetic	that	the	original	System/360	used	in	1964.	One	would	expect	that	both	systems	will	continue	to

be	supported,	owing	to	the	substantial	amount	of	older	software	that	is	running	on	these	systems.
2.5.5		Range,	Precision,	and	Accuracy
When	 discussing	 floating-point	 numbers	 it	 is	 important	 to	 understand	 the	 terms	range,	precision,	 and
accuracy.	Range	is	very	straightforward,	because	it	represents	the	interval	from	the	smallest	value	in	a
given	format	to	the	largest	value	in	that	same	format.	For	example,	the	range	of	16-bit	two’s	complement
integers	is	–32768	to	+32767.	The	range	of	IEEE-754	double-precision	floating-point	numbers	is	given	in
Figure	2.3.	Even	with	this	large	range,	we	know	there	are	infinitely	many	numbers	that	do	not	exist	within
the	range	specified	by	IEEE-754.	The	reason	floating-point	numbers	work	at	all	is	that	there	will	always
be	a	number	in	this	range	that	is	close	to	the	number	you	want.
People	have	no	problem	understanding	range,	but	accuracy	and	precision	are	often	confused	with	each
other.	Accuracy	refers	to	how	close	a	number	is	to	its	true	value;	for	example,	we	can’t	represent	0.1	in
floating	point,	but	we	can	find	a	number	in	the	range	that	is	relatively	close,	or	reasonably	accurate,	to
0.1.	Precision,	on	the	other	hand,	deals	with	how	much	information	we	have	about	a	value	and	the	amount
of	 information	 used	 to	 represent	 the	 value.	 1.666	 is	 a	 number	 with	 four	 decimal	 digits	 of	 precision;
1.6660	is	the	same	exact	number	with	five	decimal	digits	of	precision.	The	second	number	is	not	more
accurate	than	the	first.
Accuracy	must	be	put	into	context—to	know	how	accurate	a	value	is,	one	must	know	how	close	it	is
to	its	intended	target	or	“true	value.”	We	can’t	look	at	two	numbers	and	immediately	declare	that	the	first
is	more	accurate	than	the	second	simply	because	the	first	has	more	digits	of	precision.
Although	they	are	separate,	accuracy	and	precision	are	related.	Higher	precision	often	allows	a	value
to	be	more	accurate,	but	that	is	not	always	the	case.	For	example,	we	can	represent	the	value	1	as	an
integer,	 a	 single-precision	 floating	 point,	 or	 a	 double-precision	 floating	 point,	 but	 each	 is	 equally
(exactly)	 accurate.	 As	 another	 example,	 consider	 3.13333	 as	 an	 estimate	 for	 pi.	 It	 has	 6	 digits	 of
precision,	 yet	 is	 accurate	 to	 only	 two	 digits.	 Adding	 more	 precision	 will	 do	 nothing	 to	 increase	 the
accuracy.
On	the	other	hand,	when	multiplying	0.4	×	0.3,	our	accuracy	depends	on	our	precision.	If	we	allow
only	one	decimal	place	for	precision,	our	result	is	0.1	(which	is	close	to,	but	not	exactly,	the	product).	If
we	allow	two	decimal	places	of	precision,	we	get	0.12,	which	accurately	reflects	the	answer.
2.5.6		Additional	Problems	with	Floating-Point	Numbers
We	 have	 seen	 that	 floating-point	 numbers	 can	 overflow	 and	 underflow.	 In	 addition,	 we	 know	 that	 a
floating-point	number	may	not	exactly	represent	the	value	we	wish,	as	is	the	case	with	the	rounding	error
that	occurs	with	the	binary	floating-point	representation	for	the	decimal	number	0.1.	As	we	have	seen,
these	rounding	errors	can	propagate,	resulting	in	substantial	problems.
Although	rounding	is	undesirable,	it	is	understandable.	In	addition	to	this	rounding	problem,	however,
floating-point	 arithmetic	 differs	 from	 real	 number	 arithmetic	 in	 two	 relatively	 disturbing,	 and	 not
necessarily	intuitive,	ways.	First,	floating-point	arithmetic	is	not	always	associative.	This	means	that	for
three	floating-point	numbers	a,	b,	and	c,
(a	+	b)	+	c	≠	a	+	(b	+	c)
The	same	holds	true	for	associativity	under	multiplication.	Although	in	many	cases	the	left-hand	side	will
equal	the	right-hand	side,	there	is	no	guarantee.	Floating-point	arithmetic	is	also	not	distributive:

a	×	(b)	+	c)	≠	ab	+	ac
Although	results	can	vary	depending	on	compiler	(we	used	Gnu	C),	declaring	the	doubles	a	=	0.1,	b	=	0.2,
and	c	=	0.3	illustrates	the	above	inequalities	nicely.	We	encourage	you	to	find	three	additional	floating-
point	numbers	to	illustrate	that	floating-point	arithmetic	is	neither	associative	nor	distributive.
What	does	this	all	mean	to	you	as	a	programmer?	Programmers	should	use	extra	care	when	using	the
equality	 operator	 on	 floating-point	 numbers.	 This	 implies	 that	 they	 should	 be	 avoided	 in	 controlling
looping	structures	such	as	do...while	and	for	loops.	It	is	good	practice	to	declare	a	“nearness	to	x”
epsilon	(e.g.,	epsilon	=	1.0	×	10
–20
)	and	then	test	an	absolute	value.
For	example,	instead	of	using:
if	x	=	2	then...
it	is	better	to	use:
Floating-Point	Ops	or	Oops?
In	 this	 chapter,	 we	 have	 introduced	 floating-point	 numbers	 and	 the	 means	 by	 which	 computers
represent	them.	We	have	touched	upon	floating-point	rounding	errors	(studies	in	numerical	analysis
will	 provide	 further	 depth	 on	 this	 topic)	 and	 the	 fact	 that	 floating-point	 numbers	 don’t	 obey	 the
standard	 associative	 and	 distributive	 laws.	 But	 just	 how	 serious	 are	 these	 issues?	 To	 answer	 this
question,	we	introduce	three	major	floating-point	blunders.
In	1994,	when	Intel	introduced	the	Pentium	microprocessor,	number	crunchers	around	the	world
noticed	 something	 weird	 was	 happening.	 Calculations	 involving	 double-precision	 divisions	 and
certain	bit	patterns	were	producing	incorrect	results.	Although	the	flawed	chip	was	slightly	inaccurate
for	some	pairs	of	numbers,	other	instances	were	more	extreme.	For	example,	if	x	=	4,195,835	and	y	=
3,145,727,	finding	z	=	x	–	(x/y)	×	y	should	produce	a	z	of	0.	The	Intel	286,	386,	and	486	chips	gave
exactly	that	result.	Even	taking	into	account	the	possibility	of	floating-point	round-off	error,	the	value
of	z	should	have	been	about	9.3	×	10
–10
.	But	on	the	new	Pentium,	z	was	equal	to	256!
Once	Intel	was	informed	of	the	problem,	research	and	testing	revealed	the	flaw	to	be	an	omission
in	the	chip’s	design.	The	Pentium	was	using	the	radix-4	SRT	algorithm	for	speedy	division,	which
necessitated	a	1066-element	table.	Once	implemented	in	silicon,	5	of	those	table	entries	were	0	and
should	have	been	+2.
Although	the	Pentium	bug	was	a	public	relations	debacle	for	Intel,	it	was	not	a	catastrophe	for
those	using	the	chip.	In	fact,	it	was	a	minor	thing	compared	to	the	programming	mistakes	with	floating-
point	numbers	that	have	resulted	in	disasters	in	areas	from	off-shore	oil	drilling,	to	stock	markets,	to
missile	defense.	The	list	of	actual	disasters	that	resulted	from	floating-point	errors	is	very	long.	The
following	two	instances	are	among	the	worst	of	them.
During	the	Persian	Gulf	War	of	1991,	the	United	States	relied	on	Patriot	missiles	to	track	and
intercept	cruise	missiles	and	Scud	missiles.	One	of	these	missiles	failed	to	track	an	incoming	Scud
missile,	allowing	the	Scud	to	hit	an	American	army	barracks,	killing	28	people	and	injuring	many
more.	After	an	investigation,	it	was	determined	that	the	failure	of	the	Patriot	missile	was	due	to	using

too	little	precision	to	allow	the	missile	to	accurately	determine	the	incoming	Scud	velocity.
The	 Patriot	 missile	 uses	 radar	 to	 determine	 the	 location	 of	 an	 object.	 If	 the	 internal	 weapons
control	 computer	 identifies	 the	 object	 as	 something	 that	 should	 be	 intercepted,	 calculations	 are
performed	 to	 predict	 the	 air	 space	 in	 which	 the	 object	 should	 be	 located	 at	 a	 specific	 time.	 This
prediction	is	based	on	the	object’s	known	velocity	and	time	of	last	detection.
The	problem	was	in	the	clock,	which	measured	time	in	tenths	of	seconds.	But	the	time	since	boot
was	stored	as	an	integer	number	of	seconds	(determined	by	multiplying	the	elapsed	time	by	1/10).	For
predicting	 where	 an	 object	 would	 be	 at	 a	 specific	 time,	 the	 time	 and	 velocity	 needed	 to	 be	 real
numbers.	It	was	no	problem	to	convert	the	integer	to	a	real	number;	however,	using	24-bit	registers	for
its	calculations,	the	Patriot	was	limited	in	the	precision	of	this	operation.	The	potential	problem	is
easily	seen	when	one	realizes	1/10	in	binary	is:
0.0001100110011001100110011001100	...
When	the	elapsed	time	was	small,	this	“chopping	error”	was	insignificant	and	caused	no	problems.
The	Patriot	was	designed	to	be	on	for	only	a	few	minutes	at	a	time,	so	this	limit	of	24-bit	precision
would	be	of	no	consequence.	The	problem	was	that	during	the	Gulf	War,	the	missiles	were	on	for	days.
The	longer	a	missile	was	on,	the	larger	the	error	became,	and	the	more	probable	that	the	inaccuracy	of
the	 prediction	 calculation	 would	 cause	 an	 unsuccessful	 interception.	 And	 this	 is	 precisely	 what
happened	on	February	25,	1991,	when	a	failed	interception	resulted	in	28	people	killed—a	failed
interception	 caused	 by	 loss	 of	 precision	 (required	 for	 accuracy)	 in	 floating-point	 numbers.	 It	 is
estimated	that	the	Patriot	missile	had	been	operational	about	100	hours,	introducing	a	rounding	error	in
the	time	conversion	of	about	0.34	seconds,	which	translates	to	approximately	half	a	kilometer	of	travel
for	a	Scud	missile.
Designers	were	aware	of	the	conversion	problem	well	before	the	incident	occurred.	However,
deploying	new	software	under	wartime	conditions	is	anything	but	trivial.	Although	the	new	software
would	have	fixed	the	bug,	field	personnel	could	have	simply	rebooted	the	systems	at	specific	intervals
to	keep	the	clock	value	small	enough	so	that	24-bit	precision	would	have	been	sufficient.
One	of	the	most	famous	examples	of	a	floating-point	numeric	disaster	is	the	explosion	of	the	Ariane
5	rocket.	On	June	4,	1996,	the	unmanned	Ariane	5	was	launched	by	the	European	Space	Agency.	Forty
seconds	 after	 liftoff,	 the	 rocket	 exploded,	 scattering	 a	 $500	 million	 cargo	 across	 parts	 of	 French
Guiana.	Investigation	revealed	perhaps	one	of	the	most	devastatingly	careless	but	efficient	software
bugs	 in	 the	 annals	 of	 computer	 science—a	 floating-point	 conversion	 error.	 The	 rocket’s	 inertial
reference	system	converted	a	64-bit	floating-point	number	(dealing	with	the	horizontal	velocity	of	the
rocket)	to	a	16-bit	signed	integer.	However,	the	particular	64-bit	floating-point	number	to	be	converted
was	larger	than	32,767	(the	largest	integer	that	can	be	stored	in	16-bit	signed	representation),	so	the
conversion	process	failed.	The	rocket	tried	to	make	an	abrupt	course	correction	for	a	wrong	turn	that	it
had	never	taken,	and	the	guidance	system	shut	down.	Ironically,	when	the	guidance	system	shut	down,
control	reverted	to	a	backup	unit	installed	in	the	rocket	in	case	of	just	such	a	failure,	but	the	backup
system	was	running	the	same	flawed	software.
It	seems	obvious	that	a	64-bit	floating-point	number	could	be	much	larger	than	32,767,	so	how	did
the	rocket	programmers	make	such	a	glaring	error?	They	decided	the	velocity	value	would	never	get
large	enough	to	be	a	problem.	Their	reasoning?	It	had	never	gotten	too	large	before.	Unfortunately,	this
rocket	was	faster	than	all	previous	rockets,	resulting	in	a	larger	velocity	value	than	the	programmers
expected.	One	of	the	most	serious	mistakes	a	programmer	can	make	is	to	accept	the	old	adage	“But

we’ve	always	done	it	that	way.”
Computers	are	everywhere—in	our	washing	machines,	our	televisions,	our	microwaves,	even	our
cars.	We	certainly	hope	the	programmers	who	work	on	computer	software	for	our	cars	don’t	make	such
hasty	 assumptions.	 With	 approximately	 15	 to	 60	 microprocessors	 in	 all	 new	 cars	 that	 roll	 off	 the
assembly	 line	 and	 innumerable	 processors	 in	 commercial	 aircraft	 and	 medical	 equipment,	 a	 deep
understanding	of	floating-point	anomalies	can	quite	literally	be	a	lifesaver.
2.6			CHARACTER	CODES
We	have	seen	how	digital	computers	use	the	binary	system	to	represent	and	manipulate	numeric	values.
We	 have	 yet	 to	 consider	 how	 these	 internal	 values	 can	 be	 converted	 to	 a	 form	 that	 is	 meaningful	 to
humans.	The	manner	in	which	this	is	done	depends	on	both	the	coding	system	used	by	the	computer	and
how	the	values	are	stored	and	retrieved.
2.6.1		Binary-Coded	Decimal
For	many	applications,	we	need	the	exact	binary	equivalent	of	the	decimal	system,	which	means	we	need
an	encoding	for	individual	decimal	digits.	This	is	precisely	the	case	in	many	business	applications	that
deal	 with	 money—we	 can’t	 afford	 the	 rounding	 errors	 that	 occur	 when	 we	 convert	 real	 numbers	 to
floating	point	when	making	financial	transactions!
Binary-coded	 decimal	 (BCD)	 is	 very	 common	 in	 electronics,	 particularly	 those	 that	 display
numerical	data,	such	as	alarm	clocks	and	calculators.	BCD	encodes	each	digit	of	a	decimal	number	into	a
4-bit	binary	form.	Each	decimal	digit	is	individually	converted	to	its	binary	equivalent,	as	seen	in	Table
2.5.	For	example,	to	encode	146,	the	decimal	digits	are	replaced	by	0001,	0100,	and	0110,	respectively.
Because	most	computers	use	bytes	as	the	smallest	unit	of	access,	most	values	are	stored	in	8	bits,	not
4.	That	gives	us	two	choices	for	storing	4-bit	BCD	digits.	We	can	ignore	the	cost	of	extra	bits	and	pad	the
high-order	nibbles	with	zeros	(or	ones),	forcing	each	decimal	digit	to	be	replaced	by	8	bits.	Using	this
approach,	 padding	 with	 zeros,	 146	 would	 be	 stored	 as	 00000001	 00000100	 00000110.	 Clearly,	 this
approach	is	quite	wasteful.	The	second	approach,	called	packed	BCD,	stores	two	digits	per	byte.	Packed
decimal	format	allows	numbers	to	be	signed,	but	instead	of	putting	the	sign	at	the	beginning,	the	sign	is
stored	at	the	end.	The	standard	values	for	this	“sign	digit”	are	1100	for	+,	1101	for	–,	and	1111	to	indicate
that	 the	 value	 is	 unsigned	 (see	Table	 2.5).	 Using	 packed	 decimal	 format,	 +146	 would	 be	 stored	 as
00010100	01101100.	Padding	would	still	be	required	for	an	even	number	of	digits.	Note	that	if	a	number
has	a	decimal	point	(as	with	monetary	values),	this	is	not	stored	in	the	BCD	representation	of	the	number
and	must	be	retained	by	the	application	program.
Another	variation	of	BCD	is	zoned	decimal	format.	Zoned	decimal	representation	stores	a	decimal
digit	 in	 the	 low-order	 nibble	 of	 each	 byte,	 which	 is	 exactly	 the	 same	 as	 unpacked	 decimal	 format.
However,	instead	of	padding	the	high-order	nibbles	with	zeros,	a	specific	pattern	is	used.	There	are	two
choices	for	the	high-order	nibble,	called	the	numeric	zone.	EBCDIC	zoned	decimal	format	requires	the
zone	 to	 be	 all	 ones	 (hexadecimal	 F).	ASCII	 zoned	 decimal	 format	 requires	 the	 zone	 to	 be	 0011
(hexadecimal	 3).	 (See	 the	 next	 two	 sections	 for	 detailed	 explanations	 of	 EBCDIC	 and	 ASCII.)	 Both
formats	allow	for	signed	numbers	(using	the	sign	digits	found	in	Table	2.5)	and	typically	expect	the	sign	to
be	located	in	the	high-order	nibble	of	the	least	significant	byte	(although	the	sign	could	be	a	completely
separate	byte).	For	example,	+146	in	EBCDIC	zoned	decimal	format	is	11110001	11110100	11000110

(note	that	the	high-order	nibble	of	the	last	byte	is	the	sign).	In	ASCII	zoned	decimal	format,	+146	is
00110001	00110100	11000110.

DigitBCD
0
1
2
3
4
5
6
7
8
9
0000
0001
0010
0011
0100
0101
0110
0111
1000
1001
Zones
1111
1100
1101
Unsigned
Positive
Negative
TABLE	2.5	Binary-Coded	Decimal
Note	from	Table	2.5	that	six	of	the	possible	binary	values	are	not	used—1010	through	1111.	Although
it	may	appear	that	nearly	40%	of	our	values	are	going	to	waste,	we	are	gaining	a	considerable	advantage
in	accuracy.	For	example,	the	number	0.3	is	a	repeating	decimal	when	stored	in	binary.	Truncated	to	an	8-
bit	fraction,	it	converts	back	to	0.296875,	giving	us	an	error	of	approximately	1.05%.	In	EBCDIC	zoned
decimal	BCD,	the	number	is	stored	directly	as	1111	0011	(we	are	assuming	the	decimal	point	is	implied
by	the	data	format),	giving	no	error	at	all.
	EXAMPLE	2.37	Represent	–1265	using	packed	BCD	and	EBCDIC	zoned	decimal.
The	4-bit	BCD	representation	for	1265	is:
0001	0010	0110	0101
Adding	the	sign	after	the	low-order	digit	and	padding	the	high-order	bit	with	0000,	we	have:
The	EBCDIC	zoned	decimal	representation	requires	4	bytes:
The	sign	bit	is	shaded	in	both	representations.

2.6.2		EBCDIC
Before	the	development	of	the	IBM	System/360,	IBM	had	used	a	6-bit	variation	of	BCD	for	representing
characters	and	numbers.	This	code	was	severely	limited	in	how	it	could	represent	and	manipulate	data;	in
fact,	lowercase	letters	were	not	part	of	its	repertoire.	The	designers	of	the	System/360	needed	more
information	processing	capability	as	well	as	a	uniform	manner	in	which	to	store	both	numbers	and	data.
To	maintain	compatibility	with	earlier	computers	and	peripheral	equipment,	the	IBM	engineers	decided
that	it	would	be	best	to	simply	expand	BCD	from	6	bits	to	8	bits.	Accordingly,	this	new	code	was	called
Extended	Binary	Coded	Decimal	Interchange	Code	(EBCDIC).	IBM	continues	to	use	EBCDIC	in	IBM
mainframe	and	midrange	computer	systems;	however,	IBM’s	AIX	operating	system	(found	on	the	RS/6000
and	its	successors)	and	operating	systems	for	the	IBM	PC	use	ASCII.	The	EBCDIC	code	is	shown	in
Table	 2.6	 in	 zone-digit	 form.	 Characters	 are	 represented	 by	 appending	 digit	 bits	 to	 zone	 bits.	 For
example,	the	character	a	 is	 1000	 0001	 and	 the	 digit	 3	 is	 1111	 0011	 in	 EBCDIC.	 Note	 that	 the	 only
difference	between	uppercase	and	lowercase	characters	is	in	bit	position	2,	making	a	translation	from
uppercase	to	lowercase	(or	vice	versa)	a	simple	matter	of	flipping	one	bit.	Zone	bits	also	make	it	easier
for	a	programmer	to	test	the	validity	of	input	data.
2.6.3		ASCII
While	IBM	was	busy	building	its	iconoclastic	System/360,	other	equipment	makers	were	trying	to	devise
better	 ways	 for	 transmitting	 data	 between	 systems.	 The	American	 Standard	 Code	 for	 Information
Interchange	(ASCII)	is	one	outcome	of	those	efforts.	ASCII	is	a	direct	descendant	of	the	coding	schemes
used	for	decades	by	teletype	(telex)	devices.	These	devices	used	a	5-bit	(Murray)	code	that	was	derived
from	the	Baudot	code,	which	was	invented	in	the	1880s.	By	the	early	1960s,	the	limitations	of	the	5-bit
codes	were	becoming	apparent.	The	International	Organization	for	Standardization	devised	a	7-bit	coding
scheme	that	it	called	International	Alphabet	Number	5.	In	1967,	a	derivative	of	this	alphabet	became	the
official	standard	that	we	now	call	ASCII.
As	you	can	see	in	Table	2.7,	ASCII	defines	codes	for	32	control	characters,	10	digits,	52	letters
(uppercase	and	lowercase),	32	special	characters	(such	as	$	and	#),	and	the	space	character.	The	high-
order	(eighth)	bit	was	intended	to	be	used	for	parity.
Parity	is	the	most	basic	of	all	error-detection	schemes.	It	is	easy	to	implement	in	simple	devices	like
teletypes.	A	parity	bit	is	turned	“on”	or	“off”	depending	on	whether	the	sum	of	the	other	bits	in	the	byte	is
even	or	odd.	For	example,	if	we	decide	to	use	even	parity	and	we	are	sending	an	ASCII	A,	the	lower	7
bits	are	100	0001.	Because	the	sum	of	the	bits	is	even,	the	parity	bit	would	be	set	to	off	and	we	would
transmit	0100	0001.	Similarly,	if	we	transmit	an	ASCII	C,	100	0011,	the	parity	bit	would	be	set	to	on
before	we	sent	the	8-bit	byte,	1100	0011.	Parity	can	be	used	to	detect	only	single-bit	errors.	We	will
discuss	more	sophisticated	error-detection	methods	in	Section	2.7.
To	 allow	 compatibility	 with	 telecommunications	 equipment,	 computer	 manufacturers	 gravitated
toward	the	ASCII	code.	As	computer	hardware	became	more	reliable,	however,	the	need	for	a	parity	bit
began	to	fade.	In	the	early	1980s,	microcomputer	and	microcomputer-peripheral	makers	began	to	use	the
parity	bit	to	provide	an	“extended”	character	set	for	values	between	128
10
	and	255
10
.
Depending	on	the	manufacturer,	the	higher-valued	characters	could	be	anything	from	mathematical
symbols	to	characters	that	form	the	sides	of	boxes	to	foreign-language	characters	such	as	ñ.	Unfortunately,
no	number	of	clever	tricks	can	make	ASCII	a	truly	international	interchange	code.

2.6.4		Unicode
Both	 EBCDIC	 and	 ASCII	 were	 built	 around	 the	 Latin	 alphabet.	 As	 such,	 they	 are	 restricted	 in	 their
abilities	to	provide	data	representation	for	the	non-Latin	alphabets	used	by	the	majority	of	the	world’s
population.	As	all	countries	began	using	computers,	each	was	devising	codes	that	would	most	effectively
represent	their	native	languages.	None	of	these	was	necessarily	compatible	with	any	others,	placing	yet
another	barrier	in	the	way	of	the	emerging	global	economy.
TABLE	2.6	The	EBCDIC	Code	(Values	Given	in	Binary	Zone-Digit	Format)

TABLE	2.7	The	ASCII	Code	(Values	Given	in	Decimal)
In	1991,	before	things	got	too	far	out	of	hand,	a	consortium	of	industry	and	public	leaders	was	formed
to	establish	a	new	international	information	exchange	code	called	Unicode.	This	group	is	appropriately
called	the	Unicode	Consortium.
Unicode	is	a	16-bit	alphabet	that	is	downward	compatible	with	ASCII	and	the	Latin-1	character	set.	It
is	conformant	with	the	ISO/IEC	10646-1	international	alphabet.	Because	the	base	coding	of	Unicode	is	16
bits,	it	has	the	capacity	to	encode	the	majority	of	characters	used	in	every	language	of	the	world.	If	this
weren’t	 enough,	 Unicode	 also	 defines	 an	 extension	 mechanism	 that	 will	 allow	 for	 the	 coding	 of	 an
additional	million	characters.	This	is	sufficient	to	provide	codes	for	every	written	language	in	the	history
of	civilization.
The	 Unicode	 codespace	 consists	 of	 five	 parts,	 as	 shown	 in	Table	 2.8.	 A	 full	 Unicode-compliant
system	 will	 also	 allow	 formation	 of	 composite	 characters	 from	 the	 individual	 codes,	 such	 as	 the

combination	of	 ́	and	A	to	form	Á.	The	algorithms	used	for	these	composite	characters,	as	well	as	the
Unicode	extensions,	can	be	found	in	the	references	at	the	end	of	this	chapter.
Although	 Unicode	 has	 yet	 to	 become	 the	 exclusive	 alphabet	 of	 American	 computers,	 most
manufacturers	are	including	at	least	some	limited	support	for	it	in	their	systems.	Unicode	is	currently	the
default	character	set	of	the	Java	programming	language.	Ultimately,	the	acceptance	of	Unicode	by	all
manufacturers	will	depend	on	how	aggressively	they	wish	to	position	themselves	as	international	players
and	 how	 inexpensively	 disk	 drives	 can	 be	 produced	 to	 support	 an	 alphabet	 with	 double	 the	 storage
requirements	of	ASCII	or	EBCDIC.
TABLE	2.8	Unicode	Codespace
2.7			ERROR	DETECTION	AND	CORRECTION
No	 communications	 channel	 or	 storage	 medium	 can	 be	 completely	 error-free.	 It	 is	 a	 physical
impossibility.	As	transmission	rates	are	increased,	bit	timing	gets	tighter.	As	more	bits	are	packed	per
square	millimeter	of	storage,	magnetic	flux	densities	increase.	Error	rates	increase	in	direct	proportion	to
the	number	of	bits	per	second	transmitted,	or	the	number	of	bits	per	square	millimeter	of	magnetic	storage.
In	Section	2.6.3,	we	mentioned	that	a	parity	bit	could	be	added	to	an	ASCII	byte	to	help	determine
whether	any	of	the	bits	had	become	corrupted	during	transmission.	This	method	of	error	detection	is
limited	in	its	effectiveness:	Simple	parity	can	detect	only	an	odd	number	of	errors	per	byte.	If	two	errors
occur,	we	are	helpless	to	detect	a	problem.	Nonsense	could	pass	for	good	data.	If	such	errors	occur	in
sending	financial	information	or	program	code,	the	effects	can	be	disastrous.
As	you	read	the	sections	that	follow,	you	should	keep	in	mind	that	just	as	it	is	impossible	to	create	an

error-free	medium,	it	is	also	impossible	to	detect	or	correct	100%	of	all	errors	that	could	occur	in	a
medium.	 Error	 detection	 and	 correction	 is	 yet	 another	 study	 in	 the	 trade-offs	 that	 one	 must	 make	 in
designing	computer	systems.	The	well-constructed	error	control	system	is	therefore	a	system	where	a
“reasonable”	number	of	the	“reasonably”	expected	errors	can	be	detected	or	corrected	within	the	bounds
of	“reasonable”	economics.	(Note:	The	word	reasonable	is	implementation-dependent.)
2.7.1		Cyclic	Redundancy	Check
Checksums	are	used	in	a	wide	variety	of	coding	systems,	from	bar	codes	to	International	Standard	Book
Numbers.	These	are	self-checking	codes	that	will	quickly	indicate	whether	the	preceding	digits	have	been
misread.	 A	cyclic	 redundancy	 check	 (CRC)	 is	 a	 type	 of	 checksum	 used	 primarily	 in	 data
communications	 that	 determines	 whether	 an	 error	 has	 occurred	 within	 a	 large	 block	 or	 stream	 of
information	 bytes.	 The	 larger	 the	 block	 to	 be	 checked,	 the	 larger	 the	 checksum	 must	 be	 to	 provide
adequate	protection.	Checksums	and	CRCs	are	types	of	systematic	error	detection	schemes,	meaning
that	the	error-checking	bits	are	appended	to	the	original	information	byte.	The	group	of	error-checking
bits	 is	 called	 a	syndrome.	 The	 original	 information	 byte	 is	 unchanged	 by	 the	 addition	 of	 the	 error-
checking	bits.
The	word	cyclic	in	cyclic	redundancy	check	refers	to	the	abstract	mathematical	theory	behind	this
error	 control	 system.	 Although	 a	 discussion	 of	 this	 theory	 is	 beyond	 the	 scope	 of	 this	 text,	 we	 can
demonstrate	how	the	method	works	to	aid	in	your	understanding	of	its	power	to	economically	detect
transmission	errors.
Arithmetic	Modulo	2
You	may	be	familiar	with	integer	arithmetic	taken	over	a	modulus.	Twelve-hour	clock	arithmetic	is	a
modulo	12	system	that	you	use	every	day	to	tell	time.	When	we	add	2	hours	to	11:00,	we	get	1:00.
Arithmetic	modulo	2	uses	two	binary	operands	with	no	borrows	or	carries.	The	result	is	likewise	binary
and	is	also	a	member	of	the	modulus	2	system.	Because	of	this	closure	under	addition,	and	the	existence
of	identity	elements,	mathematicians	say	that	this	modulo	2	system	forms	an	algebraic	field.
The	addition	rules	are	as	follows:
0	+	0	=	0
0	+	1	=	1
1	+	0	=	1
1	+	1	=	0
	EXAMPLE	2.38	Find	the	sum	of	1011
2
	and	110
2
	modulo	2.
This	sum	makes	sense	only	in	modulo	2.
Modulo	2	division	operates	through	a	series	of	partial	sums	using	the	modulo	2	addition	rules.	Example
2.39	illustrates	the	process.

	EXAMPLE	2.39	Find	the	quotient	and	remainder	when	1001011
2
	is	divided	by	1011
2
.
The	quotient	is	1010
2
.
Arithmetic	 operations	 over	 the	 modulo	 2	 field	 have	 polynomial	 equivalents	 that	 are	 analogous	 to
polynomials	over	the	field	of	integers.	We	have	seen	how	positional	number	systems	represent	numbers	in
increasing	powers	of	a	radix,	for	example,
1011
2
	=	1	×	2
3
	+	0	×	2
2
	+	1	×	2
1
	+	1	×	2
0
.
By	letting	X	=	2,	the	binary	number	1011
2
	becomes	shorthand	for	the	polynomial:
1	×	X
3
	+	0	×	X
2
	+	1	×	X
1
	+	1	×	X
0
.
The	division	performed	in	Example	2.39	then	becomes	the	polynomial	operation:
Calculating	and	Using	CRCs
With	that	lengthy	preamble	behind	us,	we	can	now	proceed	to	show	how	CRCs	are	constructed.	We	will
do	this	by	example:
1.		Let	the	information	byte	I	=	1001011
2
.	(Any	number	of	bytes	can	be	used	to	form	a	message	block.)
2.		The	sender	and	receiver	agree	upon	an	arbitrary	binary	pattern,	say,	P	=	1011
2
.	(Patterns	beginning
and	ending	with	1	work	best.)
3.		Shift	I	to	the	left	by	one	less	than	the	number	of	bits	in	P,	giving	a	new	I	=	1001011000
2
.
4.		Using	I	as	a	dividend	and	P	as	a	divisor,	perform	the	modulo	2	division	(as	shown	in	Example	2.39).
We	 ignore	 the	 quotient	 and	 note	 that	 the	 remainder	 is	 100
2
.	 The	 remainder	 is	 the	 actual	 CRC
checksum.
5.		Add	the	remainder	to	I,	giving	the	message	M:

1001011000
2
	+	100
2
	=	1001011100
2
6.		M	is	decoded	and	checked	by	the	message	receiver	using	the	reverse	process.	Only	now	P	divides	M
exactly:
Note:	The	reverse	process	would	include	appending	the	remainder.
A	remainder	other	than	zero	indicates	that	an	error	has	occurred	in	the	transmission	of	M.	This	method
works	best	when	a	large	prime	polynomial	is	used.	There	are	four	standard	polynomials	used	widely	for
this	purpose:
•			CRC-CCITT	(ITU-T):	X
16
	+	X
12
	+	X
5
	+	1
•			CRC-12:	X
12
	+	X
11
	+	X
3
	+	X
2
	+	X	+	1
•			CRC-16	(ANSI):	X
16
	+	X
15
	+	X
2
	+	1
•			CRC-32:	X
32
	+	X
26
	+	X
23
	+	X
22
	+	X
16
	+	X
12
	+	X
11
	+	X
10
	+	X
8
	+	X
7
	+	X
5
	+	X
4
	+	X	+	1
CRC-CCITT,	 CRC-12,	 and	 CRC-16	 operate	 over	 pairs	 of	 bytes;	 CRC-32	 uses	 four	 bytes,	 which	 is
appropriate	for	systems	operating	on	32-bit	words.	It	has	been	proven	that	CRCs	using	these	polynomials
can	detect	more	than	99.8%	of	all	single-bit	errors.
CRCs	can	be	implemented	effectively	using	lookup	tables	as	opposed	to	calculating	the	remainder
with	each	byte.	The	remainder	generated	by	each	possible	input	bit	pattern	can	be	“burned”	directly	into
communications	and	storage	electronics.	The	remainder	can	then	be	retrieved	using	a	1-cycle	lookup	as
compared	to	a	16-	or	32-cycle	division	operation.	Clearly,	the	trade-off	is	in	speed	versus	the	cost	of
more	complex	control	circuitry.
2.7.2		Hamming	Codes
Data	communications	channels	are	simultaneously	more	error-prone	and	more	tolerant	of	errors	than	disk
systems.	 In	 data	 communications,	 it	 is	 sufficient	 to	 have	 only	 the	 ability	 to	 detect	 errors.	 If	 a
communications	device	determines	that	a	message	contains	an	erroneous	bit,	all	it	has	to	do	is	request
retransmission.	Storage	systems	and	memory	do	not	have	this	luxury.	A	disk	can	sometimes	be	the	sole
repository	 of	 a	 financial	 transaction	 or	 other	 collection	 of	 nonreproducible	 real-time	 data.	 Storage
devices	and	memory	must	therefore	have	the	ability	to	not	only	detect	but	to	correct	a	reasonable	number
of	errors.
Error-recovery	coding	has	been	studied	intensively	over	the	past	century.	One	of	the	most	effective
codes—and	the	oldest—is	the	Hamming	code.	Hamming	codes	are	an	adaptation	of	the	concept	of	parity,

whereby	error	detection	and	correction	capabilities	are	increased	in	proportion	to	the	number	of	parity
bits	added	to	an	information	word.	Hamming	codes	are	used	in	situations	where	random	errors	are	likely
to	 occur.	 With	 random	 errors,	 we	 assume	 each	 bit	 failure	 has	 a	 fixed	 probability	 of	 occurrence
independent	of	other	bit	failures.	It	is	common	for	computer	memory	to	experience	such	errors,	so	in	our
following	 discussion,	 we	 present	 Hamming	 codes	 in	 the	 context	 of	 memory	 bit	 error	 detection	 and
correction.
We	mentioned	that	Hamming	codes	use	parity	bits,	also	called	check	bits	or	redundant	bits.	The
memory	word	itself	consists	of	m	bits,	but	r	redundant	bits	are	added	to	allow	for	error	detection	and/or
correction.	Thus,	the	final	word,	called	a	code	word,	is	an	n-bit	unit	containing	m	data	bits	and	r	check
bits.	There	exists	a	unique	code	word	consisting	of	n	=	m	+	r	bits	for	each	data	word	as	follows:
The	number	of	bit	positions	in	which	two	code	words	differ	is	called	the	Hamming	distance	of	those	two
code	words.	For	example,	if	we	have	the	following	two	code	words:
we	see	that	they	differ	in	3	bit	positions	(marked	by	*),	so	the	Hamming	distance	of	these	two	code	words
is	3.	(Please	note	that	we	have	not	yet	discussed	how	to	create	code	words;	we	will	do	that	shortly.)
The	Hamming	distance	between	two	code	words	is	important	in	the	context	of	error	detection.	If	two
code	words	are	a	Hamming	distance	d	apart,	d	single-bit	errors	are	required	to	convert	one	code	word	to
the	other,	which	implies	that	this	type	of	error	would	not	be	detected.	Therefore,	if	we	wish	to	create	a
code	that	guarantees	detection	of	all	single-bit	errors	(an	error	in	only	1	bit),	all	pairs	of	code	words	must
have	a	Hamming	distance	of	at	least	2.	If	an	n-bit	word	is	not	recognized	as	a	legal	code	word,	it	is
considered	an	error.
Given	an	algorithm	for	computing	check	bits,	it	is	possible	to	construct	a	complete	list	of	legal	code
words.	The	smallest	Hamming	distance	found	among	all	pairs	of	the	code	words	in	this	code	is	called	the
minimum	Hamming	distance	for	the	code.	The	minimum	Hamming	distance	of	a	code,	often	signified	by
the	notation	D(min),	determines	its	error	detecting	and	correcting	capability.	Stated	succinctly,	for	any
code	word	X	to	be	received	as	another	valid	code	word	Y,	at	least	D(min)	errors	must	occur	in	X.	So,	to
detect	k	(or	fewer)	single-bit	errors,	the	code	must	have	a	Hamming	distance	of	D(min)	=	k	+	1.	Hamming
codes	 can	 always	 detect	 D(min)	 –	 1	 errors	 and	 correct	 (D(min)	–	1)/2	errors.
1
	 Accordingly,	 the
Hamming	distance	of	a	code	must	be	at	least	2k	+	1	in	order	for	it	to	be	able	to	correct	k	errors.
Code	 words	 are	 constructed	 from	 information	 words	 using	r	 parity	 bits.	 Before	 we	 continue	 the
discussion	of	error	detection	and	correction,	let’s	consider	a	simple	example.	The	most	common	error
detection	 uses	 a	 single	 parity	 bit	 appended	 to	 the	 data	 (recall	 the	 discussion	 on	 ASCII	 character
representation).	A	single-bit	error	in	any	bit	of	the	code	word	produces	the	wrong	parity.
	EXAMPLE	2.40	Assume	a	memory	with	2	data	bits	and	1	parity	bit	(appended	at	the	end	of	the	code
word)	that	uses	even	parity	(so	the	number	of	1s	in	the	code	word	must	be	even).	With	2	data	bits,	we
have	a	total	of	4	possible	words.	We	list	here	the	data	word,	its	corresponding	parity	bit,	and	the	resulting
code	word	for	each	of	these	4	possible	words:

Data	WordParity	BitCode	Word
000000
011011
101101
110110
The	resulting	code	words	have	3	bits.	However,	using	3	bits	allows	for	8	different	bit	patterns,	as	follows
(valid	code	words	are	marked	with	an	*):

000*100
001101*
010110*
011*111
If	the	code	word	001	is	encountered,	it	is	invalid	and	thus	indicates	that	an	error	has	occurred	somewhere
in	the	code	word.	For	example,	suppose	the	correct	code	word	to	be	stored	in	memory	is	011,	but	an	error
produces	001.	This	error	can	be	detected,	but	it	cannot	be	corrected.	It	is	impossible	to	determine	exactly
how	many	bits	have	been	flipped	and	exactly	which	ones	are	in	error.	Error-correcting	codes	require
more	than	a	single	parity	bit,	as	we	see	in	the	following	discussion.
What	happens	in	the	above	example	if	a	valid	code	word	is	subject	to	two-bit	errors?	For	example,
suppose	the	code	word	011	is	converted	into	000.	This	error	is	not	detected.	If	you	examine	the	code	in
the	above	example,	you	will	see	that	D(min)	is	2,	which	implies	that	this	code	is	guaranteed	to	detect	only
single-bit	errors.
We	have	already	stated	that	the	error	detecting	and	correcting	capabilities	of	a	code	are	dependent	on
D(min),	and	from	an	error	detection	point	of	view,	we	have	seen	this	relationship	exhibited	in	Example
2.40.	 Error	 correction	 requires	 the	 code	 to	 contain	 additional	 redundant	 bits	 to	 ensure	 a	 minimum
Hamming	distance	D(min)	=	2k	+	1	if	the	code	is	to	detect	and	correct	k	errors.	This	Hamming	distance
guarantees	that	all	legal	code	words	are	far	enough	apart	that	even	with	k	changes,	the	original	invalid
code	word	is	closer	to	one	unique	valid	code	word.	This	is	important	because	the	method	used	in	error
correction	is	to	change	the	invalid	code	word	into	the	valid	code	word	that	differs	in	the	fewest	number	of
bits.	This	idea	is	illustrated	in	Example	2.41.
	EXAMPLE	2.41	Suppose	we	have	the	following	code	(do	not	worry	at	this	time	about	how	this	code
was	generated;	we	will	address	this	issue	shortly):

First,	 let’s	 determine	D(min).	 By	 examining	 all	 possible	 pairs	 of	 code	 words,	 we	 discover	 that	 the
minimum	Hamming	distance	D(min)	=	3.	Thus,	this	code	can	detect	up	to	two	errors	and	correct	one
single-bit	error.	How	is	correction	handled?	Suppose	we	read	the	invalid	code	word	10000.	There	must
be	at	least	one	error	because	this	does	not	match	any	of	the	valid	code	words.	We	now	determine	the
Hamming	distance	between	the	observed	code	word	and	each	legal	code	word:	It	differs	in	1	bit	from	the
first	code	word,	4	from	the	second,	2	from	the	third,	and	3	from	the	last,	resulting	in	a	difference	vector
of	[1,4,2,3].	To	make	the	correction	using	this	code,	we	automatically	correct	to	the	legal	code	word
closest	 to	 the	 observed	 word,	 resulting	 in	 a	 correction	 to	 00000.	 Note	 that	 this	 “correction”	 is	 not
necessarily	correct!	We	are	assuming	that	the	minimum	number	of	possible	errors	has	occurred,	namely,	1.
It	is	possible	that	the	original	code	word	was	supposed	to	be	10110	and	was	changed	to	10000	when	two
errors	occurred.
Suppose	two	errors	really	did	occur.	For	example,	assume	we	read	the	invalid	code	word	11000.	If
we	calculate	the	distance	vector	of	[2,3,3,2],	we	see	there	is	no	“closest”	code	word,	and	we	are	unable
to	make	the	correction.	The	minimum	Hamming	distance	of	3	permits	correction	of	one	error	only,	and
cannot	ensure	correction,	as	evidenced	in	this	example,	if	more	than	one	error	occurs.
In	our	discussion	up	to	this	point,	we	have	simply	presented	you	with	various	codes,	but	have	not
given	any	specifics	as	to	how	the	codes	are	generated.	There	are	many	methods	that	are	used	for	code
generation;	perhaps	one	of	the	more	intuitive	is	the	Hamming	algorithm	for	code	design,	which	we	now
present.	Before	explaining	the	actual	steps	in	the	algorithm,	we	provide	some	background	material.
Suppose	we	wish	to	design	a	code	with	words	consisting	of	m	data	bits	and	r	check	bits,	which
allows	for	single-bit	errors	to	be	corrected.	This	implies	that	there	are	2
m
	legal	code	words,	each	with	a
unique	combination	of	check	bits.	Because	we	are	focused	on	single-bit	errors,	let’s	examine	the	set	of
invalid	code	words	that	are	a	distance	of	1	from	all	legal	code	words.
Each	valid	code	word	has	n	bits,	and	an	error	could	occur	in	any	of	these	n	positions.	Thus,	each
valid	code	word	has	n	illegal	code	words	at	a	distance	of	1.	Therefore,	if	we	are	concerned	with	each
legal	 code	 word	 and	 each	 invalid	 code	 word	 consisting	 of	 one	 error,	 we	 have	n	 +	 1	 bit	 patterns
associated	with	each	code	word	(1	legal	word	and	n	illegal	words).	Because	each	code	word	consists	of
n	bits,	where	n	=	m	+	r,	there	are	2
n
	total	bit	patterns	possible.	This	results	in	the	following	inequality:
(n	+	1)	×	2
m
	≤	2
n
where	n	+	1	is	the	number	of	bit	patterns	per	code	word,	2
m
	is	the	number	of	legal	code	words,	and	2
n
	is
the	total	number	of	bit	patterns	possible.	Because	n	=	m	+	r,	we	can	rewrite	the	inequality	as:
(m	+	r	+	1)	×	2
m
	≤	2
m+r
or
(m	+	r	+	1)	≤	2
r
This	inequality	is	important	because	it	specifies	the	lower	limit	on	the	number	of	check	bits	required	(we
always	use	as	few	check	bits	as	possible)	to	construct	a	code	with	m	data	bits	and	r	check	bits	that
corrects	all	single-bit	errors.
Suppose	we	have	data	words	of	length	m	=	4.	Then:
(4	+	r	+	1)	≤	2
r
which	implies	that	r	must	be	greater	than	or	equal	to	3.	We	choose	r	=	3.	This	means	to	build	a	code	with

data	words	of	4	bits	that	should	correct	single-bit	errors,	we	must	add	3	check	bits.
The	Hamming	Algorithm
The	 Hamming	 algorithm	 provides	 a	 straightforward	 method	 for	 designing	 codes	 to	 correct	 single-bit
errors.	To	construct	error-correcting	codes	for	any	size	memory	word,	we	follow	these	steps:
1.		Determine	the	number	of	check	bits,	r,	necessary	for	the	code	and	then	number	the	n	bits	(where	n	=	m
+	r),	right	to	left,	starting	with	1	(not	0).
2.		Each	bit	whose	bit	number	is	a	power	of	2	is	a	parity	bit—the	others	are	data	bits.
3.		Assign	parity	bits	to	check	bit	positions	as	follows:	Bit	b	is	checked	by	those	parity	bits	b
1
,	b
2
,	...,	b
j
such	that	b
1
	+	b
2
	+	...	+	b
j
	=	b	(where	“+”	indicates	the	modulo	2	sum).
We	now	present	an	example	to	illustrate	these	steps	and	the	actual	process	of	error	correction.
	EXAMPLE	2.42	Using	the	Hamming	code	just	described	and	even	parity,	encode	the	8-bit	ASCII
character	K.	(The	high-order	bit	will	be	zero.)	Induce	a	single-bit	error	and	then	indicate	how	to	locate
the	error.
We	first	determine	the	code	word	for	K.
Step	1:	Determine	the	number	of	necessary	check	bits,	add	these	bits	to	the	data	bits,	and	number	all	n
bits.
Because	m	=	8,	we	have:	(8	+	r	+	1)	≤	2
r
,	which	implies	that	r	must	be	greater	than	or	equal	to	4.	We
choose	r	=	4.
Step	2:	Number	the	n	bits	right	to	left,	starting	with	1,	which	results	in:
The	parity	bits	are	marked	by	boxes.
Step	3:	Assign	parity	bits	to	check	the	various	bit	positions.
To	perform	this	step,	we	first	write	all	bit	positions	as	sums	of	those	numbers	that	are	powers	of	2:
The	number	1	contributes	to	1,	3,	5,	7,	9,	and	11,	so	this	parity	bit	will	reflect	the	parity	of	the	bits	in	these
positions.	Similarly,	2	contributes	to	2,	3,	6,	7,	10,	and	11,	so	the	parity	bit	in	position	2	reflects	the	parity
of	this	set	of	bits.	Bit	4	provides	parity	for	4,	5,	6,	7,	and	12,	and	bit	8	provides	parity	for	bits	8,	9,	10,
11,	and	12.	If	we	write	the	data	bits	in	the	nonboxed	blanks,	and	then	add	the	parity	bits,	we	have	the
following	code	word	as	a	result:
Therefore,	the	code	word	for	K	is	010011010110.

Let’s	introduce	an	error	in	bit	position	b
9
,	resulting	in	the	code	word	010111010110.	If	we	use	the
parity	bits	to	check	the	various	sets	of	bits,	we	find	the	following:
Bit	1	checks	1,	3,	5,	7,	9,	and	11:	With	even	parity,	this	produces	an	error.
Bit	2	checks	2,	3,	6,	7,	10,	and	11:	This	is	ok.
Bit	4	checks	4,	5,	6,	7,	and	12:	This	is	ok.
Bit	8	checks	8,	9,	10,	11,	and	12:	This	produces	an	error.
Parity	bits	1	and	8	show	errors.	These	two	parity	bits	both	check	9	and	11,	so	the	single-bit	error	must	be
in	either	bit	9	or	bit	11.	However,	because	bit	2	checks	bit	11	and	indicates	no	error	has	occurred	in	the
subset	 of	 bits	 it	 checks,	 the	 error	 must	 occur	 in	 bit	 9.	 (We	 know	 this	 because	 we	 created	 the	 error;
however,	note	that	even	if	we	have	no	clue	where	the	error	is,	using	this	method	allows	us	to	determine
the	position	of	the	error	and	correct	it	by	simply	flipping	the	bit.)
Because	of	the	way	the	parity	bits	are	positioned,	an	easier	method	to	detect	and	correct	the	error	bit
is	to	add	the	positions	of	the	parity	bits	that	indicate	an	error.	We	found	that	parity	bits	1	and	8	produced
an	error,	and	1	+	8	=	9,	which	is	exactly	where	the	error	occurred.
	EXAMPLE	2.43	 Use	 the	 Hamming	 algorithm	 to	 find	 all	 code	 words	 for	 a	 3-bit	 memory	 word,
assuming	odd	parity.
We	have	8	possible	words:	000,	001,	010,	011,	100,	101,	110,	and	111.	We	first	need	to	determine	the
required	number	of	check	bits.	Because	m	=	3,	we	have:	(3	+	r	+	1)	≤	2r,	which	implies	that	r	must	be
greater	than	or	equal	to	3.	We	choose	r	=	3.	Therefore,	each	code	word	has	6	bits,	and	the	check	bits	are
in	positions	1,	2,	and	4,	as	shown	here:
From	our	previous	example,	we	know	that:
•			bit	1	checks	the	parity	over	bits	1,	3,	and	5
•			bit	2	check	the	parity	over	bits	2,	3,	and	6
•			bit	4	checks	the	parity	over	bits	4,	5,	and	6
Therefore,	we	have	the	following	code	words	for	each	memory	word:

Our	set	of	code	words	is	001011,	001100,	010010,	010101,	100001,	100110,	111000,	111111.	If	a	single
bit	in	any	of	these	words	is	flipped,	we	can	determine	exactly	which	one	it	is	and	correct	it.	For	example,
to	send	111,	we	actually	send	the	code	word	111111	instead.	If	110111	is	received,	parity	bit	1	(which
checks	bits	1,	3,	and	5)	is	ok,	and	parity	bit	2	(which	checks	bits	2,	3,	and	6)	is	ok,	but	parity	bit	4	shows
an	error,	as	only	bits	5	and	6	are	ones,	violating	odd	parity.	Bit	5	cannot	be	incorrect,	because	parity	bit	1
checked	out	ok.	Bit	6	cannot	be	wrong	because	parity	bit	2	checked	out	ok.	Therefore,	it	must	be	bit	4	that
is	wrong,	so	it	is	changed	from	a	0	to	a	1,	resulting	in	the	correct	code	word	111111.
In	the	next	chapter,	you	will	see	how	easy	it	is	to	implement	a	Hamming	code	using	simple	binary
circuits.	 Because	 of	 its	 simplicity,	 Hamming	 code	 protection	 can	 be	 added	 inexpensively	 and	 with
minimal	effect	on	performance.
2.7.3		Reed-Solomon
Hamming	codes	work	well	in	situations	where	one	can	reasonably	expect	errors	to	be	rare	events.	Fixed
magnetic	disk	drives	have	error	ratings	on	the	order	of	1	bit	in	100	million.	The	3-bit	Hamming	code	that
we	just	studied	will	easily	correct	this	type	of	error.	However,	Hamming	codes	are	useless	in	situations
where	there	is	a	likelihood	that	multiple	adjacent	bits	will	be	damaged.	These	kinds	of	errors	are	called
burst	errors.	 Because	 of	 their	 exposure	 to	 mishandling	 and	 environmental	 stresses,	 burst	 errors	 are
common	on	removable	media	such	as	magnetic	tapes	and	compact	discs.
If	we	expect	errors	to	occur	in	blocks,	it	stands	to	reason	that	we	should	use	an	error-correcting	code
that	operates	at	a	block	level,	as	opposed	to	a	Hamming	code,	which	operates	at	the	bit	level.	A	Reed-
Solomon	(RS)	code	can	be	thought	of	as	a	CRC	that	operates	over	entire	characters	instead	of	only	a	few
bits.	RS	codes,	like	CRCs,	are	systematic:	The	parity	bytes	are	appended	to	a	block	of	information	bytes.
RS(n,	k)	codes	are	defined	using	the	following	parameters:

•			s	=	The	number	of	bits	in	a	character	(or	“symbol”)
•			k	=	The	number	of	s-bit	characters	comprising	the	data	block
•			n	=	The	number	of	bits	in	the	code	word
RS(n,	k)	can	correct		errors	in	the	k	information	bytes.
The	popular	RS(255,	223)	code,	therefore,	uses	223	8-bit	information	bytes	and	32	syndrome	bytes	to
form	255-byte	code	words.	It	will	correct	as	many	as	16	erroneous	bytes	in	the	information	block.
The	 generator	 polynomial	 for	 an	 RS	 code	 is	 given	 by	 a	 polynomial	 defined	 over	 an	 abstract
mathematical	structure	called	a	Galois	field.	(A	lucid	discussion	of	Galois	mathematics	is	beyond	the
scope	of	this	text.	See	the	references	at	the	end	of	the	chapter.)	The	RS-generating	polynomial	is:
where	t	=	n	–	k	and	x	is	an	entire	byte	(or	symbol)	and	g(x)	operates	over	the	field	GF(2
s
).	(Note:	This
polynomial	expands	over	the	Galois	field,	which	is	considerably	different	from	the	integer	fields	used	in
ordinary	algebra.)
The	n-byte	RS	code	word	is	computed	using	the	equation:
where	i(x)	is	the	information	block.
Despite	the	daunting	algebra	behind	them,	RS	error-correction	algorithms	lend	themselves	well	to
implementation	 in	 computer	 hardware.	 They	 are	 implemented	 in	 high-performance	 disk	 drives	 for
mainframe	computers	as	well	as	compact	discs	used	for	music	and	data	storage.	These	implementations
will	be	described	in	Chapter	7.
CHAPTER	SUMMARY
We	have	presented	the	essentials	of	data	representation	and	numerical	operations	in	digital	computers.
You	should	master	the	techniques	described	for	base	conversion	and	memorize	the	smaller	hexadecimal
and	binary	numbers.	This	knowledge	will	be	beneficial	to	you	as	you	study	the	remainder	of	this	text.
Your	knowledge	of	hexadecimal	coding	will	be	useful	if	you	are	ever	required	to	read	a	core	(memory)
dump	after	a	system	crash	or	if	you	do	any	serious	work	in	the	field	of	data	communications.
You	have	also	seen	that	floating-point	numbers	can	produce	significant	errors	when	small	errors	are
allowed	to	compound	over	iterative	processes.	There	are	various	numerical	techniques	that	can	be	used
to	control	such	errors.	These	techniques	merit	detailed	study	but	are	beyond	the	scope	of	this	text.
You	have	learned	that	most	computers	use	ASCII	or	EBCDIC	to	represent	characters.	It	is	generally	of
little	value	to	memorize	any	of	these	codes	in	their	entirety,	but	if	you	work	with	them	frequently,	you	will
find	yourself	learning	a	number	of	“key	values”	from	which	you	can	compute	most	of	the	others	that	you
need.
Unicode	is	the	default	character	set	used	by	Java	and	recent	versions	of	Windows.	It	is	likely	to
replace	 EBCDIC	 and	 ASCII	 as	 the	 basic	 method	 of	 character	 representation	 in	 computer	 systems;
however,	the	older	codes	will	be	with	us	for	the	foreseeable	future,	owing	both	to	their	economy	and	their
pervasiveness.
Error	detecting	and	correcting	codes	are	used	in	virtually	all	facets	of	computing	technology.	Should
the	need	arise,	your	understanding	of	the	various	error	control	methods	will	help	you	to	make	informed

choices	among	the	various	options	available.	The	method	that	you	choose	will	depend	on	a	number	of
factors	 including	 computational	 overhead	 and	 the	 capacity	 of	 the	 storage	 and	 transmission	 media
available	to	you.
FURTHER	READING
A	brief	account	of	early	mathematics	in	Western	civilization	can	be	found	in	Bunt	et	al.	(1988).
Knuth	(1998)	presents	a	delightful	and	thorough	discussion	of	the	evolution	of	number	systems	and
computer	arithmetic	in	Volume	2	of	his	series	on	computer	algorithms.	(Every	computer	scientist	should
own	a	set	of	the	Knuth	books.)
A	definitive	account	of	floating-point	arithmetic	can	be	found	in	Goldberg	(1991).	Schwartz	et	al.
(1999)	describe	how	the	IBM	System/390	performs	floating-point	operations	in	both	the	older	form	and
the	IEEE	standard.	Soderquist	and	Leeser	(1996)	provide	an	excellent	and	detailed	discussion	of	the
problems	surrounding	floating-point	division	and	square	roots.
Detailed	 information	 about	 Unicode	 can	 be	 found	 at	 the	 Unicode	 Consortium	 website,
www.unicode.org,	as	well	as	in	the	Unicode	Standard,	Version	4.0	(2003).
The	International	Standards	Organization	website	can	be	found	at	www.iso.ch.	You	will	be	amazed	at
the	span	of	influence	of	this	group.	A	similar	trove	of	information	can	be	found	at	the	American	National
Standards	Institute	website:	www.ansi.org.
After	you	master	the	concepts	of	Boolean	algebra	and	digital	logic,	you	will	enjoy	reading	Arazi’s
book	(1988).	This	well-written	book	shows	how	error	detection	and	correction	are	achieved	using	simple
digital	circuits.	Arazi’s	appendix	gives	a	remarkably	lucid	discussion	of	the	Galois	field	arithmetic	that	is
used	in	Reed-Solomon	codes.
If	you’d	prefer	a	rigorous	and	exhaustive	study	of	error-correction	theory,	Pretzel’s	(1992)	book	is	an
excellent	place	to	start.	The	text	is	accessible,	well-written,	and	thorough.
Detailed	discussions	of	Galois	fields	can	be	found	in	the	(inexpensive!)	books	by	Artin	(1998)	and
Warner	(1990).	Warner’s	much	larger	book	is	a	clearly	written	and	comprehensive	introduction	to	the
concepts	of	abstract	algebra.	A	study	of	abstract	algebra	will	be	helpful	to	you	should	you	delve	into	the
study	of	mathematical	cryptography,	a	fast-growing	area	of	interest	in	computer	science.
REFERENCES
Arazi,	B.	A	Commonsense	Approach	to	the	Theory	of	Error	Correcting	Codes.	Cambridge,	MA:	The
MIT	Press,	1988.
Artin,	E.	Galois	Theory.	New	York:	Dover	Publications,	1998.
Bunt,	L.	N.	H.,	Jones,	P.	S.,	&	Bedient,	J.	D.	The	Historical	Roots	of	Elementary	Mathematics.
New	York:	Dover	Publications,	1988.
Goldberg,	D.	“What	Every	Computer	Scientist	Should	Know	about	Floating-Point	Arithmetic.”	ACM
Computing	Surveys	23:1,	March	1991,	pp.	5–47.
Knuth,	D.	E.	The	Art	of	Computer	Programming,	3rd	ed.	Reading,	MA:	Addison-Wesley,	1998.
Pretzel,	O.	Error-Correcting	Codes	and	Finite	Fields.	New	York:	Oxford	University	Press,	1992.
Schwartz,	E.	M.,	Smith,	R.	M.,	&	Krygowski,	C.	A.	“The	S/390	G5	Floating-Point	Unit	Supporting	Hex
and	Binary	Architectures.”	IEEE	Proceedings	from	the	14th	Symposium	on	Computer	Arithmetic,
1999,	pp.	258–265.

Soderquist,	P.,	&	Leeser,	M.	“Area	and	Performance	Tradeoffs	in	Floating-Point	Divide	and	Square-Root
Implementations.”	ACM	Computing	Surveys	28:3,	September	1996,	pp.	518–564.
The	Unicode	Consortium.	The	Unicode	Standard,	Version	4.0.	Reading,	MA:	Addison-Wesley,	2003.
Warner,	S.	Modern	Algebra.	New	York:	Dover	Publications,	1990.
REVIEW	OF	ESSENTIAL	TERMS	AND	CONCEPTS
1.		The	word	bit	is	a	contraction	for	what	two	words?
2.		Explain	how	the	terms	bit,	byte,	nibble,	and	word	are	related.
3.		Why	are	binary	and	decimal	called	positional	numbering	systems?
4.		Explain	how	base	2,	base	8,	and	base	16	are	related.
5.		What	is	a	radix?
6.		How	many	of	the	“numbers	to	remember”	(in	all	bases)	from	Table	2.1	can	you	remember?
7.		What	does	overflow	mean	in	the	context	of	unsigned	numbers?
8.		Name	the	four	ways	in	which	signed	integers	can	be	represented	in	digital	computers,	and	explain	the
differences.
9.		Which	one	of	the	four	representations	for	signed	integers	is	used	most	often	by	digital	computer
systems?
10.		How	are	complement	systems	similar	to	the	odometer	on	a	bicycle?
11.		Do	you	think	that	double-dabble	is	an	easier	method	than	the	other	binary-to-decimal	conversion
methods	explained	in	this	chapter?	Why?
12.	 	 With	 reference	 to	 the	 previous	 question,	 what	 are	 the	 drawbacks	 of	 the	 other	 two	 conversion
methods?
13.		What	is	overflow,	and	how	can	it	be	detected?	How	does	overflow	in	unsigned	numbers	differ	from
overflow	in	signed	numbers?
14.	 	 If	 a	 computer	 is	 capable	 only	 of	 manipulating	 and	 storing	 integers,	 what	 difficulties	 present
themselves?	How	are	these	difficulties	overcome?
15.		What	are	the	goals	of	Booth’s	algorithm?
16.		How	does	carry	differ	from	overflow?
17.		What	is	arithmetic	shifting?
18.		What	are	the	three	component	parts	of	a	floating-point	number?
19.		What	is	a	biased	exponent,	and	what	efficiencies	can	it	provide?
20.		What	is	normalization,	and	why	is	it	necessary?
21.		Why	is	there	always	some	degree	of	error	in	floating-point	arithmetic	when	performed	by	a	binary
digital	computer?
22.		How	many	bits	long	is	a	double-precision	number	under	the	IEEE-754	floating-point	standard?
23.		What	is	EBCDIC,	and	how	is	it	related	to	BCD?

24.		What	is	ASCII,	and	how	did	it	originate?
25.		Explain	the	difference	between	ASCII	and	Unicode.
26.		How	many	bits	does	a	Unicode	character	require?
27.		Why	was	Unicode	created?
28.		How	do	cyclic	redundancy	checks	work?
29.		What	is	systematic	error	detection?
30.		What	is	a	Hamming	code?
31.		What	is	meant	by	Hamming	distance,	and	why	is	it	important?	What	is	meant	by	minimum	Hamming
distance?
32.		How	is	the	number	of	redundant	bits	necessary	for	code	related	to	the	number	of	data	bits?
33.		What	is	a	burst	error?
34.		Name	an	error-detection	method	that	can	compensate	for	burst	errors.
EXERCISES
	1.		Perform	the	following	base	conversions	using	subtraction	or	division-remainder:
	a)	458
10
	=	________	
3
	b)	677
10
	=	________	
5
	c)	1518
10
	=	_______	
7
	d)	4401
10
	=	_______	
9
2.		Perform	the	following	base	conversions	using	subtraction	or	division-remainder:
a)		588
10
	=	_________	
3
b)		2254
10
	=	________	
5
c)		652
10
	=	________	
7
d)		3104
10
	=	________	
9
3.		Perform	the	following	base	conversions	using	subtraction	or	division-remainder:
a)		137
10
	=	_________	
3
b)		248
10
	=	________	
5
c)		387
10
	=	________	
7
d)		633
10
	=	________	
9
4.		Perform	the	following	base	conversions:
a)		20101
3
	=	_________	
10
b)		2302
5
	=	________	
10
c)		1605
7
	=________	
10
d)		687
9
	=	________	
10
5.		Perform	the	following	base	conversions:

a)		20012
3
	=	_________	
10
b)		4103
5
	=	________	
10
c)		3236
7
	=	________	
10
d)		1378
9
	=	________	
10
6.		Perform	the	following	base	conversions:
a)		21200
3
	=	_________	
10
b)		3244
5
	=	________	
10
c)		3402
7
	=	________	
10
d)		7657
9
	=	________	
10
	7.		Convert	the	following	decimal	fractions	to	binary	with	a	maximum	of	six	places	to	the	right	of	the
binary	point:
	a)	26.78125
	b)	194.03125
	c)	298.796875
	d)	16.1240234375
8.		Convert	the	following	decimal	fractions	to	binary	with	a	maximum	of	six	places	to	the	right	of	the
binary	point:
a)	25.84375
b)	57.55
c)	80.90625
d)	84.874023
9.		Convert	the	following	decimal	fractions	to	binary	with	a	maximum	of	six	places	to	the	right	of	the
binary	point:
a)	27.59375
b)	105.59375
c)	241.53125
d)	327.78125
10.		Convert	the	following	binary	fractions	to	decimal:
a)	10111.1101
b)	100011.10011
c)	1010011.10001
d)	11000010.111
11.		Convert	the	following	binary	fractions	to	decimal:
a)	100001.111
b)	111111.10011
c)	1001100.1011
d)	10001001.0111

12.		Convert	the	following	binary	fractions	to	decimal:
a)		110001.10101
b)		111001.001011
c)		1001001.10101
d)		11101001.110001
13.		Convert	the	hexadecimal	number	AC12
16
	to	binary.
14.		Convert	the	hexadecimal	number	7A01
16
	to	binary.
15.		Convert	the	hexadecimal	number	DEAD	BEEF
16
	to	binary.
16.	 	 Represent	 the	 following	 decimal	 numbers	 in	 binary	 using	 8-bit	 signed	 magnitude,	 one’s
complement,	two’s	complement,	and	excess-127	representations.
	a)	77
	b)	–42
c)		119
d)		–107
17.	 	 Represent	 the	 following	 decimal	 numbers	 in	 binary	 using	 8-bit	 signed	 magnitude,	 one’s
complement,	two’s	complement,	and	excess-127	representations:
a)		60
b)	–60
c)	20
d)	–20
18.	 	 Represent	 the	 following	 decimal	 numbers	 in	 binary	 using	 8-bit	 signed	 magnitude,	 one’s
complement,	two’s	complement,	and	excess-127	representations:
a)	97
b)	–97
c)	44
d)	–44
19.	 	 Represent	 the	 following	 decimal	 numbers	 in	 binary	 using	 8-bit	 signed	 magnitude,	 one’s
complement,	two’s	complement,	and	excess-127	representations:
a)	89
b)	–89
c)	66
d)	–66
20.		What	decimal	value	does	the	8-bit	binary	number	10011110	have	if:
a)		it	is	interpreted	as	an	unsigned	number?
b)		it	is	on	a	computer	using	signed-magnitude	representation?
c)		it	is	on	a	computer	using	one’s	complement	representation?
d)		it	is	on	a	computer	using	two’s	complement	representation?
e)		it	is	on	a	computer	using	excess-127	representation?

21.		What	decimal	value	does	the	8-bit	binary	number	00010001	have	if:
a)		it	is	interpreted	as	an	unsigned	number?
b)		it	is	on	a	computer	using	signed-magnitude	representation?
c)		it	is	on	a	computer	using	one’s	complement	representation?
d)		it	is	on	a	computer	using	two’s	complement	representation?
e)		it	is	on	a	computer	using	excess-127	representation?
22.		What	decimal	value	does	the	8-bit	binary	number	10110100	have	if:
a)		it	is	interpreted	as	an	unsigned	number?
b)		it	is	on	a	computer	using	signed-magnitude	representation?
c)		it	is	on	a	computer	using	one’s	complement	representation?
d)		it	is	on	a	computer	using	two’s	complement	representation?
e)		it	is	on	a	computer	using	excess-127	representation?
23.		Given	the	following	two	binary	numbers:	11111100	and	01110000.
a)		Which	of	these	two	numbers	is	the	larger	unsigned	binary	number?
b)		Which	of	these	two	is	the	larger	when	it	is	being	interpreted	on	a	computer	using	signed	two’s
complement	representation?
c)		Which	of	these	two	is	the	smaller	when	it	is	being	interpreted	on	a	computer	using	signed-
magnitude	representation?
24.		Using	a	“word”	of	3	bits,	list	all	the	possible	signed	binary	numbers	and	their	decimal	equivalents
that	are	representable	in:
a)		Signed	magnitude
b)		One’s	complement
c)		Two’s	complement
25.		Using	a	“word”	of	4	bits,	list	all	the	possible	signed	binary	numbers	and	their	decimal	equivalents
that	are	representable	in:
a)		Signed	magnitude
b)		One’s	complement
c)		Two’s	complement
26.		From	the	results	of	the	previous	two	questions,	generalize	the	range	of	values	(in	decimal)	that	can
be	represented	in	any	given	x	number	of	bits	using:
a)		Signed	magnitude
b)		One’s	complement
c)		Two’s	complement
27.	 	 Fill	 in	 the	 following	 table	 to	 indicate	 what	 each	 binary	 pattern	 represents	 using	 the	 various
formats.

28.		Given	a	(very)	tiny	computer	that	has	a	word	size	of	6	bits,	what	are	the	smallest	negative	numbers
and	 the	 largest	 positive	 numbers	 that	 this	 computer	 can	 represent	 in	 each	 of	 the	 following
representations?
	a)	One’s	complement
b)		Two’s	complement
29.		To	add	2	two’s	complement	numbers	together,	what	must	be	true?
30.		What	is	the	most	common	representation	used	in	most	computers	to	store	signed	integer	values	and
why?
31.		You	have	stumbled	on	an	unknown	civilization	while	sailing	around	the	world.	The	people,	who
call	themselves	Zebronians,	do	math	using	40	separate	characters	(probably	because	there	are	40
stripes	on	a	zebra).	They	would	very	much	like	to	use	computers,	but	would	need	a	computer	to	do
Zebronian	math,	which	would	mean	a	computer	that	could	represent	all	40	characters.	You	are	a
computer	designer	and	decide	to	help	them.	You	decide	the	best	thing	is	to	use	BCZ,	Binary-Coded
Zebronian	(which	is	like	BCD	except	it	codes	Zebronian,	not	Decimal).	How	many	bits	will	you
need	to	represent	each	character	if	you	want	to	use	the	minimum	number	of	bits?
	32.		Add	the	following	unsigned	binary	numbers	as	shown.
33.		Add	the	following	unsigned	binary	numbers	as	shown.

	34.		Subtract	the	following	signed	binary	numbers	as	shown	using	two’s	complement	arithmetic.
35.		Subtract	the	following	signed	binary	numbers	as	shown	using	two’s	complement	arithmetic.
36.		Perform	the	following	binary	multiplications,	assuming	unsigned	integers:
37.		Perform	the	following	binary	multiplications,	assuming	unsigned	integers:
38.		Perform	the	following	binary	divisions,	assuming	unsigned	integers:
	a)	101101	÷	101
b)		10000001	÷	101
c)		1001010010	÷	1011
39.		Perform	the	following	binary	divisions,	assuming	unsigned	integers:
a)		11111101	÷	1011
b)		110010101	÷	1001
c)		1001111100	÷	1100
	40.		Use	the	double-dabble	method	to	convert	10212
3
	directly	to	decimal.	(Hint:	You	have	to	change	the
multiplier.)
41.		Using	signed-magnitude	representation,	complete	the	following	operations:

	42.		Suppose	a	computer	uses	4-bit	one’s	complement	representation.	Ignoring	overflows,	what	value
will	be	stored	in	the	variable	j	after	the	following	pseudocode	routine	terminates?
43.	 	 Perform	 the	 following	 binary	 multiplications	 using	 Booth’s	 algorithm,	 assuming	 signed	 two’s
complement	integers:
44.		Using	arithmetic	shifting,	perform	the	following:
a)		double	the	value	00010101
2
b)	quadruple	the	value	01110111
2
c)	divide	the	value	11001010
2
	in	half
45.		If	the	floating-point	number	representation	on	a	certain	system	has	a	sign	bit,	a	3-bit	exponent,	and
a	4-bit	significand:
a)		What	is	the	largest	positive	and	the	smallest	positive	number	that	can	be	stored	on	this	system	if
the	storage	is	normalized?	(Assume	that	no	bits	are	implied,	there	is	no	biasing,	exponents	use
two’s	complement	notation,	and	exponents	of	all	zeros	and	all	ones	are	allowed.)
b)		What	bias	should	be	used	in	the	exponent	if	we	prefer	all	exponents	to	be	non-negative?	Why
would	you	choose	this	bias?
	46.		Using	the	model	in	the	previous	question,	including	your	chosen	bias,	add	the	following	floating-
point	numbers	and	express	your	answer	using	the	same	notation	as	the	addend	and	augend:
Calculate	the	relative	error,	if	any,	in	your	answer	to	the	previous	question.
47.		Assume	we	are	using	the	simple	model	for	floating-point	representation	as	given	in	the	text	(the
representation	 uses	 a	 14-bit	 format,	 5	 bits	 for	 the	 exponent	 with	 a	 bias	 of	 15,	 a	 normalized
mantissa	of	8	bits,	and	a	single	sign	bit	for	the	number):
a)		Show	how	the	computer	would	represent	the	numbers	100.0	and	0.25	using	this	floating-point
format.
b)		Show	how	the	computer	would	add	the	two	floating-point	numbers	in	part	a	by	changing	one	of
the	numbers	so	they	are	both	expressed	using	the	same	power	of	2.
c)	 	 Show	 how	 the	 computer	 would	 represent	 the	 sum	 in	 part	 b	 using	 the	 given	 floating-point

representation.	What	decimal	value	for	the	sum	is	the	computer	actually	storing?	Explain.
48.		What	causes	divide	underflow,	and	what	can	be	done	about	it?
49.		Why	do	we	usually	store	floating-point	numbers	in	normalized	form?	What	is	the	advantage	of
using	a	bias	as	opposed	to	adding	a	sign	bit	to	the	exponent?
50.		Let	a	=	1.0	×	2
9
,	b	=	–1.0	×	2
9
	and	c	=	1.0	×	2
1
.	Using	the	simple	floating-point	model	described	in
the	 text	 (the	 representation	 uses	 a	 14-bit	 format,	 5	 bits	 for	 the	 exponent	 with	 a	 bias	 of	 15,	 a
normalized	 mantissa	 of	 8	 bits,	 and	 a	 single	 sign	 bit	 for	the	 number),	 perform	 the	 following
calculations,	 paying	 close	 attention	 to	 the	 order	 of	 operations.	 What	 can	 you	 say	 about	 the
algebraic	properties	of	floating-point	arithmetic	in	our	finite	model?	Do	you	think	this	algebraic
anomaly	holds	under	multiplication	as	well	as	addition?
b	+	(a	+	c)	=
(b	+	a)	+	c	=
51.		Show	how	each	of	the	following	floating-point	values	would	be	stored	using	IEEE-754	single
precision	(be	sure	to	indicate	the	sign	bit,	the	exponent,	and	the	significand	fields):
a)	12.5					b)	–1.5					c)	0.75					d)	26.625
52.		Show	how	each	of	the	following	floating-point	values	would	be	stored	using	IEEE-754	double
precision	(be	sure	to	indicate	the	sign	bit,	the	exponent,	and	the	significand	fields):
a)	12.5					b)	–1.5					c)	0.75					d)	26.625
53.	 	 Suppose	 we	 have	 just	 found	 yet	 another	 representation	 for	 floating-point	 numbers.	 Using	 this
representation,	a	12-bit	floating-point	number	has	1	bit	for	the	sign	of	the	number,	4	bits	for	the
exponent,	and	7	bits	for	the	mantissa,	which	is	normalized	as	in	the	Simple	Model	so	that	the	first
digit	to	the	right	of	the	radix	points	must	be	a	1.	Numbers	in	the	exponent	are	in	signed	two’s
complement	representation.	No	bias	is	used,	and	there	are	no	implied	bits.	Show	the	representation
for	the	smallest	positive	number	this	machine	can	represent	using	the	following	format	(simply	fill
in	the	squares	provided).	What	decimal	number	does	this	equate	to?
54.		Find	three	floating-point	values	to	illustrate	that	floating-point	addition	is	not	associative.	(You
will	need	to	run	a	program	on	specific	hardware	with	a	specific	compiler.)
55.		a)	Given	that	the	ASCII	code	for	A	is	1000001,	what	is	the	ASCII	code	for	J?
b)		Given	that	the	EBCDIC	code	for	A	is	1100	0001,	what	is	the	EBCDIC	code	for	J?
56.		a)	The	ASCII	code	for	the	letter	A	is	1000001,	and	the	ASCII	code	for	the	letter	a	is	1100001.
Given	that	the	ASCII	code	for	the	letter	G	is	1000111,	without	looking	at	Table	2.7,	what	is	the
ASCII	code	for	the	letter	g?
b)		The	EBCDIC	code	for	the	letter	A	is	1100	0001,	and	the	EBCDIC	code	for	the	letter	a	is	1000
0001.	Given	that	the	EBCDIC	code	for	the	letter	G	is	1100	0111,	without	looking	at	Table	2.6,
what	is	the	EBCDIC	code	for	the	letter	g?
c)		The	ASCII	code	for	the	letter	A	is	1000001,	and	the	ASCII	code	for	the	letter	a	is	1100001.
Given	that	the	ASCII	code	for	the	letter	Q	is	1010001,	without	looking	at	Table	2.7,	what	is	the

ASCII	code	for	the	letter	q?
d)		The	EBCDIC	code	for	the	letter	J	is	1101	0001,	and	the	EBCDIC	code	for	the	letter	j	is	1001
0001.	Given	that	the	EBCDIC	code	for	the	letter	Q	is	1101	1000,	without	looking	at	Table	2.6,
what	is	the	EBCDIC	code	for	the	letter	q?
e)	In	general,	if	you	were	going	to	write	a	program	to	convert	uppercase	ASCII	characters	to
lowercase,	how	would	you	do	it?	Looking	at	Table	2.6,	could	you	use	the	same	algorithm	to
convert	uppercase	EBCDIC	letters	to	lowercase?
f)		If	you	were	tasked	with	interfacing	an	EBCDIC-based	computer	with	an	ASCII	or	Unicode
computer,	what	would	be	the	best	way	to	convert	the	EBCDIC	characters	to	ASCII	characters?
	57.		Assume	a	24-bit	word	on	a	computer.	In	these	24	bits,	we	wish	to	represent	the	value	295.
	a)	How	would	the	computer	represent	the	decimal	value	295?
	b)	If	our	computer	uses	8-bit	ASCII	and	even	parity,	how	would	the	computer	represent	the	string
295?
	c)	If	our	computer	uses	packed	BCD	with	zero	padding,	how	would	the	computer	represent	the
number	+295?
58.		Decode	the	following	ASCII	message,	assuming	7-bit	ASCII	characters	and	no	parity:	1001010
1001111	1001000	1001110	0100000	1000100	1001111	1000101
59.		Why	would	a	system	designer	wish	to	make	Unicode	the	default	character	set	for	their	new	system?
What	reason(s)	could	you	give	for	not	using	Unicode	as	a	default?	(Hint:	Think	about	language
compatibility	versus	storage	space.)
60.		Assume	we	wish	to	create	a	code	using	3	information	bits,	1	parity	bit	(appended	to	the	end	of	the
information),	and	odd	parity.	List	all	legal	code	words	in	this	code.	What	is	the	Hamming	distance
of	your	code?
61.		Suppose	we	are	given	the	following	subset	of	code	words,	created	for	a	7-bit	memory	word	with
one	parity	bit:	11100110,	00001000,	10101011,	and	11111110.	Does	this	code	use	even	or	odd
parity?	Explain.
62.		Are	the	error-correcting	Hamming	codes	systematic?	Explain.
63.		Compute	the	Hamming	distance	of	the	following	code:
0011010010111100
0000011110001111
0010010110101101
0001011010011110
64.		Compute	the	Hamming	distance	of	the	following	code:
0000000101111111
0000001010111111
0000010011011111
0000100011101111
0001000011110111
0010000011111011
0100000011111101
1000000011111110

65.		In	defining	the	Hamming	distance	for	a	code,	we	choose	to	use	the	minimum	(Hamming)	distance
between	any	two	encodings.	Explain	why	it	would	not	be	better	to	use	the	maximum	or	average
distance.
66.		Suppose	we	want	an	error-correcting	code	that	will	allow	all	single-bit	errors	to	be	corrected	for
memory	words	of	length	10.
a)		How	many	parity	bits	are	necessary?
b)		Assuming	we	are	using	the	Hamming	algorithm	presented	in	this	chapter	to	design	our	error-
correcting	code,	find	the	code	word	to	represent	the	10-bit	information	word:
1	0	0	1	1	0	0	1	1	0.
67.		Suppose	we	want	an	error-correcting	code	that	will	allow	all	single-bit	errors	to	be	corrected	for
memory	words	of	length	12.
a)		How	many	parity	bits	are	necessary?
b)		Assuming	we	are	using	the	Hamming	algorithm	presented	in	this	chapter	to	design	our	error-
correcting	code,	find	the	code	word	to	represent	the	12-bit	information	word:
1	0	0	1	0	0	0	1	1	0	1	0.
	68.		Suppose	we	are	working	with	an	error-correcting	code	that	will	allow	all	single-bit	errors	to	be
corrected	for	memory	words	of	length	7.	We	have	already	calculated	that	we	need	4	check	bits,
and	the	length	of	all	code	words	will	be	11.	Code	words	are	created	according	to	the	Hamming
algorithm	presented	in	the	text.	We	now	receive	the	following	code	word:
1	0	1	0	1	0	1	1	1	1	0
Assuming	even	parity,	is	this	a	legal	code	word?	If	not,	according	to	our	error-correcting	code,
where	is	the	error?
69.		Repeat	exercise	68	using	the	following	code	word:
0	1	1	1	1	0	1	0	1	0	1
70.		Suppose	we	are	working	with	an	error-correcting	code	that	will	allow	all	single-bit	errors	to	be
corrected	for	memory	words	of	length	12.	We	have	already	calculated	that	we	need	5	check	bits,
and	the	length	of	all	code	words	will	be	17.	Code	words	are	created	according	to	the	Hamming
algorithm	presented	in	the	text.	We	now	receive	the	following	code	word:
0	1	1	0	0	1	0	1	0	0	1	0	0	1	0	0	1
Assuming	even	parity,	is	this	a	legal	code	word?	If	not,	according	to	our	error-correcting	code,
where	is	the	error?
71.		Name	two	ways	in	which	Reed-Solomon	coding	differs	from	Hamming	coding.
72.		When	would	you	choose	a	CRC	code	over	a	Hamming	code?	A	Hamming	code	over	a	CRC?
	73.		Find	the	quotients	and	remainders	for	the	following	division	problems	modulo	2.
	a)	1010111
2
	÷	1101
2
	b)	1011111
2
	÷	11101
2
	c)	1011001101
2
	÷	10101
2
	d)	111010111
2
	÷	10111
2
74.		Find	the	quotients	and	remainders	for	the	following	division	problems	modulo	2.

a)		1111010
2
	÷	1011
2
	b)	1010101
2
	÷	1100
2
	c)	1101101011
2
	÷	10101
2
	d)	1111101011
2
	÷	101101
2
75.		Find	the	quotients	and	remainders	for	the	following	division	problems	modulo	2.
a)		11001001
2
	÷	1101
2
b)	1011000
2
	÷	10011
2
c)	11101011
2
	÷	10111
2
d)	111110001
2
	÷	1001
2
76.		Find	the	quotients	and	remainders	for	the	following	division	problems	modulo	2.
a)		1001111
2
	÷	1101
2
b)		1011110
2
	÷	1100
2
c)		1001101110
2
	÷	11001
2
d)		111101010
2
	÷	10011
2
	77.		Using	the	CRC	polynomial	1011,	compute	the	CRC	code	word	for	the	information	word,	1011001.
Check	the	division	performed	at	the	receiver.
78.	 	 Using	 the	 CRC	 polynomial	 1101,	 compute	 the	 CRC	 code	 word	 for	 the	 information	 word,
01001101.	Check	the	division	performed	at	the	receiver.
79.		Using	the	CRC	polynomial	1101,	compute	the	CRC	code	word	for	the	information	word,	1100011.
Check	the	division	performed	at	the	receiver.
80.	 	 Using	 the	 CRC	 polynomial	 1101,	 compute	 the	 CRC	 code	 word	 for	 the	 information	 word,
01011101.	Check	the	division	performed	at	the	receiver.
81.		Pick	an	architecture	(such	as	80486,	Pentium,	Pentium	IV,	SPARC,	Alpha,	or	MIPS).	Do	research
to	find	out	how	your	architecture	approaches	the	concepts	introduced	in	this	chapter.	For	example,
what	representation	does	it	use	for	negative	values?	What	character	codes	does	it	support?
82.		We	have	seen	that	floating-point	arithmetic	is	neither	associative	nor	distributive.	Why	do	you
think	this	is	the	case?
FOCUS	ON	CODES	FOR	DATA	RECORDING	AND
TRANSMISSION
ASCII,	 EBCDIC,	 and	 Unicode	 are	 represented	 unambiguously	 in	 computer	 memories.	 (Chapter	 3
describes	 how	 this	 is	 done	 using	 binary	 digital	 devices.)	 Digital	 switches,	 such	 as	 those	 used	 in
memories,	are	either	“off”	or	“on”	with	nothing	in	between.	However,	when	data	are	written	to	some	sort
of	recording	medium	(such	as	tape	or	disk),	or	transmitted	over	long	distances,	binary	signals	can	become
blurred,	particularly	when	long	strings	of	ones	and	zeros	are	involved.	This	blurring	is	partly	attributable
to	timing	drifts	that	occur	between	senders	and	receivers.	Magnetic	media,	such	as	tapes	and	disks,	can
also	lose	synchronization	owing	to	the	electrical	behavior	of	the	magnetic	material	from	which	they	are
made.	 Signal	 transitions	 between	 the	 “high”	 and	 “low”	 states	 of	 digital	 signals	 help	 to	 maintain
synchronization	 in	 data	 recording	 and	 communications	 devices.	 To	 this	 end,	 ASCII,	 EBCDIC,	 and
Unicode	are	translated	into	other	codes	before	they	are	transmitted	or	recorded.	This	translation	is	carried
out	 by	 control	 electronics	 in	 data	 recording	 and	 transmission	 devices.	 Neither	 the	 user	 nor	 the	 host
computer	is	ever	aware	that	this	translation	has	taken	place.

Bytes	are	sent	and	received	by	telecommunications	devices	by	using	“high”	and	“low”	pulses	in	the
transmission	media	(copper	wire,	for	example).	Magnetic	storage	devices	record	data	using	changes	in
magnetic	polarity	called	flux	reversals.	Certain	coding	methods	are	better	suited	for	data	communications
than	for	data	recording.	New	codes	are	continually	being	invented	to	accommodate	evolving	recording
methods	and	improved	transmission	and	recording	media.	We	will	examine	a	few	of	the	more	popular
recording	and	transmission	codes	to	show	how	some	of	the	challenges	in	this	area	have	been	overcome.
For	the	sake	of	brevity,	we	will	use	the	term	data	encoding	to	mean	the	process	of	converting	a	simple
character	 code	 such	 as	 ASCII	 to	 some	 other	 code	 that	 better	 lends	 itself	 to	 storage	 or	 transmission.
Encoded	data	will	be	used	to	refer	to	character	codes	so	encoded.
2A.1	NON-RETURN-TO-ZERO	CODE
The	simplest	data	encoding	method	is	the	non-return-to-zero	(NRZ)	code.	We	use	this	code	implicitly
when	we	say	that	“highs”	and	“lows”	represent	ones	and	zeros:	ones	are	usually	high	voltage,	and	zeroes
are	low	voltage.	Typically,	high	voltage	is	positive	3	or	5	volts;	low	voltage	is	negative	3	or	5	volts.	(The
reverse	is	logically	equivalent.)
For	example,	the	ASCII	code	for	the	English	word	OK	with	even	parity	is	11001111	01001011.	This
pattern	in	NRZ	code	is	shown	in	its	signal	form	as	well	as	in	its	magnetic	flux	form	in	Figure	2A.1.	Each
of	the	bits	occupies	an	arbitrary	slice	of	time	in	a	transmission	medium	or	an	arbitrary	speck	of	space	on	a
disk.	These	slices	and	specks	are	called	bit	cells.
FIGURE	2A.1	NRZ	Encoding	of	OKas
a)	Transmission	Waveform
b)	Magnetic	Flux	Pattern	(The	direction	of	the	arrows	indicates	the	magnetic	polarity.)
As	you	can	see	by	the	figure,	we	have	a	long	run	of	ones	in	the	ASCII	O.	If	we	transmit	the	longer
form	of	the	word	OK,	OKAY,	we	would	have	a	long	string	of	zeros	as	well	as	a	long	string	of	ones:
11001111	01001011	01000001	01011001.	Unless	the	receiver	is	synchronized	precisely	with	the	sender,
it	is	not	possible	for	either	to	know	the	exact	duration	of	the	signal	for	each	bit	cell.	Slow	or	out-of-phase
timing	within	the	receiver	might	cause	the	bit	sequence	for	OKAY	to	be	received	as:	10011	0100101
010001	0101001,	which	would	be	translated	back	to	ASCII	as	<ETX>(),	bearing	no	resemblance	to	what
was	sent.	(<ETX>	is	used	here	to	mean	the	single	ASCII	End-of-Text	character,	26	in	decimal.)
A	little	experimentation	with	this	example	will	demonstrate	to	you	that	if	only	one	bit	is	missed	in
NRZ	code,	the	entire	message	can	be	reduced	to	gibberish.

2A.2	NON-RETURN-TO-ZERO-INVERT	CODE
The	non-return-to-zero-invert	(NRZI)	method	addresses	part	of	the	problem	of	synchronization	loss.
NRZI	provides	a	transition—either	high-to-low	or	low-to-high—for	each	binary	one,	and	no	transition	for
binary	zero.	The	NRZI	coding	for	OK	(with	even	parity)	is	shown	in	Figure	2A.2.
Although	NRZI	eliminates	the	problem	of	dropping	binary	ones,	we	are	still	faced	with	the	problem	of
long	strings	of	zeros	causing	the	receiver	or	reader	to	drift	out	of	phase,	potentially	dropping	bits	along
the	way.
The	obvious	approach	to	solving	this	problem	is	to	inject	sufficient	transitions	into	the	transmitted
waveform	to	keep	the	sender	and	receiver	synchronized,	while	preserving	the	information	content	of	the
message.	This	is	the	essential	idea	behind	all	coding	methods	used	today	in	the	storage	and	transmission
of	data.
FIGURE	2A.2	NRZI	Encoding	of	OK
2A.3	PHASE	MODULATION	(MANCHESTER	CODE)
The	coding	method	known	commonly	as	phase	modulation	(PM),	or	Manchester	coding,	deals	with	the
synchronization	problem	head-on.	PM	provides	a	transition	for	each	bit,	whether	a	one	or	a	zero.	In	PM,
each	 binary	 one	 is	 signaled	 by	 an	 “up”	 transition,	 and	 binary	 zeros	 with	 a	 “down”	 transition.	 Extra
transitions	are	provided	at	bit	cell	boundaries	when	necessary.	The	PM	coding	of	the	word	OK	is	shown
in	Figure	2A.3.
Phase	modulation	is	often	used	in	data	transmission	applications	such	as	local	area	networks.	It	is
inefficient	for	use	in	data	storage,	however.	If	PM	were	used	for	tape	and	disk,	phase	modulation	would
require	twice	the	bit	density	of	NRZ.	(One	flux	transition	for	each	half	bit	cell,	depicted	in	Figure	2A.3b.)
However,	we	have	just	seen	how	using	NRZ	might	result	in	unacceptably	high	error	rates.	We	could
therefore	 define	 a	 “good”	 encoding	 scheme	 as	 a	 method	 that	 most	 economically	 achieves	 a	 balance
between	“excessive”	storage	volume	requirements	and	“excessive”	error	rates.	A	number	of	codes	have
been	created	in	trying	to	find	this	middle	ground.

FIGURE	2A.3	Phase	Modulation	(Manchester	Coding)	of	the	Word	OK	as:
a)	Transmission	Waveform
b)	Magnetic	Flux	Pattern
2A.4	FREQUENCY	MODULATION
As	used	in	digital	applications,	frequency	modulation	(FM)	is	similar	to	phase	modulation	in	that	at
least	one	transition	is	supplied	for	each	bit	cell.	These	synchronizing	transitions	occur	at	the	beginning	of
each	bit	cell.	To	encode	a	binary	1,	an	additional	transition	is	provided	in	the	center	of	the	bit	cell.	The
FM	coding	for	OK	is	shown	in	Figure	2A.4.
As	you	can	readily	see	from	the	figure,	FM	is	only	slightly	better	than	PM	with	respect	to	its	storage
requirements.	 FM,	 however,	 lends	 itself	 to	 a	 coding	 method	 called	modified	 frequency	 modulation
(MFM),	whereby	bit	cell	boundary	transitions	are	provided	only	between	consecutive	zeros.	With	MFM,
then,	at	least	one	transition	is	supplied	for	every	pair	of	bit	cells,	as	opposed	to	each	cell	in	PM	or	FM.
With	fewer	transitions	than	PM	and	more	transitions	than	NRZ,	MFM	is	a	highly	effective	code	in
terms	of	economy	and	error	control.	For	many	years,	MFM	was	virtually	the	only	coding	method	used	for
rigid	disk	storage.	The	MFM	coding	for	OK	is	shown	in	Figure	2A.5.
2A.5	RUN-LENGTH-LIMITED	CODE
Run-length-limited	(RLL)	is	a	coding	method	in	which	block	character	code	words	such	as	ASCII	or
EBCDIC	 are	 translated	 into	 code	 words	 specially	 designed	 to	 limit	 the	 number	 of	 consecutive	 zeros
appearing	in	the	code.	An	RLL(d,	k)	code	allows	a	minimum	of	d	and	a	maximum	of	k	consecutive	zeros
to	appear	between	any	pair	of	consecutive	ones.

FIGURE	2A.4	Frequency	Modulation	Coding	of	OK
FIGURE	2A.5	Modified	Frequency	Modulation	Coding	of	OK
Clearly,	RLL	code	words	must	contain	more	bits	than	the	original	character	code.	However,	because
RLL	is	coded	using	NRZI	on	the	disk,	RLL-coded	data	actually	occupy	less	space	on	magnetic	media
because	fewer	flux	transitions	are	involved.	The	code	words	employed	by	RLL	are	designed	to	prevent	a
disk	from	losing	synchronization	as	it	would	if	a	“flat”	binary	NRZI	code	were	used.
Although	there	are	many	variants,	RLL(2,	7)	is	the	predominant	code	used	by	magnetic	disk	systems.	It
is	technically	a	16-bit	mapping	of	8-bit	ASCII	or	EBCDIC	characters.	However,	it	is	nearly	50%	more
efficient	than	MFM	in	terms	of	flux	reversals.	(Proof	of	this	is	left	as	an	exercise.)
Theoretically	speaking,	RLL	is	a	form	of	data	compression	called	Huffman	coding	 (discussed	 in
Chapter	7),	where	the	most	likely	information	bit	patterns	are	encoded	using	the	shortest	code	word	bit
patterns.	(In	our	case,	we	are	talking	about	the	fewest	number	of	flux	reversals.)	The	theory	is	based	on
the	assumption	that	the	presence	or	absence	of	a	1	in	any	bit	cell	is	an	equally	likely	event.	From	this
assumption,	 we	 can	 infer	 that	 the	 probability	 is	 0.25	 of	 the	 pattern	 10	 occurring	 within	 any	 pair	 of
adjacent	bit	cells.	(
Similarly,	 the	 bit	 pattern	 011	 has	 a	 probability	 of	 0.125	 of	 occurring.	Figure	 2A.6	 shows	 the
probability	tree	for	the	bit	patterns	used	in	RLL(2,	7).	Table	2A.1	gives	the	bit	patterns	used	by	RLL(2,
7).
As	you	can	see	by	the	table,	it	is	impossible	to	have	more	than	seven	consecutive	0s,	whereas	at	least
two	0s	will	appear	in	any	possible	combination	of	bits.
Figure	2A.7	compares	the	MFM	coding	for	OK	with	its	RLL(2,	7)	NRZI	coding.	MFM	has	12	flux
transitions	to	8	transitions	for	RLL.	If	the	limiting	factor	in	the	design	of	a	disk	is	the	number	of	flux
transitions	per	square	millimeter,	we	can	pack	50%	more	OKs	in	the	same	magnetic	area	using	RLL	than
we	could	using	MFM.	For	this	reason,	RLL	is	used	almost	exclusively	in	high-capacity	disk	drives.

FIGURE	2A.6	The	Probability	Tree	for	RLL(2,	7)	Coding
Character	Bit	PatternRLL(2,	7)	Code
100100
111000
000000100
010100100
011001000
001000100100
001100001000
Table	2A.1	RLL(2,	7)	Coding
FIGURE	2A.7	MFM	(top)	and	RLL(2,	7)	Coding	(bottom)	for	OK
2A.6	PARTIAL	RESPONSE	MAXIMUM	LIKELIHOOD	CODING
RLL	by	itself	is	insufficient	for	reliable	encoding	on	today’s	ultra-high-capacity	magnetic	disk	and	tape
media.	As	data	density	increases,	encoded	bits	are	necessarily	written	closer	together.	This	means	that
fewer	grains	of	magnetic	material	participate	in	the	encoding	of	each	bit,	causing	decreased	magnetic
signal	strength.	As	signal	strength	decreases,	adjacent	flux	reversals	begin	to	interfere	with	each	other.
This	phenomenon,	known	as	superpositioning,	is	characterized	in	Figure	2A.8,	which	shows	how	a	nice,
neat,	easy-to-detect	magnetic	sine	wave	starts	looking	like	a	string	of	overcooked	spaghetti.
Despite	its	wild	appearance,	superpositioned	waveforms	are	well	defined	and	understood.	However,
unlike	traditional	sine	waves,	their	characteristics	cannot	be	captured	by	a	simple	peak	detector	that	takes
one	 measurement	 per	 bit	 cell.	 They	 are	 instead	 sampled	 several	 times	 across	 the	 bit	 cell	 waveform,
giving	a	“partial	response”	pattern	to	the	detector	circuit.	The	detector	circuit	(Viterbi	detector)	then
matches	the	partial	response	pattern	to	a	relatively	small	set	of	possible	response	patterns	and	the	closest
match	(the	pattern	with	the	“maximum	likelihood”	of	being	correct)	is	passed	to	the	digital	decoder.	Thus,
this	 encoding	 scheme	 is	 called	partial	 response	 maximum	 likelihood,	 or	PRML.	 (After	 you	 read
Chapter	3,	you	will	understand	how	a	Viterbi	detector	decides	which	pattern	is	the	most	likely.)

PRML	 is	 a	 generic	 designation	 for	 a	 family	 of	 encoding	 methods	 that	 are	 distinguished	 from	 one
another	by	the	number	of	samples	taken	per	bit	cell.	More	frequent	sampling	permits	greater	data	density.
Along	with	improvements	in	magnetic	head	technology,	PRML	has	been	a	fundamental	enabler	of	the
geometric	increase	in	disk	and	tape	densities	since	2000,	and	it	is	indeed	possible	that	this	technology	has
not	yet	been	fully	exploited.
FIGURE	2A.8	Magnetic	Behaviors	as	Bit	Density	Increases
In	a),	b),	and	c),	magnetic	flux	changes	are	pushed	increasingly	closer	together.
2A.7	SUMMARY
Your	knowledge	of	how	bytes	are	stored	on	disks	and	tape	will	help	you	to	understand	many	of	the
concepts	and	problems	relating	to	data	storage.	Your	familiarity	with	error	control	methods	will	aid	you
in	 your	 study	 of	 both	 data	 storage	 and	 data	 communications.	 The	 best	 information	 pertinent	 to	 data
encoding	 for	 magnetic	 storage	 can	 be	 found	 in	 electrical	 engineering	 books.	 They	 contain	 a	 trove	 of
fascinating	information	regarding	the	behavior	of	physical	media,	and	how	this	behavior	is	employed	by
various	coding	methods.	You	will	learn	more	about	data	storage	in	Chapter	7.	Chapter	12	presents	topics
relating	to	data	communications.
EXERCISES
1.		Why	is	non-return-to-zero	coding	avoided	as	a	method	for	writing	data	to	a	magnetic	disk?
2.		Why	is	Manchester	coding	not	a	good	choice	for	writing	data	to	a	magnetic	disk?
3.		Explain	how	run-length-limited	encoding	works.
4.		Write	the	7-bit	ASCII	code	for	the	character	4	using	the	following	encoding:
a)	Non-return-to-zero
b)	Non-return-to-zero-invert
c)	Manchester	code

d)	Frequency	modulation
e)	Modified	frequency	modulation
f)	Run-length-limited
(Assume	1	is	“high”	and	0	is	“low.”)

1
The			brackets	denote	the	integer	floor	function,	which	is	the	largest	integer	that	is	smaller	than	or	equal	to	the	enclosed	quantity.	For
example,	8.3	=	8	and	8.9	=	8.

“I’ve	always	loved	that	word,	Boolean.”
—Claude	Shannon
CHAPTER	3

Boolean	Algebra	and	Digital	Logic
3.1			INTRODUCTION
George	 Boole	 lived	 in	 England	 during	 the	 first	 half	 of	 the	 nineteenth	 century.	 The	 firstborn	 son	 of	 a
cobbler,	Boole	taught	himself	Greek,	Latin,	French,	German,	and	the	language	of	mathematics.	Just	before
he	turned	16,	Boole	accepted	a	position	teaching	at	a	small	Methodist	school,	providing	his	family	with
much-needed	income.	At	the	age	of	19,	Boole	returned	home	to	Lincoln,	England,	and	founded	his	own
boarding	school	to	better	provide	support	for	his	family.	He	operated	this	school	for	15	years,	until	he
became	Professor	of	Mathematics	at	Queen’s	College	in	Cork,	Ireland.	His	social	status	as	the	son	of	a
tradesman	prevented	Boole’s	appointment	to	a	more	prestigious	university,	despite	his	authoring	of	more
than	a	dozen	highly	esteemed	papers	and	treatises.	His	most	famous	monograph,	The	Laws	of	Thought,
published	in	1854,	created	a	branch	of	mathematics	known	as	symbolic	logic	or	Boolean	algebra.
Nearly	85	years	later,	John	Vincent	Atanasoff	applied	Boolean	algebra	to	computing.	He	recounted	the
moment	of	his	insight	to	Linda	Null.	At	the	time,	Atanasoff	was	attempting	to	build	a	calculating	machine
based	on	the	same	technology	used	by	Pascal	and	Babbage.	His	aim	was	to	use	this	machine	to	solve
systems	of	linear	equations.	After	struggling	with	repeated	failures,	Atanasoff	was	so	frustrated	that	he
decided	to	take	a	drive.	He	was	living	in	Ames,	Iowa,	at	the	time,	but	found	himself	200	miles	away	in
Illinois	before	he	suddenly	realized	how	far	he	had	driven.
Atanasoff	had	not	intended	to	drive	that	far,	but	because	he	was	in	Illinois,	where	it	was	legal	to	buy	a
drink	in	a	tavern,	he	sat	down	and	ordered	a	bourbon.	He	chuckled	to	himself	when	he	realized	that	he	had
driven	such	a	distance	for	a	drink!	Even	more	ironic	is	the	fact	that	he	never	touched	the	drink.	He	felt	he
needed	a	clear	head	to	write	down	the	revelations	that	came	to	him	during	his	long,	aimless	journey.
Exercising	 his	 physics	 and	 mathematics	 backgrounds	 and	 focusing	 on	 the	 failures	 of	 his	 previous
computing	machine,	he	made	four	critical	breakthroughs	necessary	in	the	machine’s	new	design:
1.		He	would	use	electricity	instead	of	mechanical	movements	(vacuum	tubes	would	allow	him	to	do
this).
2.		Because	he	was	using	electricity,	he	would	use	base	2	numbers	instead	of	base	10	(this	correlated
directly	with	switches	that	were	either	“on”	or	“off”),	resulting	in	a	digital,	rather	than	an	analog,
machine.
3.	 	 He	 would	 use	 capacitors	 (condensers)	 for	 memory	 because	 they	 store	 electrical	 charges	 with	 a
regenerative	process	to	avoid	power	leakage.
4.		Computations	would	be	done	by	what	Atanasoff	termed	“direct	logical	action”	(which	is	essentially
equivalent	to	Boolean	algebra)	and	not	by	enumeration	as	all	previous	computing	machines	had	done.
It	should	be	noted	that	at	the	time,	Atanasoff	did	not	recognize	the	application	of	Boolean	algebra	to
his	problem	and	that	he	devised	his	own	direct	logical	action	by	trial	and	error.	He	was	unaware	that	in
1938,	Claude	Shannon	proved	that	two-valued	Boolean	algebra	could	describe	the	operation	of	two-
valued	electrical	switching	circuits.	Today,	we	see	the	significance	of	Boolean	algebra’s	application	in
the	design	of	modern	computing	systems.	It	is	for	this	reason	that	we	include	a	chapter	on	Boolean	logic

and	its	relationship	to	digital	computers.
This	chapter	contains	a	brief	introduction	to	the	basics	of	logic	design.	It	provides	minimal	coverage
of	 Boolean	 algebra	 and	 this	 algebra’s	 relationship	 to	 logic	 gates	 and	 basic	 digital	 circuits.	 You	 may
already	be	familiar	with	the	basic	Boolean	operators	from	your	previous	programming	experience.	It	is	a
fair	question,	then,	to	ask	why	you	must	study	this	material	in	more	detail.	The	relationship	between
Boolean	logic	and	the	actual	physical	components	of	any	computer	system	is	strong,	as	you	will	see	in	this
chapter.	 As	 a	 computer	 scientist,	 you	 may	 never	 have	 to	 design	 digital	 circuits	 or	 other	 physical
components—in	fact,	this	chapter	will	not	prepare	you	to	design	such	items.	Rather,	it	provides	sufficient
background	for	you	to	understand	the	basic	motivation	underlying	computer	design	and	implementation.
Understanding	how	Boolean	logic	affects	the	design	of	various	computer	system	components	will	allow
you	to	use,	from	a	programming	perspective,	any	computer	system	more	effectively.	If	you	are	interested
in	delving	deeper,	there	are	many	resources	listed	at	the	end	of	the	chapter	to	allow	further	investigation
into	these	topics.
3.2			BOOLEAN	ALGEBRA
Boolean	algebra	is	an	algebra	for	the	manipulation	of	objects	that	can	take	on	only	two	values,	typically
true	 and	 false,	 although	 it	 can	 be	 any	 pair	 of	 values.	 Because	 computers	 are	 built	 as	 collections	 of
switches	that	are	either	“on”	or	“off,”	Boolean	algebra	is	a	natural	way	to	represent	digital	information.	In
reality,	digital	circuits	use	low	and	high	voltages,	but	for	our	level	of	understanding,	0	and	1	will	suffice.
It	is	common	to	interpret	the	digital	value	0	as	false	and	the	digital	value	1	as	true.
3.2.1		Boolean	Expressions
In	addition	to	binary	objects,	Boolean	algebra	also	has	operations	that	can	be	performed	on	these	objects,
or	variables.	Combining	the	variables	and	operators	yields	Boolean	expressions.	A	Boolean	function
typically	has	one	or	more	input	values	and	yields	a	result,	based	on	the	input	values,	in	the	set	{0,1}.
Three	common	Boolean	operators	are	AND,	OR,	and	NOT.	To	better	understand	these	operators,	we
need	 a	 mechanism	 to	 allow	 us	 to	 examine	 their	 behaviors.	 A	 Boolean	 operator	 can	 be	 completely
described	using	a	table	that	lists	the	inputs,	all	possible	values	for	these	inputs,	and	the	resulting	values	of
the	operation	for	all	possible	combinations	of	these	inputs.	This	table	is	called	a	truth	table.	A	truth	table
shows	the	relationship,	in	tabular	form,	between	the	input	values	and	the	result	of	a	specific	Boolean
operator	or	function	on	the	input	variables.	Let’s	look	at	the	Boolean	operators	AND,	OR,	and	NOT	to
see	how	each	is	represented,	using	both	Boolean	algebra	and	truth	tables.
The	logical	operator	AND	is	typically	represented	by	either	a	dot	or	no	symbol	at	all.	For	example,
the	Boolean	expression	xy	is	equivalent	to	the	expression	x	º	y	and	is	read	“x	and	y.”	The	expression	xy	is
often	referred	to	as	a	Boolean	product.	The	behavior	of	this	operator	is	characterized	by	the	truth	table
shown	in	Table	3.1.
The	result	of	the	expression	xy	is	1	only	when	both	inputs	are	1,	and	0	otherwise.	Each	row	in	the
table	represents	a	different	Boolean	expression,	and	all	possible	combinations	of	values	for	x	and	y	are
represented	by	the	rows	in	the	table.
The	Boolean	operator	OR	is	typically	represented	by	a	plus	sign.	Therefore,	the	expression	x	+	y	is
read	“x	or	y.”	The	result	of	x	+	y	is	0	only	when	both	of	its	input	values	are	0.	The	expression	x	+	y	is
often	referred	to	as	a	Boolean	sum.	The	truth	table	for	OR	is	shown	in	Table	3.2.
The	remaining	logical	operator,	NOT,	is	represented	typically	by	either	an	overscore	or	a	prime.

Therefore,	both	x–	and	x′	are	read	“not	x.”	The	truth	table	for	NOT	is	shown	in	Table	3.3.

InputsOutputs
x						yxy
0						00
0						10
1						00
1						11
TABLE	3.1	Truth	Table	for	AND

InputsOutputs
x						yx	+	y
0						00
0						11
1						01
1						11
TABLE	3.2	Truth	Table	for	OR

InputsOutputs
xx′
01
10
TABLE	3.3	Truth	Table	for	NOT
We	now	understand	that	Boolean	algebra	deals	with	binary	variables	and	logical	operations	on	those
variables.	Combining	these	two	concepts,	we	can	examine	Boolean	expressions	composed	of	Boolean
variables	and	multiple	logic	operators.	For	example,	the	Boolean	function
F(x,y,z)	=	x	+	y′z
is	represented	by	a	Boolean	expression	involving	the	three	Boolean	variables	x,	y,	and	z	and	the	logical
operators	OR,	NOT,	and	AND.	How	do	we	know	which	operator	to	apply	first?	The	rules	of	precedence
for	Boolean	operators	give	NOT	top	priority,	followed	by	AND,	and	then	OR.	For	our	previous	function
F,	we	would	negate	y	first,	then	perform	the	AND	of	y′	and	z,	and	finally	OR	this	result	with	x.

We	can	also	use	a	truth	table	to	represent	this	expression.	It	is	often	helpful,	when	creating	a	truth
table	for	a	more	complex	function	such	as	this,	to	build	the	table	representing	different	pieces	of	the
function,	one	column	at	a	time,	until	the	final	function	can	be	evaluated.	The	truth	table	for	our	function	F
is	shown	in	Table	3.4.
The	last	column	in	the	truth	table	indicates	the	values	of	the	function	for	all	possible	combinations	of
x,	y,	and	z.	We	note	that	the	real	truth	table	for	our	function	F	consists	of	only	the	first	three	columns	and
the	last	column.	The	shaded	columns	show	the	intermediate	steps	necessary	to	arrive	at	our	final	answer.
Creating	truth	tables	in	this	manner	makes	it	easier	to	evaluate	the	function	for	all	possible	combinations
of	the	input	values.
3.2.2		Boolean	Identities
Frequently,	a	Boolean	expression	is	not	in	its	simplest	form.	Recall	from	algebra	that	an	expression	such
as	2x	+	6x	is	not	in	its	simplest	form;	it	can	be	reduced	(represented	by	fewer	or	simpler	terms)	to	8x.
Boolean	expressions	can	also	be	simplified,	but	we	need	new	identities,	or	laws,	that	apply	to	Boolean
algebra	instead	of	regular	algebra.	These	identities,	which	apply	to	single	Boolean	variables	as	well	as
Boolean	expressions,	are	listed	in	Table	3.5.	Note	that	each	relationship	(with	the	exception	of	the	last
one)	has	both	an	AND	(or	product)	form	and	an	OR	(or	sum)	form.	This	is	known	as	the	duality	principle.
TABLE	3.4	Truth	Table	for	F(x,y,z)	=	x	+	y′z

Identity	NameAND	FormOR	Form
Identity	Law1x	=	x0	+	x	=	x
Null	(or	Dominance)	Law0x	=	01	+	x	=	1
Idempotent	Lawxx	=	xx	+	x	=	x
Inverse	Lawxx′	=	0x	+	x′	=	1
Commutative	Lawxy	=	yxx	+	y	=	y	+	x
Associative	Law(xy)z	=	x(yz)(x	+	y)	+	z	=	x	+	(y	+	z)
Distributive	Lawx	+	(yz)	=	(x	+	y)	(x	+	z)x(y	+	z)	=	xy	+	xz
Absorption	Lawx(x	+	y)	=	xx	+	xy	=	x

DeMorgan’s	Law(xy)′	=	x′	+	y′(x	+	y)′	=	x′y′
Double	Complement	Lawx′′	=	x
TABLE	3.5	Basic	Identities	of	Boolean	Algebra
The	Identity	Law	states	that	any	Boolean	variable	ANDed	with	1	or	ORed	with	0	simply	results	in	the
original	variable	(1	is	the	identity	element	for	AND;	0	is	the	identity	element	for	OR).	The	Null	Law
states	 that	 any	 Boolean	 variable	 ANDed	 with	 0	 is	 0,	 and	 a	 variable	 ORed	 with	 1	 is	 always	 1.	 The
Idempotent	Law	states	that	ANDing	or	ORing	a	variable	with	itself	produces	the	original	variable.	The
Inverse	Law	states	that	ANDing	or	ORing	a	variable	with	its	complement	produces	the	identity	for	that
given	operation.	Boolean	variables	can	be	reordered	(commuted)	and	regrouped	(associated)	without
affecting	the	final	result.	You	should	recognize	these	as	the	Commutative	and	Associative	Laws	from
algebra.	The	Distributive	Law	shows	how	OR	distributes	over	AND	and	vice	versa.
The	Absorption	Law	and	DeMorgan’s	Law	are	not	so	obvious,	but	we	can	prove	these	identities	by
creating	a	truth	table	for	the	various	expressions:	If	the	right-hand	side	is	equal	to	the	left-hand	side,	the
expressions	represent	the	same	function	and	result	in	identical	truth	tables.	Table	3.6	depicts	the	truth
table	for	both	the	left-	and	right-hand	sides	of	DeMorgan’s	Law	for	AND.	It	is	left	as	exercises	to	prove
the	validity	of	the	remaining	laws,	in	particular,	the	OR	form	of	DeMorgan’s	Law	and	both	forms	of	the
Absorption	Law.
The	Double	Complement	Law	formalizes	the	idea	of	the	double	negative,	which	evokes	rebuke	from
high	school	English	teachers.	The	Double	Complement	Law	can	be	useful	in	digital	circuits	as	well	as	in
your	life.	For	example,	let	x	=	1	represent	the	idea	that	you	have	a	positive	quantity	of	cash.	If	you	have	no
cash,	you	have	x′.	When	an	untrustworthy	acquaintance	asks	to	borrow	some	cash,	you	can	truthfully	say
that	you	don’t	have	no	money.	That	is,	x	=	(x)′′	even	if	you	just	got	paid.
TABLE	3.6	Truth	Table	for	the	AND	Form	of	DeMorgan’s	Law
One	of	the	most	common	errors	that	beginners	make	when	working	with	Boolean	logic	is	to	assume
the	 following:	 (xy)′	 =	x′y′.	Please	note	that	this	is	not	a	valid	equality!	 DeMorgan’s	 Law	 clearly
indicates	that	this	statement	is	incorrect.	Instead,	(xy)′	=	x′	+	y′.	This	is	a	very	easy	mistake	to	make,	and
one	that	should	be	avoided.	Care	must	be	taken	with	other	expressions	involving	negation	as	well.
3.2.3		Simplification	of	Boolean	Expressions
The	algebraic	identities	we	studied	in	algebra	class	allow	us	to	reduce	algebraic	expressions	(such	as
10x	+	2y	–	x	+	3y)	to	their	simplest	forms	(9x	+	5y).	The	Boolean	identities	can	be	used	to	simplify
Boolean	expressions	in	a	similar	manner.	We	apply	these	identities	in	the	following	examples.
	EXAMPLE	3.1	Suppose	we	have	the	function	F(x,y)	=	xy	+	xy.	Using	the	OR	form	of	the	Idempotent

Law	and	treating	the	expression	xy	as	a	Boolean	variable,	we	simplify	the	original	expression	to	xy.
Therefore,	F(x,y)	=	xy	+	xy	=	xy.
	EXAMPLE	3.2	Given	the	function	F(x,y,z)	=	x′yz	+	x′yz′	+	xz,	we	simplify	as	follows:
	EXAMPLE	3.3	Given	the	function	F(x,y)	=	y	+	(xy)′,	we	simplify	as	follows:
	EXAMPLE	3.4	Given	the	function	F(x,y)	=	(xy)′(x′	+	y)(y′ +	y),	we	simplify	as	follows:
At	times,	the	simplification	is	reasonably	straightforward,	as	in	the	preceding	examples.	However,
using	the	identities	can	be	tricky,	as	we	see	in	the	next	two	examples.
	EXAMPLE	3.5	Given	the	function	F(x,y)	=	x′(x	+	y)	+	(y	+	x)(x	+	y′),	we	simplify	as	follows:

	EXAMPLE	3.6	Given	the	function	F(x,y,z)	=	xy	+	x′z	+	yz,	we	simplify	as	follows:
Example	3.6	illustrates	what	is	commonly	known	as	the	Consensus	Theorem.
How	did	we	know	to	insert	additional	terms	to	simplify	the	function	in	Example	3.6?	Unfortunately,
there	is	no	defined	set	of	rules	for	using	these	identities	to	minimize	a	Boolean	expression;	it	is	simply
something	that	comes	with	experience.	There	are	other	methods	that	can	be	used	to	simplify	Boolean
expressions;	we	mention	these	later	in	this	section.
We	can	also	use	these	identities	to	prove	Boolean	equalities,	as	we	see	in	Example	3.7.
	EXAMPLE	3.7	Prove	that	(x	+	y)(x′	+	y)	=	y

To	prove	the	equality	of	two	Boolean	expressions,	you	can	also	create	the	truth	tables	for	each	and
compare.	If	the	truth	tables	are	identical,	the	expressions	are	equal.	We	leave	it	as	an	exercise	to	find	the
truth	tables	for	the	equality	proven	in	Example	3.7.
3.2.4		Complements
As	you	saw	in	Example	3.1,	the	Boolean	identities	can	be	applied	to	Boolean	expressions,	not	simply
Boolean	variables	(we	treated	xy	as	a	Boolean	variable	and	then	applied	the	Idempotent	Law).	The	same
is	true	for	the	Boolean	operators.	The	most	common	Boolean	operator	applied	to	more	complex	Boolean
expressions	is	the	NOT	operator,	resulting	in	the	complement	of	the	expression.	Quite	often,	it	is	cheaper
and	less	complicated	to	implement	the	complement	of	a	function	rather	than	the	function	itself.	If	we
implement	 the	 complement,	 we	 must	 invert	 the	 final	 output	 to	 yield	 the	 original	 function;	 this	 is
accomplished	with	one	simple	NOT	operation.	Therefore,	complements	are	quite	useful.
To	find	the	complement	of	a	Boolean	function,	we	use	DeMorgan’s	Law.	The	OR	form	of	this	law
states	that	(x	+	y)′	=	x′y′.	We	can	easily	extend	this	to	three	or	more	variables	as	follows:
Given	the	function:
F(x,y,z)	=	(x+y+z).	Then	F′(x,y,z)	=	(x	+	y	+	z)′.
Let	w	=	(x	+	y).	Then	F′(x,y,z)	=	(w	+	z)′	=	w′z′.
Now,	applying	DeMorgan’s	Law	again,	we	get:
w′z′	=	(x	+	y)′z′	=	x′y′z′	=	F′(x,y,z)
Therefore,	if	F(x,y,z)	=	(x	+	y	+	z),	then	F′(x,y,z)	=	x′y′z′.	Applying	the	principle	of	duality,	we	see	that
(xyz)′	=	x′	+	y′ +	z′.
It	appears	that,	to	find	the	complement	of	a	Boolean	expression,	we	simply	replace	each	variable	by
its	 complement	 (x	 is	 replaced	 by	x′)	 and	 interchange	 ANDs	 and	 ORs.	 In	 fact,	 this	 is	 exactly	 what
DeMorgan’s	Law	tells	us	to	do.	For	example,	the	complement	of	x′	+	yz′	is	x(y′ +	z).	We	have	to	add	the
parentheses	to	ensure	the	correct	precedence.
You	can	verify	that	this	simple	rule	of	thumb	for	finding	the	complement	of	a	Boolean	expression	is
correct	by	examining	the	truth	tables	for	both	the	expression	and	its	complement.	The	complement	of	any
expression,	when	represented	as	a	truth	table,	should	have	0s	for	output	everywhere	the	original	function
has	1s,	and	1s	in	those	places	where	the	original	function	has	0s.	Table	3.7	depicts	the	truth	tables	for
F(x,y,z)	=	x′	+	yz′	and	its	complement,	F′(x,y,z)	=	x(y′ +	z).	The	shaded	portions	indicate	the	final	results

for	F	and	F′.
3.2.5		Representing	Boolean	Functions
We	have	seen	that	there	are	many	different	ways	to	represent	a	given	Boolean	function.	For	example,	we
can	use	a	truth	table,	or	we	can	use	one	of	many	different	Boolean	expressions.	In	fact,	there	are	an
infinite	number	of	Boolean	expressions	that	are	logically	equivalent	to	one	another.	Two	expressions	that
can	be	represented	by	the	same	truth	table	are	considered	logically	equivalent	(see	Example	3.8).
	EXAMPLE	3.8	Suppose	F(x,y,z)	=	x	+	xy′.	We	can	also	express	F	as	F(x,y,z)	=	x	+	x	+	xy′	because	the
Idempotent	Law	tells	us	these	two	expressions	are	the	same.	We	can	also	express	F	as	F(x,y,z)	=	x(1	+	y′)
using	the	Distributive	Law.
To	help	eliminate	potential	confusion,	logic	designers	specify	a	Boolean	function	using	a	canonical,
or	standardized,	 form.	 For	 any	 given	 Boolean	 function,	 there	 exists	 a	 unique	 standardized	 form.
However,	 there	 are	 different	 “standards”	 that	 designers	 use.	 The	 two	 most	 common	 are	 the	 sum-of-
products	form	and	the	product-of-sums	form.
The	sum-of-products	form	 requires	 that	 the	 expression	 be	 a	 collection	 of	 ANDed	 variables	 (or
product	terms)	that	are	ORed	together.	The	function	F
1
(x,y,z)	=	xy	+	yz′	+	xyz	is	in	sum-of-products	form.
The	function	F
2
(x,y,z)	=	xy′ +	x	(y	+	z′)	is	not	in	sum-of-products	form.	We	apply	the	Distributive	Law	to
distribute	the	x	variable	in	F
2
,	resulting	in	the	expression	xy′ +	xy	+	xz′,	which	is	now	in	sum-of-products
form.
TABLE	3.7	Truth	Table	Representation	for	a	Function	and	Its	Complement
Boolean	expressions	stated	in	product-of-sums	form	consist	of	ORed	variables	(sum	terms)	that	are
ANDed	together.	The	function	F
1
(x,y,z)	=	(x	+	y)	(x	+	z′)(y	+	z′)(y	+	z)	is	in	product-of-sums	form.	The
product-of-sums	form	is	often	preferred	when	the	Boolean	expression	evaluates	true	in	more	cases	than	it
evaluates	false.	This	is	not	the	case	with	the	function	F
1
,	so	the	sum-of-products	form	is	appropriate.
Also,	the	sum-of-products	form	is	usually	easier	to	work	with	and	to	simplify;	we	therefore	use	this	form
exclusively	in	the	sections	that	follow.
Any	 Boolean	 expression	 can	 be	 represented	 in	 sum-of-products	 form.	 Because	 any	 Boolean
expression	 can	 also	 be	 represented	 as	 a	 truth	 table,	 we	 conclude	 that	 any	 truth	 table	 can	 also	 be
represented	in	sum-of-products	form.	Example	3.9	shows	us	that	it	is	a	simple	matter	to	convert	a	truth
table	into	sum-of-products	form.

	EXAMPLE	3.9	Consider	a	simple	majority	function.	This	is	a	function	that,	when	given	three	inputs,
outputs	a	0	if	less	than	half	of	its	inputs	are	1,	and	a	1	if	at	least	half	of	its	inputs	are	1.	Table	3.8	depicts
the	truth	table	for	this	majority	function	over	three	variables.
To	convert	the	truth	table	to	sum-of-products	form,	we	start	by	looking	at	the	problem	in	reverse.	If
we	want	the	expression	x	+	y	to	equal	1,	then	either	x	or	y	(or	both)	must	be	equal	to	1.	If	xy	+	yz	=	1,	then
either	xy	=	1	or	yz	=	1	(or	both).
Using	this	logic	in	reverse	and	applying	it	to	our	majority	function,	we	see	that	the	function	must
output	a	1	when	x	=	0,	y	=	1,	and	z	=	1.	The	product	term	that	satisfies	this	is	x′yz	(clearly,	this	is	equal	to
1	when	x	=	0,	y	=	1,	and	z	=	1).	The	second	occurrence	of	an	output	value	of	1	is	when	x	=	1,	y	=	0,	and	z
=	1.	The	product	term	to	guarantee	an	output	of	1	is	xy′z.	The	third	product	term	we	need	is	xyz′,	and	the
last	is	xyz.	In	summary,	to	generate	a	sum-of-products	expression	using	the	truth	table	for	any	Boolean
expression,	we	must	generate	a	product	term	of	the	input	variables	corresponding	to	each	row	where	the
value	of	the	output	variable	in	that	row	is	1.	In	each	product	term,	we	must	then	complement	any	variables
that	are	0	for	that	row.
Our	majority	function	can	be	expressed	in	sum-of-products	form	as	F(x,y,z)	=	x′yz	+	xy′z	+	xyz′	+	xyz.
TABLE	3.8	Truth	Table	Representation	for	the	Majority	Function
Please	note	that	the	expression	for	the	majority	function	in	Example	3.9	may	not	be	in	simplest	form;
we	are	only	guaranteeing	a	standard	form.	The	sum-of-products	and	product-of-sums	standard	forms	are
equivalent	ways	of	expressing	a	Boolean	function.	One	form	can	be	converted	to	the	other	through	an
application	of	Boolean	identities.	Whether	using	sum-of-products	or	product-of-sums,	the	expression	must
eventually	be	converted	to	its	simplest	form,	which	means	reducing	the	expression	to	the	minimum	number
of	 terms.	 Why	 must	 the	 expressions	 be	 simplified?	 A	 one-to-one	 correspondence	 exists	 between	 a
Boolean	 expression	 and	 its	 implementation	 using	 electrical	 circuits,	 as	 shown	 in	 the	 next	 section.
Unnecessary	terms	in	the	expression	lead	to	unnecessary	components	in	the	physical	circuit,	which	in	turn
yield	a	suboptimal	circuit.
3.3			LOGIC	GATES
The	logical	operators	AND,	OR,	and	NOT	that	we	have	discussed	have	been	represented	thus	far	in	an
abstract	sense	using	truth	tables	and	Boolean	expressions.	The	actual	physical	components,	or	digital
circuits,	such	as	those	that	perform	arithmetic	operations	or	make	choices	in	a	computer,	are	constructed
from	a	number	of	primitive	elements	called	gates.	Gates	implement	each	of	the	basic	logic	functions	we
have	discussed.	These	gates	are	the	basic	building	blocks	for	digital	design.	Formally,	a	gate	is	a	small,